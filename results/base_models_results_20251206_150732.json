[
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"arc_challenge",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3498",
    "acc_norm":"0.3737",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"arc_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.2769",
    "acc_norm":"0.3188",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"belebele_spa_Latn",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.2544",
    "acc_norm":"0.2544",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"global_mmlu_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.2752",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":"0.2255",
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":"0.4464",
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":"0.2059",
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":"0.1957",
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":"0.3276",
    "subcategories_medical":"0.2500"
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"gsm8k",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":"0.0000",
    "exact_match_stderr,strict-match":"0.0000",
    "exact_match,flexible-extract":"0.0174",
    "exact_match_stderr,flexible-extract":"0.0036",
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"hellaswag",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.4715",
    "acc_norm":"0.6281",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"hellaswag_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.4203",
    "acc_norm":"0.5218",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"ifeval",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":"0.0832",
    "prompt_level_strict_acc_stderr,none":"0.0119",
    "inst_level_strict_acc,none":"0.1691",
    "prompt_level_loose_acc,none":"0.0850",
    "prompt_level_loose_acc_stderr,none":"0.0120",
    "inst_level_loose_acc,none":"0.1739",
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"lambada_openai",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":"7.27",
    "word_perplexity":"0.00",
    "bits_per_byte":"0.0000",
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"mmlu",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.2512",
    "acc_norm":"N\/A",
    "category_STEM":"0.2419",
    "category_Humanities":"0.2534",
    "category_Social_Sciences":"0.2561",
    "category_Other":"0.2574",
    "subcategories_humanities":"0.2493",
    "subcategories_formal_logic":"0.2381",
    "subcategories_high_school_european_history":"0.2364",
    "subcategories_high_school_us_history":"0.2647",
    "subcategories_high_school_world_history":"0.2616",
    "subcategories_international_law":"0.2231",
    "subcategories_jurisprudence":"0.2593",
    "subcategories_logical_fallacies":"0.2393",
    "subcategories_moral_disputes":"0.2254",
    "subcategories_moral_scenarios":"0.2380",
    "subcategories_philosophy":"0.2765",
    "subcategories_prehistory":"0.2778",
    "subcategories_professional_law":"0.2438",
    "subcategories_world_religions":"0.3099",
    "subcategories_other":"0.2678",
    "subcategories_business_ethics":"0.2100",
    "subcategories_clinical_knowledge":"0.2868",
    "subcategories_college_medicine":"0.1734",
    "subcategories_global_facts":"0.3200",
    "subcategories_human_aging":"0.3363",
    "subcategories_management":"0.2621",
    "subcategories_marketing":"0.2735",
    "subcategories_medical_genetics":"0.2700",
    "subcategories_miscellaneous":"0.2771",
    "subcategories_nutrition":"0.2680",
    "subcategories_professional_accounting":"0.2376",
    "subcategories_professional_medicine":"0.2243",
    "subcategories_virology":"0.3193",
    "subcategories_social_sciences":"0.2431",
    "subcategories_econometrics":"0.2544",
    "subcategories_high_school_geography":"0.2121",
    "subcategories_high_school_government_and_politics":"0.2694",
    "subcategories_high_school_macroeconomics":"0.2256",
    "subcategories_high_school_microeconomics":"0.2017",
    "subcategories_high_school_psychology":"0.2477",
    "subcategories_human_sexuality":"0.2366",
    "subcategories_professional_psychology":"0.2533",
    "subcategories_public_relations":"0.3273",
    "subcategories_security_studies":"0.2327",
    "subcategories_sociology":"0.2388",
    "subcategories_us_foreign_policy":"0.2700",
    "subcategories_stem":"0.2429",
    "subcategories_abstract_algebra":"0.2100",
    "subcategories_anatomy":"0.1852",
    "subcategories_astronomy":"0.2105",
    "subcategories_college_biology":"0.2083",
    "subcategories_college_chemistry":"0.2400",
    "subcategories_college_computer_science":"0.2800",
    "subcategories_college_mathematics":"0.2700",
    "subcategories_college_physics":"0.1863",
    "subcategories_computer_security":"0.2800",
    "subcategories_conceptual_physics":"0.3277",
    "subcategories_electrical_engineering":"0.2345",
    "subcategories_elementary_mathematics":"0.2407",
    "subcategories_high_school_biology":"0.2129",
    "subcategories_high_school_chemistry":"0.1970",
    "subcategories_high_school_computer_science":"0.3000",
    "subcategories_high_school_mathematics":"0.2778",
    "subcategories_high_school_physics":"0.2384",
    "subcategories_high_school_statistics":"0.2639",
    "subcategories_machine_learning":"0.2321",
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"truthfulqa_mc2",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3593",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"veritas_qa_ca",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.1303",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"veritas_qa_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.1303",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"BSC-LT\/salamandra-2b",
    "model_size":"2b",
    "task":"wikitext",
    "word_perplexity,none":"11.8901",
    "byte_perplexity,none":"1.5888",
    "bits_per_byte,none":"0.6679",
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"arc_challenge",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3148",
    "acc_norm":"0.3720",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"arc_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.2564",
    "acc_norm":"0.3000",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"belebele_spa_Latn",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3233",
    "acc_norm":"0.3233",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"global_mmlu_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3509",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":"0.3431",
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":"0.3571",
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":"0.3824",
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":"0.2826",
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":"0.3793",
    "subcategories_medical":"0.3611"
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"gsm8k",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":"0.0553",
    "exact_match_stderr,strict-match":"0.0063",
    "exact_match,flexible-extract":"0.0584",
    "exact_match_stderr,flexible-extract":"0.0065",
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"hellaswag",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.4810",
    "acc_norm":"0.6419",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"hellaswag_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3731",
    "acc_norm":"0.4731",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"ifeval",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":"0.0998",
    "prompt_level_strict_acc_stderr,none":"0.0129",
    "inst_level_strict_acc,none":"0.1475",
    "prompt_level_loose_acc,none":"0.1128",
    "prompt_level_loose_acc_stderr,none":"0.0136",
    "inst_level_loose_acc,none":"0.1607",
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"lambada_openai",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":"5.43",
    "word_perplexity":"0.00",
    "bits_per_byte":"0.0000",
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"mmlu",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3198",
    "acc_norm":"N\/A",
    "category_STEM":"0.2964",
    "category_Humanities":"0.3199",
    "category_Social_Sciences":"0.3408",
    "category_Other":"0.3249",
    "subcategories_humanities":"0.2927",
    "subcategories_formal_logic":"0.2222",
    "subcategories_high_school_european_history":"0.3879",
    "subcategories_high_school_us_history":"0.3039",
    "subcategories_high_school_world_history":"0.3207",
    "subcategories_international_law":"0.4463",
    "subcategories_jurisprudence":"0.3333",
    "subcategories_logical_fallacies":"0.2515",
    "subcategories_moral_disputes":"0.2919",
    "subcategories_moral_scenarios":"0.2380",
    "subcategories_philosophy":"0.3248",
    "subcategories_prehistory":"0.3920",
    "subcategories_professional_law":"0.2666",
    "subcategories_world_religions":"0.3801",
    "subcategories_other":"0.3589",
    "subcategories_business_ethics":"0.3200",
    "subcategories_clinical_knowledge":"0.3358",
    "subcategories_college_medicine":"0.2832",
    "subcategories_global_facts":"0.2000",
    "subcategories_human_aging":"0.3767",
    "subcategories_management":"0.3204",
    "subcategories_marketing":"0.4017",
    "subcategories_medical_genetics":"0.4100",
    "subcategories_miscellaneous":"0.4202",
    "subcategories_nutrition":"0.3791",
    "subcategories_professional_accounting":"0.2660",
    "subcategories_professional_medicine":"0.3162",
    "subcategories_virology":"0.4036",
    "subcategories_social_sciences":"0.3247",
    "subcategories_econometrics":"0.2719",
    "subcategories_high_school_geography":"0.3737",
    "subcategories_high_school_government_and_politics":"0.3472",
    "subcategories_high_school_macroeconomics":"0.2282",
    "subcategories_high_school_microeconomics":"0.2941",
    "subcategories_high_school_psychology":"0.3578",
    "subcategories_human_sexuality":"0.2977",
    "subcategories_professional_psychology":"0.3088",
    "subcategories_public_relations":"0.2909",
    "subcategories_security_studies":"0.3796",
    "subcategories_sociology":"0.3333",
    "subcategories_us_foreign_policy":"0.5300",
    "subcategories_stem":"0.2927",
    "subcategories_abstract_algebra":"0.3000",
    "subcategories_anatomy":"0.3630",
    "subcategories_astronomy":"0.2500",
    "subcategories_college_biology":"0.2778",
    "subcategories_college_chemistry":"0.2200",
    "subcategories_college_computer_science":"0.2900",
    "subcategories_college_mathematics":"0.2700",
    "subcategories_college_physics":"0.2157",
    "subcategories_computer_security":"0.5200",
    "subcategories_conceptual_physics":"0.3191",
    "subcategories_electrical_engineering":"0.2621",
    "subcategories_elementary_mathematics":"0.2407",
    "subcategories_high_school_biology":"0.3129",
    "subcategories_high_school_chemistry":"0.2611",
    "subcategories_high_school_computer_science":"0.3000",
    "subcategories_high_school_mathematics":"0.2741",
    "subcategories_high_school_physics":"0.2583",
    "subcategories_high_school_statistics":"0.3750",
    "subcategories_machine_learning":"0.3214",
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"truthfulqa_mc2",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3854",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"veritas_qa_ca",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.1785",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"veritas_qa_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.1530",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-1B",
    "model_size":"1b",
    "task":"wikitext",
    "word_perplexity,none":"11.9853",
    "byte_perplexity,none":"1.5912",
    "bits_per_byte,none":"0.6701",
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"arc_challenge",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.4249",
    "acc_norm":"0.4616",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"arc_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3675",
    "acc_norm":"0.3940",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"belebele_spa_Latn",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.5656",
    "acc_norm":"0.5656",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"global_mmlu_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.5495",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":"0.6078",
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":"0.5714",
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":"0.5196",
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":"0.4565",
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":"0.5862",
    "subcategories_medical":"0.5556"
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"gsm8k",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":"0.2616",
    "exact_match_stderr,strict-match":"0.0121",
    "exact_match,flexible-extract":"0.2661",
    "exact_match_stderr,flexible-extract":"0.0122",
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"hellaswag",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.5581",
    "acc_norm":"0.7411",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"hellaswag_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.4532",
    "acc_norm":"0.5889",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"ifeval",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":"0.0684",
    "prompt_level_strict_acc_stderr,none":"0.0109",
    "inst_level_strict_acc,none":"0.1199",
    "prompt_level_loose_acc,none":"0.0758",
    "prompt_level_loose_acc_stderr,none":"0.0114",
    "inst_level_loose_acc,none":"0.1259",
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"lambada_openai",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":"3.88",
    "word_perplexity":"0.00",
    "bits_per_byte":"0.0000",
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"mmlu",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.5783",
    "acc_norm":"N\/A",
    "category_STEM":"0.4892",
    "category_Humanities":"0.6155",
    "category_Social_Sciences":"0.6668",
    "category_Other":"0.5536",
    "subcategories_humanities":"0.5131",
    "subcategories_formal_logic":"0.3968",
    "subcategories_high_school_european_history":"0.6909",
    "subcategories_high_school_us_history":"0.7059",
    "subcategories_high_school_world_history":"0.7511",
    "subcategories_international_law":"0.7438",
    "subcategories_jurisprudence":"0.6852",
    "subcategories_logical_fallacies":"0.6810",
    "subcategories_moral_disputes":"0.6156",
    "subcategories_moral_scenarios":"0.2872",
    "subcategories_philosophy":"0.6302",
    "subcategories_prehistory":"0.6605",
    "subcategories_professional_law":"0.4224",
    "subcategories_world_religions":"0.7310",
    "subcategories_other":"0.6411",
    "subcategories_business_ethics":"0.5900",
    "subcategories_clinical_knowledge":"0.6377",
    "subcategories_college_medicine":"0.5376",
    "subcategories_global_facts":"0.2500",
    "subcategories_human_aging":"0.6413",
    "subcategories_management":"0.7379",
    "subcategories_marketing":"0.8077",
    "subcategories_medical_genetics":"0.7100",
    "subcategories_miscellaneous":"0.7561",
    "subcategories_nutrition":"0.6667",
    "subcategories_professional_accounting":"0.4539",
    "subcategories_professional_medicine":"0.5846",
    "subcategories_virology":"0.5060",
    "subcategories_social_sciences":"0.6529",
    "subcategories_econometrics":"0.3509",
    "subcategories_high_school_geography":"0.7071",
    "subcategories_high_school_government_and_politics":"0.7617",
    "subcategories_high_school_macroeconomics":"0.5590",
    "subcategories_high_school_microeconomics":"0.5924",
    "subcategories_high_school_psychology":"0.7651",
    "subcategories_human_sexuality":"0.6641",
    "subcategories_professional_psychology":"0.5931",
    "subcategories_public_relations":"0.6364",
    "subcategories_security_studies":"0.6245",
    "subcategories_sociology":"0.7612",
    "subcategories_us_foreign_policy":"0.8000",
    "subcategories_stem":"0.4805",
    "subcategories_abstract_algebra":"0.3500",
    "subcategories_anatomy":"0.6074",
    "subcategories_astronomy":"0.6118",
    "subcategories_college_biology":"0.6389",
    "subcategories_college_chemistry":"0.4500",
    "subcategories_college_computer_science":"0.5200",
    "subcategories_college_mathematics":"0.3700",
    "subcategories_college_physics":"0.3627",
    "subcategories_computer_security":"0.7300",
    "subcategories_conceptual_physics":"0.4936",
    "subcategories_electrical_engineering":"0.6000",
    "subcategories_elementary_mathematics":"0.3598",
    "subcategories_high_school_biology":"0.6484",
    "subcategories_high_school_chemistry":"0.4581",
    "subcategories_high_school_computer_science":"0.5900",
    "subcategories_high_school_mathematics":"0.3259",
    "subcategories_high_school_physics":"0.3775",
    "subcategories_high_school_statistics":"0.4074",
    "subcategories_machine_learning":"0.3929",
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"truthfulqa_mc2",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.3918",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"veritas_qa_ca",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.1360",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"veritas_qa_es",
    "word_perplexity,none":null,
    "byte_perplexity,none":null,
    "bits_per_byte,none":null,
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":"0.1360",
    "acc_norm":"N\/A",
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  },
  {
    "model":"meta-llama\/Llama-3.2-3B",
    "model_size":"3b",
    "task":"wikitext",
    "word_perplexity,none":"9.5372",
    "byte_perplexity,none":"1.5246",
    "bits_per_byte,none":"0.6084",
    "perplexity":null,
    "word_perplexity":null,
    "bits_per_byte":null,
    "prompt_level_strict_acc,none":null,
    "prompt_level_strict_acc_stderr,none":null,
    "inst_level_strict_acc,none":null,
    "prompt_level_loose_acc,none":null,
    "prompt_level_loose_acc_stderr,none":null,
    "inst_level_loose_acc,none":null,
    "exact_match,strict-match":null,
    "exact_match_stderr,strict-match":null,
    "exact_match,flexible-extract":null,
    "exact_match_stderr,flexible-extract":null,
    "accuracy":null,
    "acc_norm":null,
    "category_STEM":null,
    "category_Humanities":null,
    "category_Social_Sciences":null,
    "category_Other":null,
    "subcategories_humanities":null,
    "subcategories_formal_logic":null,
    "subcategories_high_school_european_history":null,
    "subcategories_high_school_us_history":null,
    "subcategories_high_school_world_history":null,
    "subcategories_international_law":null,
    "subcategories_jurisprudence":null,
    "subcategories_logical_fallacies":null,
    "subcategories_moral_disputes":null,
    "subcategories_moral_scenarios":null,
    "subcategories_philosophy":null,
    "subcategories_prehistory":null,
    "subcategories_professional_law":null,
    "subcategories_world_religions":null,
    "subcategories_other":null,
    "subcategories_business_ethics":null,
    "subcategories_clinical_knowledge":null,
    "subcategories_college_medicine":null,
    "subcategories_global_facts":null,
    "subcategories_human_aging":null,
    "subcategories_management":null,
    "subcategories_marketing":null,
    "subcategories_medical_genetics":null,
    "subcategories_miscellaneous":null,
    "subcategories_nutrition":null,
    "subcategories_professional_accounting":null,
    "subcategories_professional_medicine":null,
    "subcategories_virology":null,
    "subcategories_social_sciences":null,
    "subcategories_econometrics":null,
    "subcategories_high_school_geography":null,
    "subcategories_high_school_government_and_politics":null,
    "subcategories_high_school_macroeconomics":null,
    "subcategories_high_school_microeconomics":null,
    "subcategories_high_school_psychology":null,
    "subcategories_human_sexuality":null,
    "subcategories_professional_psychology":null,
    "subcategories_public_relations":null,
    "subcategories_security_studies":null,
    "subcategories_sociology":null,
    "subcategories_us_foreign_policy":null,
    "subcategories_stem":null,
    "subcategories_abstract_algebra":null,
    "subcategories_anatomy":null,
    "subcategories_astronomy":null,
    "subcategories_college_biology":null,
    "subcategories_college_chemistry":null,
    "subcategories_college_computer_science":null,
    "subcategories_college_mathematics":null,
    "subcategories_college_physics":null,
    "subcategories_computer_security":null,
    "subcategories_conceptual_physics":null,
    "subcategories_electrical_engineering":null,
    "subcategories_elementary_mathematics":null,
    "subcategories_high_school_biology":null,
    "subcategories_high_school_chemistry":null,
    "subcategories_high_school_computer_science":null,
    "subcategories_high_school_mathematics":null,
    "subcategories_high_school_physics":null,
    "subcategories_high_school_statistics":null,
    "subcategories_machine_learning":null,
    "subcategories_business":null,
    "subcategories_medical":null
  }
]