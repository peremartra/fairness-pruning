{
  "metadata": {
    "model_name": "BSC-LT/salamandra-2b",
    "started_at": "2025-11-28T23:33:35.133565",
    "last_updated": "2025-12-06T17:51:21.840644",
    "completed": true,
    "completed_at": "2025-12-06T17:51:21.848119"
  },
  "results": {
    "wikitext": {
      "word_perplexity,none": "11.8901",
      "byte_perplexity,none": "1.5888",
      "bits_per_byte,none": "0.6679"
    },
    "lambada_openai": {
      "perplexity": "7.27",
      "word_perplexity": "0.00",
      "bits_per_byte": "0.0000"
    },
    "ifeval": {
      "prompt_level_strict_acc,none": "0.0832",
      "prompt_level_strict_acc_stderr,none": "0.0119",
      "inst_level_strict_acc,none": "0.1691",
      "prompt_level_loose_acc,none": "0.0850",
      "prompt_level_loose_acc_stderr,none": "0.0120",
      "inst_level_loose_acc,none": "0.1739"
    },
    "gsm8k": {
      "exact_match,strict-match": "0.0000",
      "exact_match_stderr,strict-match": "0.0000",
      "exact_match,flexible-extract": "0.0174",
      "exact_match_stderr,flexible-extract": "0.0036"
    },
    "mmlu": {
      "accuracy": "0.2512",
      "acc_norm": "N/A",
      "category_STEM": "0.2419",
      "category_Humanities": "0.2534",
      "category_Social_Sciences": "0.2561",
      "category_Other": "0.2574",
      "subcategories": {
        "humanities": "0.2493",
        "formal_logic": "0.2381",
        "high_school_european_history": "0.2364",
        "high_school_us_history": "0.2647",
        "high_school_world_history": "0.2616",
        "international_law": "0.2231",
        "jurisprudence": "0.2593",
        "logical_fallacies": "0.2393",
        "moral_disputes": "0.2254",
        "moral_scenarios": "0.2380",
        "philosophy": "0.2765",
        "prehistory": "0.2778",
        "professional_law": "0.2438",
        "world_religions": "0.3099",
        "other": "0.2678",
        "business_ethics": "0.2100",
        "clinical_knowledge": "0.2868",
        "college_medicine": "0.1734",
        "global_facts": "0.3200",
        "human_aging": "0.3363",
        "management": "0.2621",
        "marketing": "0.2735",
        "medical_genetics": "0.2700",
        "miscellaneous": "0.2771",
        "nutrition": "0.2680",
        "professional_accounting": "0.2376",
        "professional_medicine": "0.2243",
        "virology": "0.3193",
        "social_sciences": "0.2431",
        "econometrics": "0.2544",
        "high_school_geography": "0.2121",
        "high_school_government_and_politics": "0.2694",
        "high_school_macroeconomics": "0.2256",
        "high_school_microeconomics": "0.2017",
        "high_school_psychology": "0.2477",
        "human_sexuality": "0.2366",
        "professional_psychology": "0.2533",
        "public_relations": "0.3273",
        "security_studies": "0.2327",
        "sociology": "0.2388",
        "us_foreign_policy": "0.2700",
        "stem": "0.2429",
        "abstract_algebra": "0.2100",
        "anatomy": "0.1852",
        "astronomy": "0.2105",
        "college_biology": "0.2083",
        "college_chemistry": "0.2400",
        "college_computer_science": "0.2800",
        "college_mathematics": "0.2700",
        "college_physics": "0.1863",
        "computer_security": "0.2800",
        "conceptual_physics": "0.3277",
        "electrical_engineering": "0.2345",
        "elementary_mathematics": "0.2407",
        "high_school_biology": "0.2129",
        "high_school_chemistry": "0.1970",
        "high_school_computer_science": "0.3000",
        "high_school_mathematics": "0.2778",
        "high_school_physics": "0.2384",
        "high_school_statistics": "0.2639",
        "machine_learning": "0.2321"
      }
    },
    "arc_challenge": {
      "accuracy": "0.3498",
      "acc_norm": "0.3737"
    },
    "hellaswag": {
      "accuracy": "0.4715",
      "acc_norm": "0.6281"
    },
    "truthfulqa_mc2": {
      "accuracy": "0.3593",
      "acc_norm": "N/A"
    },
    "global_mmlu_es": {
      "accuracy": "0.2752",
      "acc_norm": "N/A",
      "subcategories": {
        "business": "0.3276",
        "humanities": "0.2255",
        "medical": "0.2500",
        "other": "0.4464",
        "social_sciences": "0.2059",
        "stem": "0.1957"
      }
    },
    "arc_es": {
      "accuracy": "0.2769",
      "acc_norm": "0.3188"
    },
    "hellaswag_es": {
      "accuracy": "0.4203",
      "acc_norm": "0.5218"
    },
    "belebele_spa_Latn": {
      "accuracy": "0.2544",
      "acc_norm": "0.2544"
    },
    "veritas_qa_es": {
      "accuracy": "0.2125",
      "acc_norm": "N/A"
    },
    "veritas_qa_ca": {
      "accuracy": "0.2068",
      "acc_norm": "N/A"
    }
  },
  "pending_tasks": [],
  "failed_tasks": []
}