{
  "metadata": {
    "model_name": "meta-llama/Llama-3.2-1B",
    "started_at": "2025-11-28T23:29:15.654688",
    "last_updated": "2025-12-06T17:52:06.311664",
    "completed": true,
    "completed_at": "2025-12-06T17:52:06.319012"
  },
  "results": {
    "wikitext": {
      "word_perplexity,none": "11.9853",
      "byte_perplexity,none": "1.5912",
      "bits_per_byte,none": "0.6701"
    },
    "lambada_openai": {
      "perplexity": "5.43",
      "word_perplexity": "0.00",
      "bits_per_byte": "0.0000"
    },
    "ifeval": {
      "prompt_level_strict_acc,none": "0.0998",
      "prompt_level_strict_acc_stderr,none": "0.0129",
      "inst_level_strict_acc,none": "0.1475",
      "prompt_level_loose_acc,none": "0.1128",
      "prompt_level_loose_acc_stderr,none": "0.0136",
      "inst_level_loose_acc,none": "0.1607"
    },
    "gsm8k": {
      "exact_match,strict-match": "0.0553",
      "exact_match_stderr,strict-match": "0.0063",
      "exact_match,flexible-extract": "0.0584",
      "exact_match_stderr,flexible-extract": "0.0065"
    },
    "mmlu": {
      "accuracy": "0.3198",
      "acc_norm": "N/A",
      "category_STEM": "0.2964",
      "category_Humanities": "0.3199",
      "category_Social_Sciences": "0.3408",
      "category_Other": "0.3249",
      "subcategories": {
        "humanities": "0.2927",
        "formal_logic": "0.2222",
        "high_school_european_history": "0.3879",
        "high_school_us_history": "0.3039",
        "high_school_world_history": "0.3207",
        "international_law": "0.4463",
        "jurisprudence": "0.3333",
        "logical_fallacies": "0.2515",
        "moral_disputes": "0.2919",
        "moral_scenarios": "0.2380",
        "philosophy": "0.3248",
        "prehistory": "0.3920",
        "professional_law": "0.2666",
        "world_religions": "0.3801",
        "other": "0.3589",
        "business_ethics": "0.3200",
        "clinical_knowledge": "0.3358",
        "college_medicine": "0.2832",
        "global_facts": "0.2000",
        "human_aging": "0.3767",
        "management": "0.3204",
        "marketing": "0.4017",
        "medical_genetics": "0.4100",
        "miscellaneous": "0.4202",
        "nutrition": "0.3791",
        "professional_accounting": "0.2660",
        "professional_medicine": "0.3162",
        "virology": "0.4036",
        "social_sciences": "0.3247",
        "econometrics": "0.2719",
        "high_school_geography": "0.3737",
        "high_school_government_and_politics": "0.3472",
        "high_school_macroeconomics": "0.2282",
        "high_school_microeconomics": "0.2941",
        "high_school_psychology": "0.3578",
        "human_sexuality": "0.2977",
        "professional_psychology": "0.3088",
        "public_relations": "0.2909",
        "security_studies": "0.3796",
        "sociology": "0.3333",
        "us_foreign_policy": "0.5300",
        "stem": "0.2927",
        "abstract_algebra": "0.3000",
        "anatomy": "0.3630",
        "astronomy": "0.2500",
        "college_biology": "0.2778",
        "college_chemistry": "0.2200",
        "college_computer_science": "0.2900",
        "college_mathematics": "0.2700",
        "college_physics": "0.2157",
        "computer_security": "0.5200",
        "conceptual_physics": "0.3191",
        "electrical_engineering": "0.2621",
        "elementary_mathematics": "0.2407",
        "high_school_biology": "0.3129",
        "high_school_chemistry": "0.2611",
        "high_school_computer_science": "0.3000",
        "high_school_mathematics": "0.2741",
        "high_school_physics": "0.2583",
        "high_school_statistics": "0.3750",
        "machine_learning": "0.3214"
      }
    },
    "arc_challenge": {
      "accuracy": "0.3148",
      "acc_norm": "0.3720"
    },
    "hellaswag": {
      "accuracy": "0.4810",
      "acc_norm": "0.6419"
    },
    "truthfulqa_mc2": {
      "accuracy": "0.3854",
      "acc_norm": "N/A"
    },
    "global_mmlu_es": {
      "accuracy": "0.3509",
      "acc_norm": "N/A",
      "subcategories": {
        "business": "0.3793",
        "humanities": "0.3431",
        "medical": "0.3611",
        "other": "0.3571",
        "social_sciences": "0.3824",
        "stem": "0.2826"
      }
    },
    "arc_es": {
      "accuracy": "0.2564",
      "acc_norm": "0.3000"
    },
    "hellaswag_es": {
      "accuracy": "0.3731",
      "acc_norm": "0.4731"
    },
    "belebele_spa_Latn": {
      "accuracy": "0.3233",
      "acc_norm": "0.3233"
    },
    "veritas_qa_es": {
      "accuracy": "0.2408",
      "acc_norm": "N/A"
    },
    "veritas_qa_ca": {
      "accuracy": "0.2351",
      "acc_norm": "N/A"
    }
  },
  "pending_tasks": [],
  "failed_tasks": []
}