{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/fairness-pruning/blob/main/notebooks/02_Evaluate_MBBQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# Fairness Pruning Research - MBBQ (EsBBQ) Evaluation\n",
        "## 02 - Multilingual BBQ Benchmark for Spanish Bias Detection\n",
        "\n",
        "### Establishing Bias Performance using EsBBQ (MBBQ) for Spanish Bias Mitigation Research\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/â­_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/fairness-pruning](https://github.com/peremartra/fairness-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 or A100\n",
        "\n",
        "**Models to Evaluate:**\n",
        "* Llama-3.2-1B (base)\n",
        "* Llama-3.2-3B (base)\n",
        "* Salamandra-2B (base)\n",
        "\n",
        "**EsBBQ Categories:**\n",
        "* Age, Disability Status, Gender, LGBTQIA+, Nationality\n",
        "* Physical Appearance, Race/Ethnicity, Religion, SES, Spanish Region\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This notebook evaluates ONLY base models (no pruning applied) on Spanish bias benchmarks. For English BBQ evaluation, see `02_Evaluate_BBQ.ipynb`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAo67s0lIvXF"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWIHQuIGIvXG"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG2nO7YpIvXG"
      },
      "outputs": [],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQKRdEHcSFYK"
      },
      "outputs": [],
      "source": [
        "# Download EsBBQ task YAML files from GitHub repository\n",
        "import os\n",
        "os.makedirs('custom_tasks/esbbq', exist_ok=True)\n",
        "\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_age.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_disabilitystatus.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_gender.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_lgbtqia.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_nationality.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_physicalappearance.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_raceethnicity.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_religion.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_ses.yaml\n",
        "!wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_spanishregion.yaml\n",
        "\n",
        "# Verify downloads\n",
        "yaml_count = len([f for f in os.listdir('custom_tasks/esbbq') if f.endswith('.yaml')])\n",
        "print(f\"âœ… Downloaded {yaml_count} EsBBQ YAML files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_lmeval_path"
      },
      "outputs": [],
      "source": [
        "# Set LMEVAL_INCLUDE_PATH to load custom EsBBQ tasks\n",
        "import os\n",
        "os.environ[\"LMEVAL_INCLUDE_PATH\"] = \"/content/custom_tasks/esbbq\"\n",
        "print(f\"âœ… LMEVAL_INCLUDE_PATH set to: {os.environ['LMEVAL_INCLUDE_PATH']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOqtWXbhrRf"
      },
      "source": [
        "# 2. Helper Functions\n",
        "\n",
        "Utility functions for automatic checkpoint path generation and model size detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper_functions"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def get_model_size(model_name: str) -> str:\n",
        "    \"\"\"Extract model size identifier from HuggingFace model name.\n",
        "\n",
        "    Examples:\n",
        "        \"meta-llama/Llama-3.2-1B\" â†’ \"1b\"\n",
        "        \"meta-llama/Llama-3.2-3B-Instruct\" â†’ \"3b_instruct\"\n",
        "        \"BSC-LT/salamandra-2b\" â†’ \"2b\"\n",
        "    \"\"\"\n",
        "    match = re.search(r'(\\d+\\.?\\d*)[Bb]', model_name)\n",
        "    if not match:\n",
        "        return \"unknown\"\n",
        "\n",
        "    size = match.group(1).replace('.', '_') + \"b\"\n",
        "    if \"instruct\" in model_name.lower():\n",
        "        size += \"_instruct\"\n",
        "\n",
        "    return size.lower()\n",
        "\n",
        "def get_checkpoint_path(model_name: str, base_dir: str) -> str:\n",
        "    \"\"\"Generate checkpoint path with size-based subdirectory.\n",
        "\n",
        "    Args:\n",
        "        model_name: Full HuggingFace model identifier\n",
        "        base_dir: Base directory for checkpoints\n",
        "\n",
        "    Returns:\n",
        "        Full path to checkpoint file\n",
        "    \"\"\"\n",
        "    model_size = get_model_size(model_name)\n",
        "    safe_name = model_name.replace('/', '_').replace('-', '_').lower()\n",
        "    checkpoint_dir = os.path.join(base_dir, model_size)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    return os.path.join(checkpoint_dir, f\"{safe_name}.json\")\n",
        "\n",
        "# Test with example models\n",
        "print(\"Testing helper functions:\")\n",
        "print(\"-\" * 70)\n",
        "test_models = [\n",
        "    \"BSC-LT/salamandra-2b\",\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    \"meta-llama/Llama-3.2-3B\"\n",
        "]\n",
        "for model_id in test_models:\n",
        "    size = get_model_size(model_id)\n",
        "    print(f\"{model_id:<50} â†’ {size}\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goJTl4XJQjL6"
      },
      "source": [
        "# 3. Configuration & Evaluation Plan (MBBQ/EsBBQ)\n",
        "\n",
        "Configure paths, select EsBBQ tasks, and list the models we will evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG53MfrJQjL6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from IPython.display import display\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG,\n",
        "    run_robust_evaluation,\n",
        "    load_or_create_model,\n",
        "    clear_gpu_cache,\n",
        "    format_results_table,\n",
        "    get_model_stats,\n",
        " )\n",
        "\n",
        "# Paths (Drive recommended in Colab)\n",
        "CHECKPOINT_BASE_DIR = \"/content/drive/MyDrive/fair_pruning/checkpoints_mbbq\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/fair_pruning/results\"\n",
        "Path(CHECKPOINT_BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# EsBBQ (MBBQ) task list - all categories in one group (0-shot)\n",
        "# Using task group name that encompasses all EsBBQ subtasks\n",
        "MBBQ_TASKS = [\n",
        "    {\"name\": \"esbbq_age\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_disabilitystatus\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_gender\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_lgbtqia\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_nationality\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_physicalappearance\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_raceethnicity\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_religion\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_ses\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_spanishregion\", \"num_fewshot\": 0}\n",
        "]\n",
        "\n",
        "# De-duplicate models from EXPERIMENT_CONFIG\n",
        "unique_models = list(dict.fromkeys([cfg[\"base_model\"] for cfg in EXPERIMENT_CONFIG]))\n",
        "\n",
        "logging.getLogger(\"lm_eval\").setLevel(logging.INFO)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“Š EVALUATION PLAN: MBBQ (EsBBQ) - Spanish Bias Benchmark\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Checkpoints: {CHECKPOINT_BASE_DIR}\")\n",
        "print(f\"Results: {RESULTS_DIR}\")\n",
        "print(f\"Tasks: {[task['name'] for task in MBBQ_TASKS]}\")\n",
        "print(f\"Limit per dataset: 100 (quick test mode)\")\n",
        "print(\"Models:\")\n",
        "for m in unique_models:\n",
        "    print(f\" - {m}\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYSCTWcyQjL6"
      },
      "source": [
        "# 4. Run MBBQ Evaluation\n",
        "\n",
        "Evaluate each base model on EsBBQ tasks with checkpoint/resume and raw result saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx4LW8uCQjL6"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ðŸš€ STARTING MBBQ (EsBBQ) EVALUATION (Hugging Face Native Loading)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "all_model_results = {}\n",
        "\n",
        "for idx, model_id in enumerate(unique_models, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ“Š MODEL {idx}/{len(unique_models)}: {model_id}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    try:\n",
        "        # Generate checkpoint path\n",
        "        checkpoint_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n",
        "\n",
        "        # Load model directly from Hugging Face\n",
        "        print(f\"ðŸ“¥ Loading directly from Hugging Face Hub...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        print(\"âœ… Model loaded successfully (Native HF)\")\n",
        "\n",
        "        # Show statistics\n",
        "        stats = get_model_stats(model)\n",
        "        print(f\"ðŸ“ˆ Params: {stats['total_parameters']:,} | Size: {stats['size_gb']:.2f} GB\")\n",
        "        print(f\"ðŸ“ Checkpoint: {checkpoint_path}\\n\")\n",
        "\n",
        "        # Run evaluation with limit=100 for quick testing\n",
        "        from utils import model_evaluation\n",
        "        \n",
        "        # Determine raw results directory\n",
        "        raw_results_dir = os.path.join(\n",
        "            os.path.dirname(checkpoint_path),\n",
        "            \"results\", \"lm_evals\"\n",
        "        )\n",
        "        os.makedirs(raw_results_dir, exist_ok=True)\n",
        "        \n",
        "        print(f\"ðŸ“Š Running evaluation with limit=100 per task...\")\n",
        "        print(f\"ðŸ’¾ Raw results will be saved to: {raw_results_dir}\\n\")\n",
        "        \n",
        "        results = model_evaluation(\n",
        "            model_obj=model,\n",
        "            tokenizer=tokenizer,\n",
        "            tasks=MBBQ_TASKS,\n",
        "            limit=100,\n",
        "            save_raw_results=True,\n",
        "            raw_results_dir=raw_results_dir\n",
        "        )\n",
        "        \n",
        "        # Save results to checkpoint format\n",
        "        checkpoint_data = {\n",
        "            \"metadata\": {\n",
        "                \"model_name\": model_id,\n",
        "                \"started_at\": datetime.now().isoformat(),\n",
        "                \"completed\": True,\n",
        "                \"completed_at\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"results\": results,\n",
        "            \"pending_tasks\": [],\n",
        "            \"failed_tasks\": []\n",
        "        }\n",
        "        \n",
        "        with open(checkpoint_path, 'w') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "        \n",
        "        all_model_results[model_id] = results\n",
        "        print(f\"\\nâœ… Completed: {model_id}\")\n",
        "        print(\"Results Preview (first few tasks):\")\n",
        "        preview_results = {k: v for i, (k, v) in enumerate(results.items()) if i < 3}\n",
        "        print(format_results_table(preview_results))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR evaluating {model_id}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        if 'model' in locals(): del model\n",
        "        if 'tokenizer' in locals(): del tokenizer\n",
        "        clear_gpu_cache()\n",
        "        continue\n",
        "\n",
        "    # Memory cleanup\n",
        "    del model\n",
        "    del tokenizer\n",
        "    clear_gpu_cache()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"âœ… MBBQ EVALUATION COMPLETE: {len(all_model_results)}/{len(unique_models)} models\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20JPHfQQjL7"
      },
      "source": [
        "# 5. Consolidate MBBQ Results\n",
        "\n",
        "Load checkpoint files, flatten metrics, and export combined MBBQ results in CSV/JSON format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R9TwH-iQjL7"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def flatten_metrics(metrics, prefix=''):\n",
        "    \"\"\"Recursively flatten nested metric dictionaries.\"\"\"\n",
        "    flat = {}\n",
        "    for k, v in metrics.items():\n",
        "        if isinstance(v, dict):\n",
        "            flat.update(flatten_metrics(v, prefix=f\"{prefix}{k}_\"))\n",
        "        else:\n",
        "            flat[f\"{prefix}{k}\"] = v\n",
        "    return flat\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\" CONSOLIDATING MBBQ RESULTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "checkpoint_files = glob.glob(f\"{CHECKPOINT_BASE_DIR}/**/*.json\", recursive=True)\n",
        "print(f\"Total checkpoint JSONs found: {len(checkpoint_files)}\")\n",
        "\n",
        "consolidated_data = []\n",
        "\n",
        "for json_path in sorted(checkpoint_files):\n",
        "    # Skip raw lm-eval dumps (only process model checkpoints)\n",
        "    if \"lm_evals\" in json_path:\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        metadata = data.get(\"metadata\", {})\n",
        "        model_name = metadata.get(\"model_name\", \"Unknown\")\n",
        "\n",
        "        # Extract model size\n",
        "        model_size = get_model_size(model_name)\n",
        "\n",
        "        results = data.get(\"results\", {})\n",
        "        if not results:\n",
        "            continue\n",
        "\n",
        "        for task_name, metrics in results.items():\n",
        "            row = {\n",
        "                \"model\": model_name,\n",
        "                \"model_size\": model_size,\n",
        "                \"task\": task_name,\n",
        "            }\n",
        "            row.update(flatten_metrics(metrics))\n",
        "            consolidated_data.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ Error reading {json_path}: {e}\")\n",
        "\n",
        "if consolidated_data:\n",
        "    df = pd.DataFrame(consolidated_data)\n",
        "    df = df.sort_values(by=[\"model\", \"task\"]).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Consolidated {len(df)} task results from {df['model'].nunique()} models\")\n",
        "    display(df.head(10))\n",
        "\n",
        "    # Save timestamped and latest versions\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"{RESULTS_DIR}/base_models_mbbq_results_{timestamp}.csv\"\n",
        "    latest_csv = f\"{RESULTS_DIR}/base_models_mbbq_results_latest.csv\"\n",
        "    json_path = f\"{RESULTS_DIR}/base_models_mbbq_results_{timestamp}.json\"\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_csv(latest_csv, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(\"\\nðŸ’¾ MBBQ results saved:\")\n",
        "    print(f\"   {csv_path}\")\n",
        "    print(f\"   {latest_csv}\")\n",
        "    print(f\"   {json_path}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No MBBQ results found to consolidate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-EPjbK_QjL7"
      },
      "source": [
        "# 6. Summary Analysis\n",
        "\n",
        "Quick per-model accuracy summary for MBBQ and file inventory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDBqo_5iQjL7"
      },
      "outputs": [],
      "source": [
        "# Generate summary statistics\n",
        "summary_df = None\n",
        "if 'df' in locals() and not df.empty:\n",
        "    summaries = []\n",
        "    for model_name, model_df in df.groupby('model'):\n",
        "        acc_series = pd.to_numeric(model_df.get('accuracy'), errors='coerce').dropna()\n",
        "        summaries.append({\n",
        "            \"model\": model_name,\n",
        "            \"model_size\": model_df.get('model_size', pd.Series(['unknown'])).iloc[0],\n",
        "            \"avg_accuracy\": acc_series.mean() if len(acc_series) else None,\n",
        "            \"tasks_completed\": len(model_df),\n",
        "            \"categories_evaluated\": len(model_df['task'].unique())\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summaries)\n",
        "    summary_df = summary_df.sort_values(\"model\").reset_index(drop=True)\n",
        "    \n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"ðŸ“ˆ MBBQ SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    summary_csv = f\"{RESULTS_DIR}/base_models_mbbq_summary_{timestamp}.csv\"\n",
        "    summary_df.to_csv(summary_csv, index=False)\n",
        "    print(f\"\\nðŸ’¾ Summary saved: {summary_csv}\\n\")\n",
        "else:\n",
        "    print(\"No consolidated data available for summary.\")\n",
        "\n",
        "# List all generated files\n",
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“ GENERATED FILES\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if 'csv_path' in locals() and os.path.exists(csv_path):\n",
        "    print(f\"  âœ… {csv_path}\")\n",
        "if 'latest_csv' in locals() and os.path.exists(latest_csv):\n",
        "    print(f\"  âœ… {latest_csv}\")\n",
        "if 'json_path' in locals() and os.path.exists(json_path):\n",
        "    print(f\"  âœ… {json_path}\")\n",
        "if 'summary_csv' in locals() and os.path.exists(summary_csv):\n",
        "    print(f\"  âœ… {summary_csv}\")\n",
        "\n",
        "print(\"\\nCheckpoints (first 10):\")\n",
        "if 'checkpoint_files' in locals():\n",
        "    model_checkpoints = [f for f in sorted(checkpoint_files) if \"lm_evals\" not in f]\n",
        "    for f in model_checkpoints[:10]:\n",
        "        print(f\"  âœ… {f}\")\n",
        "    if len(model_checkpoints) > 10:\n",
        "        print(f\"  ... and {len(model_checkpoints) - 10} more\")\n",
        "\n",
        "print(\"\\nRaw LM-Eval dumps (first 10):\")\n",
        "if 'checkpoint_files' in locals():\n",
        "    raw_dumps = [f for f in sorted(checkpoint_files) if \"lm_evals\" in f]\n",
        "    for f in raw_dumps[:10]:\n",
        "        print(f\"  âœ… {f}\")\n",
        "    if len(raw_dumps) > 10:\n",
        "        print(f\"  ... and {len(raw_dumps) - 10} more\")\n",
        "\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bias_analysis"
      },
      "source": [
        "# 7. Bias Analysis by Category\n",
        "\n",
        "Analyze bias metrics across EsBBQ categories for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "category_analysis"
      },
      "outputs": [],
      "source": [
        "if 'df' in locals() and not df.empty:\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"ðŸ“Š BIAS ANALYSIS BY CATEGORY\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Extract category from task name (e.g., esbbq_age -> age)\n",
        "    df['category'] = df['task'].str.replace('esbbq_', '')\n",
        "    \n",
        "    # Pivot table: models as rows, categories as columns\n",
        "    if 'accuracy' in df.columns:\n",
        "        pivot_df = df.pivot_table(\n",
        "            index='model',\n",
        "            columns='category',\n",
        "            values='accuracy',\n",
        "            aggfunc='first'\n",
        "        )\n",
        "        \n",
        "        print(\"\\nAccuracy by Category:\")\n",
        "        display(pivot_df)\n",
        "        \n",
        "        # Save category breakdown\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        category_csv = f\"{RESULTS_DIR}/mbbq_category_breakdown_{timestamp}.csv\"\n",
        "        pivot_df.to_csv(category_csv)\n",
        "        print(f\"\\nðŸ’¾ Category breakdown saved: {category_csv}\")\n",
        "    \n",
        "    # Show bias score distribution if available\n",
        "    bias_cols = [col for col in df.columns if 'bias' in col.lower()]\n",
        "    if bias_cols:\n",
        "        print(\"\\nðŸ“Š Available bias metrics:\")\n",
        "        for col in bias_cols:\n",
        "            print(f\"   - {col}\")\n",
        "else:\n",
        "    print(\"No data available for category analysis.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
