{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/fairness-pruning/blob/main/notebooks/02_Evaluate_MBBQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# Fairness Pruning Research - MBBQ (EsBBQ) Evaluation\n",
        "## 02 - Multilingual BBQ Benchmark for Spanish Bias Detection\n",
        "\n",
        "### Establishing Bias Performance using EsBBQ (MBBQ) for Spanish Bias Mitigation Research\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/‚≠ê_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/fairness-pruning](https://github.com/peremartra/fairness-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 or A100\n",
        "\n",
        "**Models to Evaluate:**\n",
        "* Llama-3.2-1B (base)\n",
        "* Llama-3.2-3B (base)\n",
        "* Salamandra-2B (base)\n",
        "\n",
        "**EsBBQ Categories:**\n",
        "* Age, Disability Status, Gender, LGBTQIA+, Nationality\n",
        "* Physical Appearance, Race/Ethnicity, Religion, SES, Spanish Region\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This notebook evaluates ONLY base models (no pruning applied) on Spanish bias benchmarks. For English BBQ evaluation, see `02_Evaluate_BBQ.ipynb`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nAo67s0lIvXF"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GWIHQuIGIvXG"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DG2nO7YpIvXG",
        "outputId": "cf29ae39-e353-49e3-c3a1-aa34e080ec85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"‚úÖ utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gQKRdEHcSFYK",
        "outputId": "fb394db7-67bb-4d0e-bfc9-9361e5c09548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Local files not found. Downloading from GitHub...\n",
            "‚úÖ Downloaded 10 EsBBQ YAML files from GitHub.\n"
          ]
        }
      ],
      "source": [
        "# Download EsBBQ task YAML files from GitHub repository\n",
        "import os\n",
        "import shutil\n",
        "os.makedirs('custom_tasks/esbbq', exist_ok=True)\n",
        "\n",
        "# Check if we are in the repo and can use local clean files\n",
        "local_source = '../custom_tasks/esbbq'\n",
        "files_to_copy = [\n",
        "    'esbbq_age.yaml', 'esbbq_disabilitystatus.yaml', 'esbbq_gender.yaml',\n",
        "    'esbbq_lgbtqia.yaml', 'esbbq_nationality.yaml', 'esbbq_physicalappearance.yaml',\n",
        "    'esbbq_raceethnicity.yaml', 'esbbq_religion.yaml', 'esbbq_ses.yaml', 'esbbq_spanishregion.yaml'\n",
        "]\n",
        "\n",
        "used_local = False\n",
        "if os.path.exists(local_source):\n",
        "    print(\"üìÇ Found local custom_tasks in ../custom_tasks. Copying to notebook dir...\")\n",
        "    for fname in files_to_copy:\n",
        "        src = os.path.join(local_source, fname)\n",
        "        dst = os.path.join('custom_tasks/esbbq', fname)\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dst)\n",
        "            used_local = True\n",
        "\n",
        "if not used_local:\n",
        "    print(\"üåê Local files not found. Downloading from GitHub...\")\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_age.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_disabilitystatus.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_gender.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_lgbtqia.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_nationality.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_physicalappearance.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_raceethnicity.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_religion.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_ses.yaml\n",
        "    !wget -q -P custom_tasks/esbbq https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/esbbq/esbbq_spanishregion.yaml\n",
        "\n",
        "# Verify downloads (or copies)\n",
        "yaml_count = len([f for f in os.listdir('custom_tasks/esbbq') if f.endswith('.yaml')])\n",
        "if used_local:\n",
        "    print(f\"‚úÖ Copied {yaml_count} EsBBQ YAML files from local repo.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Downloaded {yaml_count} EsBBQ YAML files from GitHub.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTALACI√ìN SIMPLE DE EsBBQ (archivos ya limpios en el repo)\n",
        "import os\n",
        "import shutil\n",
        "import lm_eval\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"üì¶ INSTALLING EsBBQ TASKS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# 1. Localizar directorios\n",
        "lib_path = os.path.dirname(lm_eval.__file__)\n",
        "target_dir = os.path.join(lib_path, \"tasks\", \"esbbq\")\n",
        "\n",
        "# 2. Limpiar y recrear directorio\n",
        "if os.path.exists(target_dir):\n",
        "    shutil.rmtree(target_dir)\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "print(f\"üìç TASKS DIRECTORY: {target_dir}\\n\")\n",
        "\n",
        "# 3. Copiar todos los YAMLs (YA est√°n limpios)\n",
        "yaml_files = [f for f in os.listdir('custom_tasks/esbbq') if f.endswith('.yaml')]\n",
        "\n",
        "for yaml_file in yaml_files:\n",
        "    src = os.path.join('custom_tasks/esbbq', yaml_file)\n",
        "    dst = os.path.join(target_dir, yaml_file)\n",
        "    shutil.copy(src, dst)\n",
        "    print(f\"   ‚úÖ {yaml_file}\")\n",
        "\n",
        "print(f\"\\nüöÄ OK! {len(yaml_files)} EsBBQ tasks installed.\")"
      ],
      "metadata": {
        "id": "XFuUtNZjZyRJ",
        "outputId": "2f7d4c94-078b-4074-bf99-7c247f2b4bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üì¶ INSTALLING EsBBQ TASKS\n",
            "======================================================================\n",
            "üìç TASKS DIRECTORY: /usr/local/lib/python3.12/dist-packages/lm_eval/tasks/esbbq\n",
            "\n",
            "   ‚úÖ esbbq_ses.yaml\n",
            "   ‚úÖ esbbq_gender.yaml\n",
            "   ‚úÖ esbbq_physicalappearance.yaml\n",
            "   ‚úÖ esbbq_disabilitystatus.yaml\n",
            "   ‚úÖ esbbq_raceethnicity.yaml\n",
            "   ‚úÖ esbbq_nationality.yaml\n",
            "   ‚úÖ esbbq_spanishregion.yaml\n",
            "   ‚úÖ esbbq_lgbtqia.yaml\n",
            "   ‚úÖ esbbq_age.yaml\n",
            "   ‚úÖ esbbq_religion.yaml\n",
            "\n",
            "üöÄ OK! 10 EsBBQ tasks installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOqtWXbhrRf"
      },
      "source": [
        "# 2. Helper Functions\n",
        "\n",
        "Utility functions for automatic checkpoint path generation and model size detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "helper_functions",
        "outputId": "7c3a9fbe-e69f-4af3-9017-4e9c9b2406bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing helper functions:\n",
            "----------------------------------------------------------------------\n",
            "BSC-LT/salamandra-2b                               ‚Üí 2b\n",
            "meta-llama/Llama-3.2-1B                            ‚Üí 1b\n",
            "meta-llama/Llama-3.2-3B                            ‚Üí 3b\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def get_model_size(model_name: str) -> str:\n",
        "    \"\"\"Extract model size identifier from HuggingFace model name.\n",
        "\n",
        "    Examples:\n",
        "        \"meta-llama/Llama-3.2-1B\" ‚Üí \"1b\"\n",
        "        \"meta-llama/Llama-3.2-3B-Instruct\" ‚Üí \"3b_instruct\"\n",
        "        \"BSC-LT/salamandra-2b\" ‚Üí \"2b\"\n",
        "    \"\"\"\n",
        "    match = re.search(r'(\\d+\\.?\\d*)[Bb]', model_name)\n",
        "    if not match:\n",
        "        return \"unknown\"\n",
        "\n",
        "    size = match.group(1).replace('.', '_') + \"b\"\n",
        "    if \"instruct\" in model_name.lower():\n",
        "        size += \"_instruct\"\n",
        "\n",
        "    return size.lower()\n",
        "\n",
        "def get_checkpoint_path(model_name: str, base_dir: str) -> str:\n",
        "    \"\"\"Generate checkpoint path with size-based subdirectory.\n",
        "\n",
        "    Args:\n",
        "        model_name: Full HuggingFace model identifier\n",
        "        base_dir: Base directory for checkpoints\n",
        "\n",
        "    Returns:\n",
        "        Full path to checkpoint file\n",
        "    \"\"\"\n",
        "    model_size = get_model_size(model_name)\n",
        "    safe_name = model_name.replace('/', '_').replace('-', '_').lower()\n",
        "    checkpoint_dir = os.path.join(base_dir, model_size)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    return os.path.join(checkpoint_dir, f\"{safe_name}.json\")\n",
        "\n",
        "# Test with example models\n",
        "print(\"Testing helper functions:\")\n",
        "print(\"-\" * 70)\n",
        "test_models = [\n",
        "    \"BSC-LT/salamandra-2b\",\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    \"meta-llama/Llama-3.2-3B\"\n",
        "]\n",
        "for model_id in test_models:\n",
        "    size = get_model_size(model_id)\n",
        "    print(f\"{model_id:<50} ‚Üí {size}\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goJTl4XJQjL6"
      },
      "source": [
        "# 3. Configuration & Evaluation Plan (MBBQ/EsBBQ)\n",
        "\n",
        "Configure paths, select EsBBQ tasks, and list the models we will evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EG53MfrJQjL6",
        "outputId": "554c5ff6-bbe7-409c-b7f4-a6cfe3d3770e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üìä EVALUATION PLAN: MBBQ (EsBBQ) - Spanish Bias Benchmark\n",
            "======================================================================\n",
            "Checkpoints: /content/drive/MyDrive/fair_pruning/checkpoints_mbbq\n",
            "Results: /content/drive/MyDrive/fair_pruning/results\n",
            "Tasks: ['esbbq_age', 'esbbq_disabilitystatus', 'esbbq_gender', 'esbbq_lgbtqia', 'esbbq_nationality', 'esbbq_physicalappearance', 'esbbq_raceethnicity', 'esbbq_religion', 'esbbq_ses', 'esbbq_spanishregion']\n",
            "Limit per dataset: 100 (quick test mode)\n",
            "Models:\n",
            " - BSC-LT/salamandra-2b\n",
            " - meta-llama/Llama-3.2-1B\n",
            " - meta-llama/Llama-3.2-3B\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from IPython.display import display\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG,\n",
        "    run_robust_evaluation,\n",
        "    load_or_create_model,\n",
        "    clear_gpu_cache,\n",
        "    format_results_table,\n",
        "    get_model_stats,\n",
        " )\n",
        "\n",
        "# Paths (Drive recommended in Colab)\n",
        "CHECKPOINT_BASE_DIR = \"/content/drive/MyDrive/fair_pruning/checkpoints_mbbq\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/fair_pruning/results\"\n",
        "Path(CHECKPOINT_BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# EsBBQ (MBBQ) task list - all categories in one group (0-shot)\n",
        "# Using task group name that encompasses all EsBBQ subtasks\n",
        "MBBQ_TASKS = [\n",
        "    {\"name\": \"esbbq_age\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_disabilitystatus\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_gender\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_lgbtqia\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_nationality\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_physicalappearance\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_raceethnicity\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_religion\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_ses\", \"num_fewshot\": 0},\n",
        "    {\"name\": \"esbbq_spanishregion\", \"num_fewshot\": 0}\n",
        "]\n",
        "\n",
        "# De-duplicate models from EXPERIMENT_CONFIG\n",
        "unique_models = list(dict.fromkeys([cfg[\"base_model\"] for cfg in EXPERIMENT_CONFIG]))\n",
        "\n",
        "logging.getLogger(\"lm_eval\").setLevel(logging.INFO)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"üìä EVALUATION PLAN: MBBQ (EsBBQ) - Spanish Bias Benchmark\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Checkpoints: {CHECKPOINT_BASE_DIR}\")\n",
        "print(f\"Results: {RESULTS_DIR}\")\n",
        "print(f\"Tasks: {[task['name'] for task in MBBQ_TASKS]}\")\n",
        "print(f\"Limit per dataset: 100 (quick test mode)\")\n",
        "print(\"Models:\")\n",
        "for m in unique_models:\n",
        "    print(f\" - {m}\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYSCTWcyQjL6"
      },
      "source": [
        "# 4. Run MBBQ Evaluation\n",
        "\n",
        "Evaluate each base model on EsBBQ tasks with checkpoint/resume and raw result saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Sx4LW8uCQjL6",
        "outputId": "b8add772-2cc3-4197-9573-ce34cbb63202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6149d623bd06472d9bbf949326dd211d",
            "522a25bda86543dc9202a256549e7ba8",
            "f7e4c10df07b43a9bdf12a603eaaffb9",
            "e2fd402390974efd974e6bdde026c478",
            "8eb0352a348d4cbcb60c5340976e184c",
            "a47e74220ebd42a6b7fe254f6e46029f",
            "c495e7b2ec9842f5a33d074fa205645d",
            "35be25a8e67949d78a217bb09f0cfd8d",
            "76ad2791067c443cb435059a062eb9e6",
            "5a76a06cfe2748e29984e7a621979d70",
            "0233fc34260a44c3bb0dd3458f3c6912",
            "4168749455e94282bf406f178c730627",
            "f6e05bce9adb4d5c969cb006832f3402",
            "a2491460f6b44ed092c55aca6cdf42ad",
            "6ae76a9682444b43b2d84f4ca344ff14",
            "fc28ce6bf30c452a93cf7d19d15c1b97",
            "2d05f6ec28dd41c1818ac15d98a5b987",
            "5f1e4e884e114f05918a4ee7ee6155e6",
            "46be42f5354a4126bffef54b21bb2a98",
            "a4d70bebf55840c1b4bcad8294401a24",
            "b98bc9457537446da1a40d3ef44b42d1",
            "12cc8089638740daa87b9b7690d7cdce",
            "21f38d2ea1244cbfa6747ff5743e3285",
            "8f4a91a62b464bbdb06549e32416206a",
            "20819f4334eb4e1eb91ff38618897e30",
            "26ea652fd2c54bb195001ed5ceafc215",
            "a92a71b563d64d4a8e5bf7ec2b93ff73",
            "8c75b482d0eb4c03b91a89c8ebbca768",
            "95cd741f194345e0a9607d16074a35ab",
            "b8345258a5364c35b7ade78423d79033",
            "be9867c31eff4e158e7235dfacffe2b1",
            "1e42177d42124c75bb30df4e5052c8da",
            "fbc7d07499f54e93a548b7adae9e4220"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ STARTING MBBQ (EsBBQ) EVALUATION (Hugging Face Native Loading)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìä MODEL 1/3: BSC-LT/salamandra-2b\n",
            "======================================================================\n",
            "\n",
            "üì• Loading directly from Hugging Face Hub...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Using pre-initialized model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully (Native HF)\n",
            "üìà Params: 2,253,490,176 | Size: 4.20 GB\n",
            "üìÅ Checkpoint: /content/drive/MyDrive/fair_pruning/checkpoints_mbbq/2b/bsc_lt_salamandra_2b.json\n",
            "\n",
            "üìä Running evaluation with limit=100 per task...\n",
            "üíæ Raw results will be saved to: /content/drive/MyDrive/fair_pruning/checkpoints_mbbq/2b/results/lm_evals\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Starting lm-eval on model 'BSC-LT/salamandra-2b'\n",
            "Tasks: ['esbbq_age', 'esbbq_disabilitystatus', 'esbbq_gender', 'esbbq_lgbtqia', 'esbbq_nationality', 'esbbq_physicalappearance', 'esbbq_raceethnicity', 'esbbq_religion', 'esbbq_ses', 'esbbq_spanishregion'] (limit=100)\n",
            "Few-shot config: {'esbbq_age': 0, 'esbbq_disabilitystatus': 0, 'esbbq_gender': 0, 'esbbq_lgbtqia': 0, 'esbbq_nationality': 0, 'esbbq_physicalappearance': 0, 'esbbq_raceethnicity': 0, 'esbbq_religion': 0, 'esbbq_ses': 0, 'esbbq_spanishregion': 0}\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/2784 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6149d623bd06472d9bbf949326dd211d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùå ERROR evaluating BSC-LT/salamandra-2b: [{'expected': SplitInfo(name='test', num_bytes=2207498, num_examples=2784, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=2253670, num_examples=2832, shard_lengths=None, dataset_name='es_bbq')}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2246913835.py\", line 47, in <cell line: 0>\n",
            "    results = model_evaluation(\n",
            "              ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/utils.py\", line 402, in model_evaluation\n",
            "    results = evaluator.simple_evaluate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/utils.py\", line 458, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/evaluator.py\", line 283, in simple_evaluate\n",
            "    task_dict = get_task_dict(\n",
            "                ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 645, in get_task_dict\n",
            "    task_name_from_string_dict = task_manager.load_task_or_group(\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 427, in load_task_or_group\n",
            "    collections.ChainMap(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 429, in <lambda>\n",
            "    lambda task: self._load_individual_task_or_group(task),\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 327, in _load_individual_task_or_group\n",
            "    return _load_task(task_config, task=name_or_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 287, in _load_task\n",
            "    task_object = ConfigurableTask(config=config)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 865, in __init__\n",
            "    self.download(self.config.dataset_kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 998, in download\n",
            "    self.dataset = datasets.load_dataset(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 1412, in load_dataset\n",
            "    builder_instance.download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 894, in download_and_prepare\n",
            "    self._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 988, in _download_and_prepare\n",
            "    verify_splits(self.info.splits, split_dict)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/info_utils.py\", line 77, in verify_splits\n",
            "    raise NonMatchingSplitsSizesError(str(bad_splits))\n",
            "datasets.exceptions.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='test', num_bytes=2207498, num_examples=2784, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=2253670, num_examples=2832, shard_lengths=None, dataset_name='es_bbq')}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "======================================================================\n",
            "üìä MODEL 2/3: meta-llama/Llama-3.2-1B\n",
            "======================================================================\n",
            "\n",
            "üì• Loading directly from Hugging Face Hub...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Using pre-initialized model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully (Native HF)\n",
            "üìà Params: 1,235,814,400 | Size: 2.30 GB\n",
            "üìÅ Checkpoint: /content/drive/MyDrive/fair_pruning/checkpoints_mbbq/1b/meta_llama_llama_3.2_1b.json\n",
            "\n",
            "üìä Running evaluation with limit=100 per task...\n",
            "üíæ Raw results will be saved to: /content/drive/MyDrive/fair_pruning/checkpoints_mbbq/1b/results/lm_evals\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Starting lm-eval on model 'meta-llama/Llama-3.2-1B'\n",
            "Tasks: ['esbbq_age', 'esbbq_disabilitystatus', 'esbbq_gender', 'esbbq_lgbtqia', 'esbbq_nationality', 'esbbq_physicalappearance', 'esbbq_raceethnicity', 'esbbq_religion', 'esbbq_ses', 'esbbq_spanishregion'] (limit=100)\n",
            "Few-shot config: {'esbbq_age': 0, 'esbbq_disabilitystatus': 0, 'esbbq_gender': 0, 'esbbq_lgbtqia': 0, 'esbbq_nationality': 0, 'esbbq_physicalappearance': 0, 'esbbq_raceethnicity': 0, 'esbbq_religion': 0, 'esbbq_ses': 0, 'esbbq_spanishregion': 0}\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/2784 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4168749455e94282bf406f178c730627"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùå ERROR evaluating meta-llama/Llama-3.2-1B: [{'expected': SplitInfo(name='test', num_bytes=2207498, num_examples=2784, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=2253670, num_examples=2832, shard_lengths=None, dataset_name='es_bbq')}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2246913835.py\", line 47, in <cell line: 0>\n",
            "    results = model_evaluation(\n",
            "              ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/utils.py\", line 402, in model_evaluation\n",
            "    results = evaluator.simple_evaluate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/utils.py\", line 458, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/evaluator.py\", line 283, in simple_evaluate\n",
            "    task_dict = get_task_dict(\n",
            "                ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 645, in get_task_dict\n",
            "    task_name_from_string_dict = task_manager.load_task_or_group(\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 427, in load_task_or_group\n",
            "    collections.ChainMap(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 429, in <lambda>\n",
            "    lambda task: self._load_individual_task_or_group(task),\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 327, in _load_individual_task_or_group\n",
            "    return _load_task(task_config, task=name_or_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/tasks/__init__.py\", line 287, in _load_task\n",
            "    task_object = ConfigurableTask(config=config)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 865, in __init__\n",
            "    self.download(self.config.dataset_kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 998, in download\n",
            "    self.dataset = datasets.load_dataset(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 1412, in load_dataset\n",
            "    builder_instance.download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 894, in download_and_prepare\n",
            "    self._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 988, in _download_and_prepare\n",
            "    verify_splits(self.info.splits, split_dict)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/info_utils.py\", line 77, in verify_splits\n",
            "    raise NonMatchingSplitsSizesError(str(bad_splits))\n",
            "datasets.exceptions.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='test', num_bytes=2207498, num_examples=2784, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=2253670, num_examples=2832, shard_lengths=None, dataset_name='es_bbq')}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "======================================================================\n",
            "üìä MODEL 3/3: meta-llama/Llama-3.2-3B\n",
            "======================================================================\n",
            "\n",
            "üì• Loading directly from Hugging Face Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21f38d2ea1244cbfa6747ff5743e3285"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2246913835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Load model directly from Hugging Face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üì• Loading directly from Hugging Face Hub...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5046\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5047\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5048\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5049\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5050\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5468\u001b[0;31m                 \u001b[0m_error_msgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shard_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5469\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error_msgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         disk_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mto_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üöÄ STARTING MBBQ (EsBBQ) EVALUATION (Hugging Face Native Loading)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "all_model_results = {}\n",
        "\n",
        "for idx, model_id in enumerate(unique_models, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä MODEL {idx}/{len(unique_models)}: {model_id}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    try:\n",
        "        # Generate checkpoint path\n",
        "        checkpoint_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n",
        "\n",
        "        # Load model directly from Hugging Face\n",
        "        print(f\"üì• Loading directly from Hugging Face Hub...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        print(\"‚úÖ Model loaded successfully (Native HF)\")\n",
        "\n",
        "        # Show statistics\n",
        "        stats = get_model_stats(model)\n",
        "        print(f\"üìà Params: {stats['total_parameters']:,} | Size: {stats['size_gb']:.2f} GB\")\n",
        "        print(f\"üìÅ Checkpoint: {checkpoint_path}\\n\")\n",
        "\n",
        "        # Run evaluation with limit=100 for quick testing\n",
        "        from utils import model_evaluation\n",
        "\n",
        "        # Determine raw results directory\n",
        "        raw_results_dir = os.path.join(\n",
        "            os.path.dirname(checkpoint_path),\n",
        "            \"results\", \"lm_evals\"\n",
        "        )\n",
        "        os.makedirs(raw_results_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"üìä Running evaluation with limit=100 per task...\")\n",
        "        print(f\"üíæ Raw results will be saved to: {raw_results_dir}\\n\")\n",
        "\n",
        "        results = model_evaluation(\n",
        "            model_obj=model,\n",
        "            tokenizer=tokenizer,\n",
        "            tasks=MBBQ_TASKS,\n",
        "            limit=100,\n",
        "            save_raw_results=True,\n",
        "            raw_results_dir=raw_results_dir\n",
        "        )\n",
        "\n",
        "        # Save results to checkpoint format\n",
        "        checkpoint_data = {\n",
        "            \"metadata\": {\n",
        "                \"model_name\": model_id,\n",
        "                \"started_at\": datetime.now().isoformat(),\n",
        "                \"completed\": True,\n",
        "                \"completed_at\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"results\": results,\n",
        "            \"pending_tasks\": [],\n",
        "            \"failed_tasks\": []\n",
        "        }\n",
        "\n",
        "        with open(checkpoint_path, 'w') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "        all_model_results[model_id] = results\n",
        "        print(f\"\\n‚úÖ Completed: {model_id}\")\n",
        "        print(\"Results Preview (first few tasks):\")\n",
        "        preview_results = {k: v for i, (k, v) in enumerate(results.items()) if i < 3}\n",
        "        print(format_results_table(preview_results))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR evaluating {model_id}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        if 'model' in locals(): del model\n",
        "        if 'tokenizer' in locals(): del tokenizer\n",
        "        clear_gpu_cache()\n",
        "        continue\n",
        "\n",
        "    # Memory cleanup\n",
        "    del model\n",
        "    del tokenizer\n",
        "    clear_gpu_cache()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ MBBQ EVALUATION COMPLETE: {len(all_model_results)}/{len(unique_models)} models\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20JPHfQQjL7"
      },
      "source": [
        "# 5. Consolidate MBBQ Results\n",
        "\n",
        "Load checkpoint files, flatten metrics, and export combined MBBQ results in CSV/JSON format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R9TwH-iQjL7"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def flatten_metrics(metrics, prefix=''):\n",
        "    \"\"\"Recursively flatten nested metric dictionaries.\"\"\"\n",
        "    flat = {}\n",
        "    for k, v in metrics.items():\n",
        "        if isinstance(v, dict):\n",
        "            flat.update(flatten_metrics(v, prefix=f\"{prefix}{k}_\"))\n",
        "        else:\n",
        "            flat[f\"{prefix}{k}\"] = v\n",
        "    return flat\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\" CONSOLIDATING MBBQ RESULTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "checkpoint_files = glob.glob(f\"{CHECKPOINT_BASE_DIR}/**/*.json\", recursive=True)\n",
        "print(f\"Total checkpoint JSONs found: {len(checkpoint_files)}\")\n",
        "\n",
        "consolidated_data = []\n",
        "\n",
        "for json_path in sorted(checkpoint_files):\n",
        "    # Skip raw lm-eval dumps (only process model checkpoints)\n",
        "    if \"lm_evals\" in json_path:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        metadata = data.get(\"metadata\", {})\n",
        "        model_name = metadata.get(\"model_name\", \"Unknown\")\n",
        "\n",
        "        # Extract model size\n",
        "        model_size = get_model_size(model_name)\n",
        "\n",
        "        results = data.get(\"results\", {})\n",
        "        if not results:\n",
        "            continue\n",
        "\n",
        "        for task_name, metrics in results.items():\n",
        "            row = {\n",
        "                \"model\": model_name,\n",
        "                \"model_size\": model_size,\n",
        "                \"task\": task_name,\n",
        "            }\n",
        "            row.update(flatten_metrics(metrics))\n",
        "            consolidated_data.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Error reading {json_path}: {e}\")\n",
        "\n",
        "if consolidated_data:\n",
        "    df = pd.DataFrame(consolidated_data)\n",
        "    df = df.sort_values(by=[\"model\", \"task\"]).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nüìä Consolidated {len(df)} task results from {df['model'].nunique()} models\")\n",
        "    display(df.head(10))\n",
        "\n",
        "    # Save timestamped and latest versions\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"{RESULTS_DIR}/base_models_mbbq_results_{timestamp}.csv\"\n",
        "    latest_csv = f\"{RESULTS_DIR}/base_models_mbbq_results_latest.csv\"\n",
        "    json_path = f\"{RESULTS_DIR}/base_models_mbbq_results_{timestamp}.json\"\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_csv(latest_csv, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(\"\\nüíæ MBBQ results saved:\")\n",
        "    print(f\"   {csv_path}\")\n",
        "    print(f\"   {latest_csv}\")\n",
        "    print(f\"   {json_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No MBBQ results found to consolidate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-EPjbK_QjL7"
      },
      "source": [
        "# 6. Summary Analysis\n",
        "\n",
        "Quick per-model accuracy summary for MBBQ and file inventory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDBqo_5iQjL7"
      },
      "outputs": [],
      "source": [
        "# Generate summary statistics\n",
        "summary_df = None\n",
        "if 'df' in locals() and not df.empty:\n",
        "    summaries = []\n",
        "    for model_name, model_df in df.groupby('model'):\n",
        "        acc_series = pd.to_numeric(model_df.get('accuracy'), errors='coerce').dropna()\n",
        "        summaries.append({\n",
        "            \"model\": model_name,\n",
        "            \"model_size\": model_df.get('model_size', pd.Series(['unknown'])).iloc[0],\n",
        "            \"avg_accuracy\": acc_series.mean() if len(acc_series) else None,\n",
        "            \"tasks_completed\": len(model_df),\n",
        "            \"categories_evaluated\": len(model_df['task'].unique())\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summaries)\n",
        "    summary_df = summary_df.sort_values(\"model\").reset_index(drop=True)\n",
        "\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"üìà MBBQ SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    summary_csv = f\"{RESULTS_DIR}/base_models_mbbq_summary_{timestamp}.csv\"\n",
        "    summary_df.to_csv(summary_csv, index=False)\n",
        "    print(f\"\\nüíæ Summary saved: {summary_csv}\\n\")\n",
        "else:\n",
        "    print(\"No consolidated data available for summary.\")\n",
        "\n",
        "# List all generated files\n",
        "print(f\"{'='*70}\")\n",
        "print(\"üìÅ GENERATED FILES\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if 'csv_path' in locals() and os.path.exists(csv_path):\n",
        "    print(f\"  ‚úÖ {csv_path}\")\n",
        "if 'latest_csv' in locals() and os.path.exists(latest_csv):\n",
        "    print(f\"  ‚úÖ {latest_csv}\")\n",
        "if 'json_path' in locals() and os.path.exists(json_path):\n",
        "    print(f\"  ‚úÖ {json_path}\")\n",
        "if 'summary_csv' in locals() and os.path.exists(summary_csv):\n",
        "    print(f\"  ‚úÖ {summary_csv}\")\n",
        "\n",
        "print(\"\\nCheckpoints (first 10):\")\n",
        "if 'checkpoint_files' in locals():\n",
        "    model_checkpoints = [f for f in sorted(checkpoint_files) if \"lm_evals\" not in f]\n",
        "    for f in model_checkpoints[:10]:\n",
        "        print(f\"  ‚úÖ {f}\")\n",
        "    if len(model_checkpoints) > 10:\n",
        "        print(f\"  ... and {len(model_checkpoints) - 10} more\")\n",
        "\n",
        "print(\"\\nRaw LM-Eval dumps (first 10):\")\n",
        "if 'checkpoint_files' in locals():\n",
        "    raw_dumps = [f for f in sorted(checkpoint_files) if \"lm_evals\" in f]\n",
        "    for f in raw_dumps[:10]:\n",
        "        print(f\"  ‚úÖ {f}\")\n",
        "    if len(raw_dumps) > 10:\n",
        "        print(f\"  ... and {len(raw_dumps) - 10} more\")\n",
        "\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bias_analysis"
      },
      "source": [
        "# 7. Bias Analysis by Category\n",
        "\n",
        "Analyze bias metrics across EsBBQ categories for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "category_analysis"
      },
      "outputs": [],
      "source": [
        "if 'df' in locals() and not df.empty:\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"üìä BIAS ANALYSIS BY CATEGORY\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Extract category from task name (e.g., esbbq_age -> age)\n",
        "    df['category'] = df['task'].str.replace('esbbq_', '')\n",
        "\n",
        "    # Pivot table: models as rows, categories as columns\n",
        "    if 'accuracy' in df.columns:\n",
        "        pivot_df = df.pivot_table(\n",
        "            index='model',\n",
        "            columns='category',\n",
        "            values='accuracy',\n",
        "            aggfunc='first'\n",
        "        )\n",
        "\n",
        "        print(\"\\nAccuracy by Category:\")\n",
        "        display(pivot_df)\n",
        "\n",
        "        # Save category breakdown\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        category_csv = f\"{RESULTS_DIR}/mbbq_category_breakdown_{timestamp}.csv\"\n",
        "        pivot_df.to_csv(category_csv)\n",
        "        print(f\"\\nüíæ Category breakdown saved: {category_csv}\")\n",
        "\n",
        "    # Show bias score distribution if available\n",
        "    bias_cols = [col for col in df.columns if 'bias' in col.lower()]\n",
        "    if bias_cols:\n",
        "        print(\"\\nüìä Available bias metrics:\")\n",
        "        for col in bias_cols:\n",
        "            print(f\"   - {col}\")\n",
        "else:\n",
        "    print(\"No data available for category analysis.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6149d623bd06472d9bbf949326dd211d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_522a25bda86543dc9202a256549e7ba8",
              "IPY_MODEL_f7e4c10df07b43a9bdf12a603eaaffb9",
              "IPY_MODEL_e2fd402390974efd974e6bdde026c478"
            ],
            "layout": "IPY_MODEL_8eb0352a348d4cbcb60c5340976e184c"
          }
        },
        "522a25bda86543dc9202a256549e7ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a47e74220ebd42a6b7fe254f6e46029f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c495e7b2ec9842f5a33d074fa205645d",
            "value": "Generating‚Äátest‚Äásplit:‚Äá"
          }
        },
        "f7e4c10df07b43a9bdf12a603eaaffb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35be25a8e67949d78a217bb09f0cfd8d",
            "max": 2784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76ad2791067c443cb435059a062eb9e6",
            "value": 2784
          }
        },
        "e2fd402390974efd974e6bdde026c478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a76a06cfe2748e29984e7a621979d70",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0233fc34260a44c3bb0dd3458f3c6912",
            "value": "‚Äá2832/?‚Äá[00:00&lt;00:00,‚Äá120348.42‚Äáexamples/s]"
          }
        },
        "8eb0352a348d4cbcb60c5340976e184c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47e74220ebd42a6b7fe254f6e46029f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c495e7b2ec9842f5a33d074fa205645d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35be25a8e67949d78a217bb09f0cfd8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ad2791067c443cb435059a062eb9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a76a06cfe2748e29984e7a621979d70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0233fc34260a44c3bb0dd3458f3c6912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4168749455e94282bf406f178c730627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6e05bce9adb4d5c969cb006832f3402",
              "IPY_MODEL_a2491460f6b44ed092c55aca6cdf42ad",
              "IPY_MODEL_6ae76a9682444b43b2d84f4ca344ff14"
            ],
            "layout": "IPY_MODEL_fc28ce6bf30c452a93cf7d19d15c1b97"
          }
        },
        "f6e05bce9adb4d5c969cb006832f3402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d05f6ec28dd41c1818ac15d98a5b987",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5f1e4e884e114f05918a4ee7ee6155e6",
            "value": "Generating‚Äátest‚Äásplit:‚Äá"
          }
        },
        "a2491460f6b44ed092c55aca6cdf42ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46be42f5354a4126bffef54b21bb2a98",
            "max": 2784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4d70bebf55840c1b4bcad8294401a24",
            "value": 2784
          }
        },
        "6ae76a9682444b43b2d84f4ca344ff14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b98bc9457537446da1a40d3ef44b42d1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_12cc8089638740daa87b9b7690d7cdce",
            "value": "‚Äá2832/?‚Äá[00:00&lt;00:00,‚Äá105129.52‚Äáexamples/s]"
          }
        },
        "fc28ce6bf30c452a93cf7d19d15c1b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d05f6ec28dd41c1818ac15d98a5b987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1e4e884e114f05918a4ee7ee6155e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46be42f5354a4126bffef54b21bb2a98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4d70bebf55840c1b4bcad8294401a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b98bc9457537446da1a40d3ef44b42d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12cc8089638740daa87b9b7690d7cdce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21f38d2ea1244cbfa6747ff5743e3285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f4a91a62b464bbdb06549e32416206a",
              "IPY_MODEL_20819f4334eb4e1eb91ff38618897e30",
              "IPY_MODEL_26ea652fd2c54bb195001ed5ceafc215"
            ],
            "layout": "IPY_MODEL_a92a71b563d64d4a8e5bf7ec2b93ff73"
          }
        },
        "8f4a91a62b464bbdb06549e32416206a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c75b482d0eb4c03b91a89c8ebbca768",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_95cd741f194345e0a9607d16074a35ab",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá‚Äá‚Äá0%"
          }
        },
        "20819f4334eb4e1eb91ff38618897e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8345258a5364c35b7ade78423d79033",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be9867c31eff4e158e7235dfacffe2b1",
            "value": 0
          }
        },
        "26ea652fd2c54bb195001ed5ceafc215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e42177d42124c75bb30df4e5052c8da",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fbc7d07499f54e93a548b7adae9e4220",
            "value": "‚Äá0/2‚Äá[00:01&lt;?,‚Äá?it/s]"
          }
        },
        "a92a71b563d64d4a8e5bf7ec2b93ff73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c75b482d0eb4c03b91a89c8ebbca768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95cd741f194345e0a9607d16074a35ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8345258a5364c35b7ade78423d79033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9867c31eff4e158e7235dfacffe2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e42177d42124c75bb30df4e5052c8da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc7d07499f54e93a548b7adae9e4220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}