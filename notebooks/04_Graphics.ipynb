{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e435b8c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/peremartra/fairness-pruning/blob/main/notebooks/04_Graphics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Fairness Pruning Research ‚Äì Benchmark Visualizations\n",
    "## 04 ‚Äì Comparative Graphics for Base Model Results\n",
    "\n",
    "### Visual Analytics of Baseline Performance (Unpruned Models)\n",
    "by [Pere Martra](https://github.com/peremartra)\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/‚≠ê_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
    "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
    "\n",
    "**Repository:** [github.com/peremartra/fairness-pruning](https://github.com/peremartra/fairness-pruning)\n",
    "\n",
    "---\n",
    "**Recommended Colab Environment:** GPU (L4 / A100) for faster model loading ‚Äì plotting itself does not require GPU.\n",
    "\n",
    "**Models Visualized:**\n",
    "* Llama-3.2-1B (base)\n",
    "* Salamandra-2B (base)\n",
    "* Llama-3.2-3B (base)\n",
    "\n",
    "**Benchmark Groups:**\n",
    "* Language Modeling: WikiText, Lambada-OpenAI\n",
    "* English Reasoning & Knowledge: ARC-Challenge, HellaSwag, TruthfulQA, GSM8K, IFEval, MMLU\n",
    "* Spanish / Cross-Lingual: ARC-ES, HellaSwag-ES, Belebele, MMLU-ES\n",
    "* Deep Dives: MMLU (57 subcategories) and MMLU_ES (6 categories)\n",
    "* Cross-Lingual Comparisons: EN ‚àí ES gaps (ARC, HellaSwag, MMLU)\n",
    "\n",
    "**Primary Outputs:**\n",
    "1. Grouped bar charts by task type\n",
    "2. Perplexity comparison (WikiText / Lambada)\n",
    "3. MMLU & MMLU_ES category bars\n",
    "4. English vs Spanish performance gaps\n",
    "5. Summary metrics (mean accuracy, mean perplexity, global MMLU)\n",
    "\n",
    "**Objective:**\n",
    "Provide a clear synthesized visual reference of baseline (unpruned) results to support: (a) comparison across model sizes, (b) task sensitivity analysis, and (c) pre-pruning bias diagnostics.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Consistent style with evaluation notebooks\n",
    "- ‚úÖ Automatic download of result JSONs from GitHub\n",
    "- ‚úÖ Semantic benchmark grouping\n",
    "- ‚úÖ Exportable figures (PNG/PDF) with timestamp\n",
    "- ‚úÖ EN‚ÄìES gap table for cross-lingual diagnostics\n",
    "- ‚úÖ Easy extension (heatmaps / subcategory rankings)\n",
    "\n",
    "**Suggested Future Extensions:**\n",
    "- Heatmap of MMLU subcategories sorted by difficulty\n",
    "- Efficiency curves (accuracy vs parameters)\n",
    "- Variance visualization if repeated runs are added\n",
    "\n",
    "> Note: All results here are from unpruned base models. Comparisons with pruned versions will appear in later notebooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if running in Colab (optional)\n",
    "# !pip install -q seaborn matplotlib pandas\n",
    "\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Style configuration\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "PALETTE = {\n",
    "    \"Llama-3.2-1B\": \"#1f77b4\",      # blue\n",
    "    \"Salamandra-2B\": \"#ff7f0e\",     # orange\n",
    "    \"Llama-3.2-3B\": \"#2ca02c\"       # green\n",
    "}\n",
    "MODEL_FILE_MAP = {\n",
    "    \"Llama-3.2-1B\": \"results/meta_llama_llama_3.2_1b.json\",\n",
    "    \"Salamandra-2B\": \"results/bsc_lt_salamandra_2b.json\",\n",
    "    \"Llama-3.2-3B\": \"results/meta_llama_llama_3.2_3b.json\"\n",
    "}\n",
    "OUTPUT_DIR = Path(\"results/figures\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Imports complete\")\n",
    "print(f\"üìÅ Figures output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download result JSONs from GitHub (ensures availability when running in Colab)\n",
    "RESULTS_REMOTE_BASE = \"https://raw.githubusercontent.com/peremartra/fairness-pruning/main/results\"\n",
    "LOCAL_RESULTS_DIR = Path(\"results\")\n",
    "LOCAL_RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "remote_files = {\n",
    "    \"bsc_lt_salamandra_2b.json\": f\"{RESULTS_REMOTE_BASE}/bsc_lt_salamandra_2b.json\",\n",
    "    \"meta_llama_llama_3.2_1b.json\": f\"{RESULTS_REMOTE_BASE}/meta_llama_llama_3.2_1b.json\",\n",
    "    \"meta_llama_llama_3.2_3b.json\": f\"{RESULTS_REMOTE_BASE}/meta_llama_llama_3.2_3b.json\",\n",
    "}\n",
    "\n",
    "import subprocess\n",
    "for fname, url in remote_files.items():\n",
    "    dest = LOCAL_RESULTS_DIR / fname\n",
    "    if not dest.exists():\n",
    "        try:\n",
    "            subprocess.run([\"wget\", \"-q\", url, \"-O\", str(dest)], check=True)\n",
    "            print(f\"‚úÖ Downloaded {fname}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ùå Failed to download {fname} from {url}\")\n",
    "    else:\n",
    "        print(f\"‚öôÔ∏è Using existing local file: {fname}\")\n",
    "\n",
    "# Verify mapping still valid\n",
    "for model, path in MODEL_FILE_MAP.items():\n",
    "    if not Path(path).exists():\n",
    "        print(f\"‚ö†Ô∏è Expected file missing for {model}: {path}\")\n",
    "    else:\n",
    "        print(f\"üìÅ Ready: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce4e10",
   "metadata": {},
   "source": [
    "# 2. Data Loading & Preprocessing\n",
    "\n",
    "Load JSON results produced by evaluation notebook and convert into structured DataFrames for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06411cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x):\n",
    "    \"\"\"Safely convert value to float; return None if not possible or 'N/A'.\"\"\"\n",
    "    if x in (None, \"N/A\"):\n",
    "        return None\n",
    "    try:\n",
    "        return float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_json(path: str) -> dict:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_all_results(model_file_map=MODEL_FILE_MAP):\n",
    "    data = {}\n",
    "    for model_name, file_path in model_file_map.items():\n",
    "        try:\n",
    "            raw = load_json(file_path)\n",
    "            data[model_name] = raw.get(\"results\", {})\n",
    "            print(f\"‚úÖ Loaded: {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Missing file: {file_path}\")\n",
    "    return data\n",
    "\n",
    "results_raw = load_all_results()\n",
    "\n",
    "# Flatten simple tasks into a DataFrame (excluding deep MMLU variants for separate handling)\n",
    "rows = []\n",
    "for model_name, tasks in results_raw.items():\n",
    "    for task_name, metrics in tasks.items():\n",
    "        if task_name in (\"mmlu\", \"global_mmlu_es\"):\n",
    "            continue  # handled separately\n",
    "        row = {\n",
    "            \"model\": model_name,\n",
    "            \"task\": task_name,\n",
    "            # Common accuracy metrics\n",
    "            \"accuracy\": safe_float(metrics.get(\"accuracy\")),\n",
    "            \"acc_norm\": safe_float(metrics.get(\"acc_norm\")),\n",
    "            # Perplexity metrics\n",
    "            \"perplexity\": safe_float(metrics.get(\"perplexity\")),\n",
    "            \"word_perplexity\": safe_float(metrics.get(\"word_perplexity\")),\n",
    "            \"byte_perplexity\": safe_float(metrics.get(\"byte_perplexity,none\")),\n",
    "        }\n",
    "        # Add task-specific fields if present\n",
    "        # GSM8K\n",
    "        if task_name == \"gsm8k\":\n",
    "            row[\"gsm8k_exact_match_strict\"] = safe_float(metrics.get(\"exact_match,strict-match\"))\n",
    "            row[\"gsm8k_exact_match_flexible\"] = safe_float(metrics.get(\"exact_match,flexible-extract\"))\n",
    "        # IFEval\n",
    "        if task_name == \"ifeval\":\n",
    "            row[\"ifeval_prompt_strict\"] = safe_float(metrics.get(\"prompt_level_strict_acc,none\"))\n",
    "            row[\"ifeval_prompt_loose\"] = safe_float(metrics.get(\"prompt_level_loose_acc,none\"))\n",
    "            row[\"ifeval_inst_strict\"] = safe_float(metrics.get(\"inst_level_strict_acc,none\"))\n",
    "            row[\"ifeval_inst_loose\"] = safe_float(metrics.get(\"inst_level_loose_acc,none\"))\n",
    "        rows.append(row)\n",
    "\n",
    "base_df = pd.DataFrame(rows)\n",
    "print(f\"\\nüìä Base metrics DataFrame shape: {base_df.shape}\")\n",
    "print(base_df.head().to_string())\n",
    "\n",
    "# Prepare MMLU detailed DataFrame\n",
    "mmlu_rows = []\n",
    "for model_name, tasks in results_raw.items():\n",
    "    if \"mmlu\" not in tasks:\n",
    "        continue\n",
    "    mmlu = tasks[\"mmlu\"]\n",
    "    subcats = mmlu.get(\"subcategories\", {})\n",
    "    # Overall + top-level categories\n",
    "    mmlu_rows.append({\n",
    "        \"model\": model_name,\n",
    "        \"type\": \"overall\",\n",
    "        \"category\": \"overall\",\n",
    "        \"score\": safe_float(mmlu.get(\"accuracy\"))\n",
    "    })\n",
    "    for cat_key in [\"category_STEM\", \"category_Humanities\", \"category_Social_Sciences\", \"category_Other\"]:\n",
    "        mmlu_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"top_category\",\n",
    "            \"category\": cat_key.replace(\"category_\", \"\").lower(),\n",
    "            \"score\": safe_float(mmlu.get(cat_key))\n",
    "        })\n",
    "    # Subcategories\n",
    "    for sub_name, value in subcats.items():\n",
    "        mmlu_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"subcategory\",\n",
    "            \"category\": sub_name,\n",
    "            \"score\": safe_float(value)\n",
    "        })\n",
    "\n",
    "mmlu_df = pd.DataFrame(mmlu_rows)\n",
    "print(f\"\\nüìä MMLU DataFrame: {mmlu_df.shape} (including subcategories)\")\n",
    "print(mmlu_df.head().to_string())\n",
    "\n",
    "# Prepare MMLU_ES detailed DataFrame\n",
    "mmlu_es_rows = []\n",
    "for model_name, tasks in results_raw.items():\n",
    "    if \"global_mmlu_es\" not in tasks:\n",
    "        continue\n",
    "    mmlu_es = tasks[\"global_mmlu_es\"]\n",
    "    subcats = mmlu_es.get(\"subcategories\", {})\n",
    "    mmlu_es_rows.append({\n",
    "        \"model\": model_name,\n",
    "        \"type\": \"overall\",\n",
    "        \"category\": \"overall\",\n",
    "        \"score\": safe_float(mmlu_es.get(\"accuracy\"))\n",
    "    })\n",
    "    # Spanish top categories live in subcategories directly\n",
    "    for sub_name, value in subcats.items():\n",
    "        mmlu_es_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"top_category\",\n",
    "            \"category\": sub_name,\n",
    "            \"score\": safe_float(value)\n",
    "        })\n",
    "\n",
    "mmlu_es_df = pd.DataFrame(mmlu_es_rows)\n",
    "print(f\"\\nüìä MMLU_ES DataFrame: {mmlu_es_df.shape}\")\n",
    "print(mmlu_es_df.head().to_string())\n",
    "\n",
    "# Grouping definitions for plots\n",
    "GROUPS = {\n",
    "    \"language_modeling\": [\"wikitext\", \"lambada_openai\"],\n",
    "    \"english_reasoning\": [\"arc_challenge\", \"hellaswag\", \"truthfulqa_mc2\"],\n",
    "    \"spanish_benchmarks\": [\"arc_es\", \"hellaswag_es\", \"belebele_spa_Latn\"],\n",
    "    \"specialized\": [\"gsm8k\", \"ifeval\"],\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Data loading & preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec0379",
   "metadata": {},
   "source": [
    "# 3. Visualization Helpers\n",
    "\n",
    "Utility functions to standardize figure generation (colors, saving, titles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff478fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def save_fig(name: str, fig):\n",
    "    base = OUTPUT_DIR / f\"{name}_{TIMESTAMP}\"\n",
    "    png_path = str(base) + \".png\"\n",
    "    pdf_path = str(base) + \".pdf\"\n",
    "    fig.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "    fig.savefig(pdf_path, bbox_inches='tight')\n",
    "    print(f\"üíæ Saved: {png_path} | {pdf_path}\")\n",
    "\n",
    "\n",
    "def plot_grouped_accuracy(group_key: str):\n",
    "    \"\"\"Plot grouped accuracy metrics for a predefined task group.\"\"\"\n",
    "    tasks = GROUPS[group_key]\n",
    "    subset = base_df[base_df['task'].isin(tasks)].copy()\n",
    "    if subset.empty:\n",
    "        print(f\"‚ö†Ô∏è No data for group '{group_key}'\")\n",
    "        return\n",
    "    # Choose accuracy column (standard 'accuracy' or task-specific)\n",
    "    # For gsm8k & ifeval we will adjust separately\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    width = 0.2\n",
    "    models = subset['model'].unique()\n",
    "    x_positions = range(len(tasks))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = []\n",
    "        for t in tasks:\n",
    "            row = subset[(subset['model'] == model) & (subset['task'] == t)]\n",
    "            if row.empty:\n",
    "                model_data.append(None)\n",
    "                continue\n",
    "            if t == 'gsm8k':\n",
    "                model_data.append(row.iloc[0]['gsm8k_exact_match_strict'])\n",
    "            elif t == 'ifeval':\n",
    "                model_data.append(row.iloc[0]['ifeval_prompt_strict'])\n",
    "            else:\n",
    "                model_data.append(row.iloc[0]['accuracy'])\n",
    "        offsets = [x + (i - (len(models)-1)/2)*width for x in x_positions]\n",
    "        ax.bar(offsets, model_data, width=width, label=model, color=PALETTE.get(model))\n",
    "\n",
    "    ax.set_xticks(list(x_positions))\n",
    "    ax.set_xticklabels(tasks, rotation=15)\n",
    "    ax.set_ylabel('Accuracy / Score')\n",
    "    ax.set_title(f\"Grouped Performance: {group_key.replace('_', ' ').title()}\")\n",
    "    ax.legend(title='Model')\n",
    "    ax.set_ylim(0, 1)\n",
    "    sns.despine()\n",
    "    save_fig(f\"group_{group_key}\", fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_perplexities():\n",
    "    tasks = ['wikitext', 'lambada_openai']\n",
    "    subset = base_df[base_df['task'].isin(tasks)]\n",
    "    if subset.empty:\n",
    "        print(\"‚ö†Ô∏è No perplexity data found\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    width = 0.25\n",
    "    models = subset['model'].unique()\n",
    "    x_positions = range(len(tasks))\n",
    "    for i, model in enumerate(models):\n",
    "        model_values = []\n",
    "        for t in tasks:\n",
    "            row = subset[(subset['model'] == model) & (subset['task'] == t)]\n",
    "            if row.empty:\n",
    "                model_values.append(None)\n",
    "            else:\n",
    "                # Prefer word_perplexity if available\n",
    "                val = row.iloc[0]['word_perplexity'] or row.iloc[0]['perplexity']\n",
    "                model_values.append(val)\n",
    "        offsets = [x + (i - (len(models)-1)/2)*width for x in x_positions]\n",
    "        ax.bar(offsets, model_values, width=width, label=model, color=PALETTE.get(model))\n",
    "\n",
    "    ax.set_xticks(list(x_positions))\n",
    "    ax.set_xticklabels(tasks)\n",
    "    ax.set_ylabel('Word Perplexity (lower is better)')\n",
    "    ax.set_title('Language Modeling Performance')\n",
    "    ax.legend(title='Model')\n",
    "    sns.despine()\n",
    "    save_fig('language_modeling_perplexity', fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_mmlu_categories():\n",
    "    subset = mmlu_df[mmlu_df['type'] == 'top_category']\n",
    "    if subset.empty:\n",
    "        print(\"‚ö†Ô∏è No MMLU category data\")\n",
    "        return\n",
    "    pivot = subset.pivot(index='category', columns='model', values='score')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    pivot.plot(kind='bar', ax=ax, color=[PALETTE.get(m) for m in pivot.columns])\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('MMLU Category Performance')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(title='Model')\n",
    "    sns.despine()\n",
    "    save_fig('mmlu_categories', fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_mmlu_es_categories():\n",
    "    subset = mmlu_es_df[mmlu_es_df['type'] == 'top_category']\n",
    "    if subset.empty:\n",
    "        print(\"‚ö†Ô∏è No MMLU_ES category data\")\n",
    "        return\n",
    "    pivot = subset.pivot(index='category', columns='model', values='score')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    pivot.plot(kind='bar', ax=ax, color=[PALETTE.get(m) for m in pivot.columns])\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('MMLU_ES Category Performance')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(title='Model')\n",
    "    sns.despine()\n",
    "    save_fig('mmlu_es_categories', fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"‚úÖ Visualization helpers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7897825",
   "metadata": {},
   "source": [
    "# 4. Generate Charts\n",
    "\n",
    "Run grouped visualizations and deep dives. Additional fine-grained plots (e.g., subcategory heatmaps) can be added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Modeling Perplexities\n",
    "plot_perplexities()\n",
    "\n",
    "# English Reasoning Group\n",
    "plot_grouped_accuracy('english_reasoning')\n",
    "\n",
    "# Spanish Benchmarks Group\n",
    "plot_grouped_accuracy('spanish_benchmarks')\n",
    "\n",
    "# Specialized (GSM8K + IFEval)\n",
    "plot_grouped_accuracy('specialized')\n",
    "\n",
    "# MMLU categories\n",
    "plot_mmlu_categories()\n",
    "\n",
    "# MMLU_ES categories\n",
    "plot_mmlu_es_categories()\n",
    "\n",
    "print(\"\\n‚úÖ Primary charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a696422f",
   "metadata": {},
   "source": [
    "# 5. Cross-Lingual Comparison\n",
    "\n",
    "Compute deltas between English and Spanish counterparts (ARC, HellaSwag, MMLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8348f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_rows = []\n",
    "for model in MODEL_FILE_MAP.keys():\n",
    "    # ARC\n",
    "    arc_en = base_df[(base_df.model == model) & (base_df.task == 'arc_challenge')]['accuracy']\n",
    "    arc_es = base_df[(base_df.model == model) & (base_df.task == 'arc_es')]['accuracy']\n",
    "    # HellaSwag\n",
    "    hs_en = base_df[(base_df.model == model) & (base_df.task == 'hellaswag')]['accuracy']\n",
    "    hs_es = base_df[(base_df.model == model) & (base_df.task == 'hellaswag_es')]['accuracy']\n",
    "    # MMLU overall\n",
    "    mmlu_overall = mmlu_df[(mmlu_df.model == model) & (mmlu_df.type == 'overall')]['score']\n",
    "    mmlu_es_overall = mmlu_es_df[(mmlu_es_df.model == model) & (mmlu_es_df.type == 'overall')]['score']\n",
    "\n",
    "    cross_rows.append({\n",
    "        'model': model,\n",
    "        'arc_en': arc_en.iloc[0] if not arc_en.empty else None,\n",
    "        'arc_es': arc_es.iloc[0] if not arc_es.empty else None,\n",
    "        'hellaswag_en': hs_en.iloc[0] if not hs_en.empty else None,\n",
    "        'hellaswag_es': hs_es.iloc[0] if not hs_es.empty else None,\n",
    "        'mmlu_en': mmlu_overall.iloc[0] if not mmlu_overall.empty else None,\n",
    "        'mmlu_es': mmlu_es_overall.iloc[0] if not mmlu_es_overall.empty else None,\n",
    "    })\n",
    "\n",
    "cross_df = pd.DataFrame(cross_rows)\n",
    "print(\"Cross-lingual comparison table:\")\n",
    "print(cross_df.to_string(index=False))\n",
    "\n",
    "# Plot deltas\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "index = range(len(cross_df))\n",
    "bar_width = 0.25\n",
    "ax.bar([i - bar_width for i in index], cross_df['arc_en'] - cross_df['arc_es'], width=bar_width, label='ARC Œî (EN-ES)')\n",
    "ax.bar(index, cross_df['hellaswag_en'] - cross_df['hellaswag_es'], width=bar_width, label='HellaSwag Œî (EN-ES)')\n",
    "ax.bar([i + bar_width for i in index], cross_df['mmlu_en'] - cross_df['mmlu_es'], width=bar_width, label='MMLU Œî (EN-ES)')\n",
    "ax.set_xticks(list(index))\n",
    "ax.set_xticklabels(cross_df['model'], rotation=15)\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.set_ylabel('Accuracy Gap (English - Spanish)')\n",
    "ax.set_title('Cross-Lingual Performance Gaps')\n",
    "ax.legend()\n",
    "sns.despine()\n",
    "save_fig('cross_lingual_gaps', fig)\n",
    "plt.close(fig)\n",
    "print(\"‚úÖ Cross-lingual comparison chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b333bc",
   "metadata": {},
   "source": [
    "# 6. Summary & Export\n",
    "\n",
    "Compile summary statistics and list generated figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "for model in MODEL_FILE_MAP.keys():\n",
    "    model_subset = base_df[base_df.model == model]\n",
    "    avg_acc = model_subset['accuracy'].dropna().mean()\n",
    "    avg_ppl = model_subset['word_perplexity'].dropna().mean()\n",
    "    mmlu_score = mmlu_df[(mmlu_df.model == model) & (mmlu_df.type == 'overall')]['score']\n",
    "    mmlu_es_score = mmlu_es_df[(mmlu_es_df.model == model) & (mmlu_es_df.type == 'overall')]['score']\n",
    "    summary_rows.append({\n",
    "        'model': model,\n",
    "        'avg_accuracy': avg_acc,\n",
    "        'avg_word_perplexity': avg_ppl,\n",
    "        'mmlu_overall': mmlu_score.iloc[0] if not mmlu_score.empty else None,\n",
    "        'mmlu_es_overall': mmlu_es_score.iloc[0] if not mmlu_es_score.empty else None,\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"Summary metrics:\")\n",
    "print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Export summary CSV\n",
    "summary_path = OUTPUT_DIR / f\"summary_metrics_{TIMESTAMP}.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\nüíæ Summary CSV saved: {summary_path}\")\n",
    "\n",
    "# List generated figures\n",
    "print(\"\\nüìÅ Generated Figures:\")\n",
    "for f in sorted(OUTPUT_DIR.glob(f\"*{TIMESTAMP}.*\")):\n",
    "    if f.suffix.lower() in ('.png', '.pdf'):\n",
    "        print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Visualization process complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85bd681",
   "metadata": {},
   "source": [
    "# 7. MMLU & MMLU_ES Heatmaps\n",
    "\n",
    "Detailed performance visualization by subcategory:\n",
    "- MMLU (57 subcategories): Sorted by average accuracy across models.\n",
    "- MMLU_ES (6 categories): Direct comparative view.\n",
    "\n",
    "Heatmaps include annotated values (0‚Äì1 range) to ease inspection of relative differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mmlu_subcategory_heatmap(top_n=None):\n",
    "    \"\"\"Generate a heatmap for MMLU subcategories.\n",
    "    Args:\n",
    "        top_n: if provided, show only the top_n subcategories by mean score.\n",
    "    \"\"\"\n",
    "    sub_df = mmlu_df[mmlu_df['type'] == 'subcategory'].copy()\n",
    "    if sub_df.empty:\n",
    "        print(\"‚ö†Ô∏è No MMLU subcategories available\")\n",
    "        return\n",
    "    mean_scores = sub_df.groupby('category')['score'].mean().sort_values(ascending=False)\n",
    "    ordered_categories = mean_scores.index.tolist()\n",
    "    if top_n is not None:\n",
    "        ordered_categories = ordered_categories[:top_n]\n",
    "    pivot = sub_df[sub_df['category'].isin(ordered_categories)].pivot(index='category', columns='model', values='score')\n",
    "    pivot = pivot.loc[ordered_categories]\n",
    "    fig_height = max(4, len(pivot) * 0.25 + 1)\n",
    "    fig, ax = plt.subplots(figsize=(8, fig_height))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.2f', cmap='viridis', vmin=0, vmax=1, cbar_kws={'label': 'Accuracy'}, ax=ax)\n",
    "    ax.set_title('MMLU Subcategories (Accuracy)')\n",
    "    ax.set_ylabel('Subcategory')\n",
    "    ax.set_xlabel('Model')\n",
    "    plt.tight_layout()\n",
    "    save_fig(f'mmlu_subcategories_heatmap_{\"top\"+str(top_n) if top_n else \"all\"}', fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_mmlu_es_heatmap():\n",
    "    sub_df = mmlu_es_df[mmlu_es_df['type'] == 'top_category'].copy()\n",
    "    if sub_df.empty:\n",
    "        print(\"‚ö†Ô∏è No MMLU_ES categories available\")\n",
    "        return\n",
    "    pivot = sub_df.pivot(index='category', columns='model', values='score')\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.2f', cmap='mako', vmin=0, vmax=1, cbar_kws={'label': 'Accuracy'}, ax=ax)\n",
    "    ax.set_title('MMLU_ES Categories (Accuracy)')\n",
    "    ax.set_ylabel('Category')\n",
    "    ax.set_xlabel('Model')\n",
    "    plt.tight_layout()\n",
    "    save_fig('mmlu_es_categories_heatmap', fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Execute heatmaps (all subcategories and top 25 as additional example)\n",
    "plot_mmlu_subcategory_heatmap()       # all\n",
    "plot_mmlu_subcategory_heatmap(top_n=25)  # top 25\n",
    "plot_mmlu_es_heatmap()\n",
    "\n",
    "print(\"‚úÖ Heatmaps generated and saved\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
