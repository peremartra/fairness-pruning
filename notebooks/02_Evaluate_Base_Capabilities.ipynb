{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/fairness-pruning/blob/main/notebooks/02_Evaluate_Base_Capabilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# Fairness Pruning Research - Base Model Evaluation\n",
        "## 02 - Comprehensive Benchmark Suite for Unpruned Models\n",
        "\n",
        "### Establishing Performance Baselines for Bias Mitigation Research\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/â­_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/fairness-pruning](https://github.com/peremartra/fairness-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 or A100\n",
        "\n",
        "**Models to Evaluate:**\n",
        "* Llama-3.2-1B (base)\n",
        "* Llama-3.2-3B (base)\n",
        "* Additional models defined in `EXPERIMENT_CONFIG`\n",
        "\n",
        "**Benchmarks (15 total):**\n",
        "* English: MMLU, HellaSwag, BoolQ, ARC-Challenge, WinoGrande, PIQA, TruthfulQA, GSM8K, IFEval, MUSR\n",
        "* Spanish: Belebele, XCOPA, MMLU-ES\n",
        "* Language Modeling: WikiText, Lambada-OpenAI\n",
        "\n",
        "**Estimated Runtime:** ~3-4 hours (varies by number of models)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Objective\n",
        "\n",
        "Establish **performance baselines** for the Fairness Pruning project by evaluating unpruned base models.\n",
        "\n",
        "**Purpose:**\n",
        "1. Measure baseline performance before bias mitigation interventions\n",
        "2. Create reference metrics for future pruned model comparisons\n",
        "3. Validate benchmark configurations across different architectures\n",
        "4. Capture cross-lingual performance (English + Spanish)\n",
        "\n",
        "**Features:**\n",
        "- âœ… Checkpoint/Resume Support (survives Colab disconnections)\n",
        "- âœ… Multi-Model Support (generic, not 1B-specific)\n",
        "- âœ… Robust Error Handling (continues on task failures)\n",
        "- âœ… Automated Path Management (no manual configuration needed)\n",
        "\n",
        "**Note:** This notebook evaluates ONLY base models (no pruning applied). For bias mitigation experiments with pruned models, see subsequent notebooks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAo67s0lIvXF",
        "outputId": "c07e1c6e-63a5-46ad-d81f-9ecf0dd1cb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWIHQuIGIvXG",
        "outputId": "4499fb84-0504-49f3-be1d-b2945776c23b"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2nO7YpIvXG",
        "outputId": "6f5291b0-a443-4264-edb8-c9b2ddcf9656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gQKRdEHcSFYK"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/veritasQA/veritas_qa_ca.yaml\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/veritasQA/veritas_qa_es.yaml\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/custom_tasks/veritasQA/veritas_lm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsrTGtkwAh45",
        "outputId": "fbceff55-18e2-4b28-cb2f-85bb3527c1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "ðŸ“¦ INSTALLING VERITAS QA (ES/CA) + veritas_lm.py\n",
            "==================================================\n",
            "ðŸ“ TASKS DIRECTORY: /usr/local/lib/python3.12/dist-packages/lm_eval/tasks/veritas_qa\n",
            "   âœ… veritas_qa_es.yaml -> Installation OK\n",
            "   âœ… veritas_qa_ca.yaml -> Installation OK\n",
            "   âœ… veritas_lm.py -> Created successfully in target dir\n",
            "\n",
            "ðŸš€ OK! Ready to evaluate.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import lm_eval\n",
        "\n",
        "print(f\"{'='*50}\")\n",
        "print(\"ðŸ“¦ INSTALLING VERITAS QA (ES/CA) + veritas_lm.py\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# 1. Locate installation directory\n",
        "lib_path = os.path.dirname(lm_eval.__file__)\n",
        "target_dir = os.path.join(lib_path, \"tasks\", \"veritas_qa\")\n",
        "\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "print(f\"ðŸ“ TASKS DIRECTORY: {target_dir}\")\n",
        "\n",
        "# 2. Copy YAML files\n",
        "for lang in [\"es\", \"ca\"]:\n",
        "    filename = f\"veritas_qa_{lang}.yaml\"\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        dst = os.path.join(target_dir, filename)\n",
        "        # Use copy for safety (preserves original file in current dir)\n",
        "        shutil.copy(filename, dst)\n",
        "        print(f\"   âœ… {filename} -> Installation OK\")\n",
        "    else:\n",
        "        print(f\"   âŒ Error: {filename} not found in current directory.\")\n",
        "\n",
        "# 3. CREATE veritas_lm.py with the helper function\n",
        "# This file is required by the YAML configuration (!function veritas_lm.parse...)\n",
        "veritas_lm_content = \"\"\"\n",
        "def parse_veritasqa_choices(doc):\n",
        "    \\\"\\\"\\\"Parse and clean VeritasQA answer choices.\n",
        "\n",
        "    VeritasQA stores answers as semicolon-separated strings.\n",
        "    This function splits them, cleans whitespace, and returns\n",
        "    correct answers first (so index 0 is always correct).\n",
        "\n",
        "    Args:\n",
        "        doc: Dataset example with 'correct_answers' and 'incorrect_answers'\n",
        "\n",
        "    Returns:\n",
        "        list: All answer choices (correct + incorrect)\n",
        "    \\\"\\\"\\\"\n",
        "    correct = [ans.strip() for ans in doc['correct_answers'].split(';')]\n",
        "    incorrect = [ans.strip() for ans in doc['incorrect_answers'].split(';')]\n",
        "\n",
        "    # Return correct answers first (doc_to_target=0 expects this)\n",
        "    return correct + incorrect\n",
        "\"\"\"\n",
        "\n",
        "# Write the helper file to the target task directory\n",
        "utils_path = os.path.join(target_dir, \"veritas_lm.py\")\n",
        "with open(utils_path, \"w\") as f:\n",
        "    f.write(veritas_lm_content)\n",
        "\n",
        "print(f\"   âœ… veritas_lm.py -> Created successfully in target dir\")\n",
        "print(\"\\nðŸš€ OK! Ready to evaluate.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JBfW3_wxX90",
        "outputId": "c29de8f0-b3b8-40f9-d7fa-46ae9b9a8105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”§ Overwriting file at: /usr/local/lib/python3.12/dist-packages/lm_eval/tasks/veritas_qa/veritas_lm.py\n",
            "âœ… File updated. NOW YOU MUST RESTART THE RUNTIME.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Definimos la ruta donde estÃ¡ instalada la tarea\n",
        "import lm_eval\n",
        "lib_path = os.path.dirname(lm_eval.__file__)\n",
        "target_dir = os.path.join(lib_path, \"tasks\", \"veritas_qa\")\n",
        "utils_path = os.path.join(target_dir, \"veritas_lm.py\") # O utils.py si usaste ese nombre\n",
        "\n",
        "print(f\"ðŸ”§ Overwriting file at: {utils_path}\")\n",
        "\n",
        "new_content = \"\"\"\n",
        "def parse_veritasqa_choices(doc):\n",
        "    \\\"\\\"\\\"\n",
        "    Parse VeritasQA choices using ONLY the best answer as the target.\n",
        "    This prevents multiple correct answers from penalizing the model.\n",
        "    \\\"\\\"\\\"\n",
        "    # CRITICAL FIX: Use best_answer only, do not split correct_answers\n",
        "    correct = [doc['best_answer'].strip()]\n",
        "\n",
        "    # Split incorrect answers\n",
        "    incorrect = [ans.strip() for ans in doc['incorrect_answers'].split(';')]\n",
        "\n",
        "    # Return [Target, Distractor1, Distractor2, ...]\n",
        "    return correct + incorrect\n",
        "\"\"\"\n",
        "\n",
        "with open(utils_path, \"w\") as f:\n",
        "    f.write(new_content)\n",
        "\n",
        "print(\"âœ… File updated. NOW YOU MUST RESTART THE RUNTIME.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHjkx6_QIvXG",
        "outputId": "f558a5df-2ed3-4d46-8b48-cec84f1d5f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All imports successful\n",
            "ðŸ“± Device: GPU\n",
            "   GPU: NVIDIA L4\n",
            "   Memory: 23.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Import core libraries and utilities\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Import our utility functions\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG,\n",
        "    BENCHMARKS_BASE,\n",
        "    load_or_create_model,\n",
        "    run_robust_evaluation,\n",
        "    clear_gpu_cache,\n",
        "    get_model_stats,\n",
        "    format_results_table\n",
        ")\n",
        "\n",
        "logging.getLogger(\"lm_eval\").setLevel(logging.INFO)\n",
        "\n",
        "print(\"âœ… All imports successful\")\n",
        "print(f\"ðŸ“± Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOqtWXbhrRf"
      },
      "source": [
        "# 1. Helper Functions\n",
        "\n",
        "Utility functions for automatic checkpoint path generation and model size detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb0Sq8eqhrRg",
        "outputId": "d638708e-52d1-4953-e486-4be5648f13a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing helper functions with EXPERIMENT_CONFIG:\n",
            "----------------------------------------------------------------------\n",
            "BSC-LT/salamandra-2b                               â†’ 2b\n",
            "meta-llama/Llama-3.2-1B                            â†’ 1b\n",
            "meta-llama/Llama-3.2-3B                            â†’ 3b\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def get_model_size(model_name: str) -> str:\n",
        "    \"\"\"Extract model size identifier from HuggingFace model name.\n",
        "\n",
        "    Examples:\n",
        "        \"meta-llama/Llama-3.2-1B\" â†’ \"1b\"\n",
        "        \"meta-llama/Llama-3.2-3B-Instruct\" â†’ \"3b_instruct\"\n",
        "        \"BSC-LT/salamandra-2b\" â†’ \"2b\"\n",
        "    \"\"\"\n",
        "    match = re.search(r'(\\d+\\.?\\d*)[Bb]', model_name)\n",
        "    if not match:\n",
        "        return \"unknown\"\n",
        "\n",
        "    size = match.group(1).replace('.', '_') + \"b\"\n",
        "    if \"instruct\" in model_name.lower():\n",
        "        size += \"_instruct\"\n",
        "\n",
        "    return size.lower()\n",
        "\n",
        "def get_checkpoint_path(model_name: str, base_dir: str) -> str:\n",
        "    \"\"\"Generate checkpoint path with size-based subdirectory.\n",
        "\n",
        "    Args:\n",
        "        model_name: Full HuggingFace model identifier\n",
        "        base_dir: Base directory for checkpoints\n",
        "\n",
        "    Returns:\n",
        "        Full path to checkpoint file\n",
        "    \"\"\"\n",
        "    model_size = get_model_size(model_name)\n",
        "    safe_name = model_name.replace('/', '_').replace('-', '_').lower()\n",
        "    checkpoint_dir = os.path.join(base_dir, model_size)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    return os.path.join(checkpoint_dir, f\"{safe_name}.json\")\n",
        "\n",
        "# Test with EXPERIMENT_CONFIG\n",
        "print(\"Testing helper functions with EXPERIMENT_CONFIG:\")\n",
        "print(\"-\" * 70)\n",
        "for cfg in EXPERIMENT_CONFIG:\n",
        "    model_id = cfg['base_model']\n",
        "    size = get_model_size(model_id)\n",
        "    print(f\"{model_id:<50} â†’ {size}\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI3mbbDchrRg"
      },
      "source": [
        "# 2. Configuration & Evaluation Plan\n",
        "\n",
        "This section prepares the evaluation for all models defined in `EXPERIMENT_CONFIG`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LixoDuXJIvXG",
        "outputId": "13fe8384-923b-453e-9a25-d7e4b15472a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ðŸ“Š EVALUATION PLAN: Base Model Benchmarking\n",
            "======================================================================\n",
            "\n",
            "Models to evaluate: 3\n",
            "Benchmarks per model: 14\n",
            "Total evaluations: 42\n",
            "Estimated time: ~4.5 hours\n",
            "\n",
            "Models to evaluate:\n",
            "----------------------------------------------------------------------\n",
            "Model ID                                           Size       Status\n",
            "----------------------------------------------------------------------\n",
            "BSC-LT/salamandra-2b                               2b         âœ… Exists\n",
            "meta-llama/Llama-3.2-1B                            1b         âœ… Exists\n",
            "meta-llama/Llama-3.2-3B                            3b         âœ… Exists\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Benchmarks:\n",
            "----------------------------------------------------------------------\n",
            " 1. wikitext                       0-shot\n",
            " 2. lambada_openai                 0-shot\n",
            " 3. ifeval                         0-shot\n",
            " 4. gsm8k                          5-shot\n",
            " 5. mmlu                           5-shot\n",
            " 6. arc_challenge                  0-shot\n",
            " 7. hellaswag                      0-shot\n",
            " 8. truthfulqa_mc2                 0-shot\n",
            " 9. global_mmlu_es                 5-shot\n",
            "10. arc_es                         0-shot\n",
            "11. hellaswag_es                   0-shot\n",
            "12. belebele_spa_Latn              0-shot\n",
            "13. veritas_qa_es                  0-shot\n",
            "14. veritas_qa_ca                  0-shot\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "âš™ï¸  Configuration:\n",
            "   - Checkpointing: Enabled (per-task granularity)\n",
            "   - Auto-resume: Yes (survives disconnections)\n",
            "   - Error handling: Skip failed tasks, continue evaluation\n",
            "   - Device: GPU\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Directory setup\n",
        "CHECKPOINT_BASE_DIR = \"/content/drive/MyDrive/fair_pruning/checkpoints\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/fair_pruning/results\"\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# De-duplicate models from EXPERIMENT_CONFIG\n",
        "unique_models = list(dict.fromkeys([cfg[\"base_model\"] for cfg in EXPERIMENT_CONFIG]))\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“Š EVALUATION PLAN: Base Model Benchmarking\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(f\"Models to evaluate: {len(unique_models)}\")\n",
        "print(f\"Benchmarks per model: {len(BENCHMARKS_BASE)}\")\n",
        "print(f\"Total evaluations: {len(unique_models) * len(BENCHMARKS_BASE)}\")\n",
        "print(f\"Estimated time: ~{len(unique_models) * 1.5:.1f} hours\\n\")\n",
        "\n",
        "# Display models with checkpoint status\n",
        "print(\"Models to evaluate:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model ID':<50} {'Size':<10} {'Status'}\")\n",
        "print(\"-\" * 70)\n",
        "for model_id in unique_models:\n",
        "    size = get_model_size(model_id)\n",
        "    cp_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n",
        "    exists = \"âœ… Exists\" if Path(cp_path).exists() else \"ðŸ†• New\"\n",
        "    print(f\"{model_id:<50} {size:<10} {exists}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Display benchmarks\n",
        "print(\"\\nBenchmarks:\")\n",
        "print(\"-\" * 70)\n",
        "for i, task in enumerate(BENCHMARKS_BASE, 1):\n",
        "    fewshot_str = f\"{task['num_fewshot']}-shot\"\n",
        "    print(f\"{i:2d}. {task['name']:<30} {fewshot_str}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(f\"\\nâš™ï¸  Configuration:\")\n",
        "print(f\"   - Checkpointing: Enabled (per-task granularity)\")\n",
        "print(f\"   - Auto-resume: Yes (survives disconnections)\")\n",
        "print(f\"   - Error handling: Skip failed tasks, continue evaluation\")\n",
        "print(f\"   - Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7U0stRUIvXG",
        "outputId": "b731cf61-e051-448a-d2fd-e3dcb0e00b69"
      },
      "source": [
        "# 3. Base Model Evaluation\n",
        "\n",
        "Evaluates each base model across all benchmarks with checkpoint/resume support.\n",
        "\n",
        "**Process:**\n",
        "1. Load model directly from HuggingFace Hub (no pruning applied)\n",
        "2. Calculate model statistics (parameters, size)\n",
        "3. Run evaluation with checkpoint system (saves progress after each task)\n",
        "4. Clear GPU memory before next model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_RshCOsC6m7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ðŸš€ STARTING EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "all_model_results = {}\n",
        "\n",
        "for i, model_id in enumerate(unique_models, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ“Š MODEL {i}/{len(unique_models)}: {model_id}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    try:\n",
        "        # 1. Load model from HuggingFace Hub (NO pruning)\n",
        "        print(f\"Loading from HuggingFace Hub...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,  # Use bfloat16 for A100, float16 for T4/L4\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        print(\"âœ… Model loaded successfully\\n\")\n",
        "\n",
        "        # 2. Display model statistics\n",
        "        stats = get_model_stats(model)\n",
        "        print(f\"ðŸ“ˆ Model Statistics:\")\n",
        "        print(f\"   Parameters: {stats['total_parameters']:,}\")\n",
        "        print(f\"   Size: {stats['size_gb']:.2f} GB\\n\")\n",
        "\n",
        "        # 3. Generate checkpoint path automatically\n",
        "        checkpoint_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n",
        "        print(f\"ðŸ“ Checkpoint: {checkpoint_path}\\n\")\n",
        "\n",
        "        # 4. Run evaluation with checkpoint/resume support\n",
        "        results = run_robust_evaluation(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            tasks=BENCHMARKS_BASE,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            model_name=model_id,\n",
        "        )\n",
        "\n",
        "        all_model_results[model_id] = results\n",
        "\n",
        "        print(f\"\\nâœ… Completed: {model_id}\")\n",
        "        print(\"\\nResults Preview:\")\n",
        "        print(format_results_table(results))\n",
        "\n",
        "        # 5. Cleanup memory before next model\n",
        "        del model, tokenizer\n",
        "        clear_gpu_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR evaluating {model_id}: {str(e)}\")\n",
        "\n",
        "        # Check for common issues\n",
        "        if \"401\" in str(e) or \"403\" in str(e):\n",
        "            print(\"   â†’ Authentication required. Run: huggingface-cli login\")\n",
        "        elif \"CUDA out of memory\" in str(e):\n",
        "            print(\"   â†’ GPU OOM. Try reducing batch size or using smaller model\")\n",
        "\n",
        "        print(\"   â†’ Continuing with next model...\\n\")\n",
        "\n",
        "        # Cleanup and continue\n",
        "        if 'model' in locals():\n",
        "            del model\n",
        "        if 'tokenizer' in locals():\n",
        "            del tokenizer\n",
        "        clear_gpu_cache()\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"âœ… EVALUATION COMPLETE: {len(all_model_results)}/{len(unique_models)} models\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjcHGNrsIvXH"
      },
      "source": [
        "# 4. Results Consolidation\n",
        "\n",
        "Load checkpoint files and consolidate into a single DataFrame for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "HtbaAsxIIvXH",
        "outputId": "f337117b-4f3e-4e58-ef1d-7958ee9318ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            " CONSOLIDATING RESULTS (FILTERED)\n",
            "======================================================================\n",
            "\n",
            "Total archivos JSON encontrados: 10\n",
            " Procesando: meta_llama_llama_3.2_1b.json\n",
            " Procesando: bsc_lt_salamandra_2b.json\n",
            " Procesando: meta_llama_llama_3.2_3b.json\n",
            "\n",
            "âœ… Ã‰XITO: Se han consolidado 42 filas correctamente.\n",
            "Modelos Ãºnicos: ['BSC-LT/salamandra-2b' 'meta-llama/Llama-3.2-1B'\n",
            " 'meta-llama/Llama-3.2-3B']\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-485de993-8572-4111-b87f-6df8d2411a99\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>model_size</th>\n",
              "      <th>task</th>\n",
              "      <th>word_perplexity,none</th>\n",
              "      <th>byte_perplexity,none</th>\n",
              "      <th>bits_per_byte,none</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>word_perplexity</th>\n",
              "      <th>bits_per_byte</th>\n",
              "      <th>prompt_level_strict_acc,none</th>\n",
              "      <th>...</th>\n",
              "      <th>subcategories_elementary_mathematics</th>\n",
              "      <th>subcategories_high_school_biology</th>\n",
              "      <th>subcategories_high_school_chemistry</th>\n",
              "      <th>subcategories_high_school_computer_science</th>\n",
              "      <th>subcategories_high_school_mathematics</th>\n",
              "      <th>subcategories_high_school_physics</th>\n",
              "      <th>subcategories_high_school_statistics</th>\n",
              "      <th>subcategories_machine_learning</th>\n",
              "      <th>subcategories_business</th>\n",
              "      <th>subcategories_medical</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BSC-LT/salamandra-2b</td>\n",
              "      <td>2b</td>\n",
              "      <td>arc_challenge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BSC-LT/salamandra-2b</td>\n",
              "      <td>2b</td>\n",
              "      <td>arc_es</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BSC-LT/salamandra-2b</td>\n",
              "      <td>2b</td>\n",
              "      <td>belebele_spa_Latn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BSC-LT/salamandra-2b</td>\n",
              "      <td>2b</td>\n",
              "      <td>global_mmlu_es</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.3276</td>\n",
              "      <td>0.2500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BSC-LT/salamandra-2b</td>\n",
              "      <td>2b</td>\n",
              "      <td>gsm8k</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 88 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-485de993-8572-4111-b87f-6df8d2411a99')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-485de993-8572-4111-b87f-6df8d2411a99 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-485de993-8572-4111-b87f-6df8d2411a99');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c0527d8b-5772-4b92-ba57-b2ac102965a6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0527d8b-5772-4b92-ba57-b2ac102965a6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c0527d8b-5772-4b92-ba57-b2ac102965a6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  model model_size               task word_perplexity,none  \\\n",
              "0  BSC-LT/salamandra-2b         2b      arc_challenge                  NaN   \n",
              "1  BSC-LT/salamandra-2b         2b             arc_es                  NaN   \n",
              "2  BSC-LT/salamandra-2b         2b  belebele_spa_Latn                  NaN   \n",
              "3  BSC-LT/salamandra-2b         2b     global_mmlu_es                  NaN   \n",
              "4  BSC-LT/salamandra-2b         2b              gsm8k                  NaN   \n",
              "\n",
              "  byte_perplexity,none bits_per_byte,none perplexity word_perplexity  \\\n",
              "0                  NaN                NaN        NaN             NaN   \n",
              "1                  NaN                NaN        NaN             NaN   \n",
              "2                  NaN                NaN        NaN             NaN   \n",
              "3                  NaN                NaN        NaN             NaN   \n",
              "4                  NaN                NaN        NaN             NaN   \n",
              "\n",
              "  bits_per_byte prompt_level_strict_acc,none  ...  \\\n",
              "0           NaN                          NaN  ...   \n",
              "1           NaN                          NaN  ...   \n",
              "2           NaN                          NaN  ...   \n",
              "3           NaN                          NaN  ...   \n",
              "4           NaN                          NaN  ...   \n",
              "\n",
              "  subcategories_elementary_mathematics subcategories_high_school_biology  \\\n",
              "0                                  NaN                               NaN   \n",
              "1                                  NaN                               NaN   \n",
              "2                                  NaN                               NaN   \n",
              "3                                  NaN                               NaN   \n",
              "4                                  NaN                               NaN   \n",
              "\n",
              "  subcategories_high_school_chemistry  \\\n",
              "0                                 NaN   \n",
              "1                                 NaN   \n",
              "2                                 NaN   \n",
              "3                                 NaN   \n",
              "4                                 NaN   \n",
              "\n",
              "  subcategories_high_school_computer_science  \\\n",
              "0                                        NaN   \n",
              "1                                        NaN   \n",
              "2                                        NaN   \n",
              "3                                        NaN   \n",
              "4                                        NaN   \n",
              "\n",
              "  subcategories_high_school_mathematics subcategories_high_school_physics  \\\n",
              "0                                   NaN                               NaN   \n",
              "1                                   NaN                               NaN   \n",
              "2                                   NaN                               NaN   \n",
              "3                                   NaN                               NaN   \n",
              "4                                   NaN                               NaN   \n",
              "\n",
              "  subcategories_high_school_statistics subcategories_machine_learning  \\\n",
              "0                                  NaN                            NaN   \n",
              "1                                  NaN                            NaN   \n",
              "2                                  NaN                            NaN   \n",
              "3                                  NaN                            NaN   \n",
              "4                                  NaN                            NaN   \n",
              "\n",
              "  subcategories_business subcategories_medical  \n",
              "0                    NaN                   NaN  \n",
              "1                    NaN                   NaN  \n",
              "2                    NaN                   NaN  \n",
              "3                 0.3276                0.2500  \n",
              "4                    NaN                   NaN  \n",
              "\n",
              "[5 rows x 88 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- 1. FunciÃ³n auxiliar para arreglar los datos anidados (MMLU) ---\n",
        "def flatten_metrics(metrics, prefix=''):\n",
        "    flat = {}\n",
        "    for k, v in metrics.items():\n",
        "        if isinstance(v, dict):\n",
        "            flat.update(flatten_metrics(v, prefix=f\"{prefix}{k}_\"))\n",
        "        else:\n",
        "            flat[f\"{prefix}{k}\"] = v\n",
        "    return flat\n",
        "\n",
        "# --- 2. ConfiguraciÃ³n ---\n",
        "print(f\"{'='*70}\")\n",
        "print(\" CONSOLIDATING RESULTS (FILTERED)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Buscamos todos los JSONs recursivamente\n",
        "checkpoint_files = glob.glob(f\"{CHECKPOINT_BASE_DIR}/**/*.json\", recursive=True)\n",
        "print(f\"Total archivos JSON encontrados: {len(checkpoint_files)}\")\n",
        "\n",
        "consolidated_data = []\n",
        "\n",
        "for json_path in sorted(checkpoint_files):\n",
        "    # --- FILTRO CLAVE: Ignorar carpetas temporales ---\n",
        "    if \"lm_evals\" in json_path:\n",
        "        continue  # Saltamos este archivo silenciosamente\n",
        "    # -------------------------------------------------\n",
        "\n",
        "    print(f\" Procesando: {os.path.basename(json_path)}\")\n",
        "\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # ExtracciÃ³n de metadatos\n",
        "        metadata = data.get(\"metadata\", {})\n",
        "        model_name = metadata.get(\"model_name\", \"Unknown\")\n",
        "\n",
        "        # Determinamos el tamaÃ±o (segÃºn tu lÃ³gica o fallback simple)\n",
        "        if \"1b\" in model_name.lower(): model_size = \"1b\"\n",
        "        elif \"2b\" in model_name.lower(): model_size = \"2b\"\n",
        "        elif \"3b\" in model_name.lower(): model_size = \"3b\"\n",
        "        else: model_size = \"unknown\"\n",
        "\n",
        "        results = data.get(\"results\", {})\n",
        "\n",
        "        if not results:\n",
        "            print(\"   -> Sin resultados, saltando.\")\n",
        "            continue\n",
        "\n",
        "        # Procesar cada tarea\n",
        "        for task_name, metrics in results.items():\n",
        "            row = {\n",
        "                \"model\": model_name,\n",
        "                \"model_size\": model_size,\n",
        "                \"task\": task_name\n",
        "            }\n",
        "            # Usamos la funciÃ³n flatten para evitar errores de formato\n",
        "            row.update(flatten_metrics(metrics))\n",
        "            consolidated_data.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   -> Error leyendo {json_path}: {e}\")\n",
        "\n",
        "# --- 3. Crear DataFrame Final ---\n",
        "if consolidated_data:\n",
        "    df = pd.DataFrame(consolidated_data)\n",
        "    # Ordenamos: Modelo -> Tarea\n",
        "    df = df.sort_values(by=[\"model\", \"task\"]).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nâœ… Ã‰XITO: Se han consolidado {len(df)} filas correctamente.\")\n",
        "    print(f\"Modelos Ãºnicos: {df['model'].unique()}\")\n",
        "\n",
        "    # Mostrar vista previa\n",
        "    display(df.head())\n",
        "\n",
        "    # Guardar (usando tus rutas originales)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    df.to_csv(f\"{RESULTS_DIR}/base_models_results_{timestamp}.csv\", index=False)\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No se encontraron datos vÃ¡lidos despuÃ©s del filtrado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPoZiAfjIvXH",
        "outputId": "04f8eb6e-7b8e-4116-a779-a5448e2d52ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ’¾ Results saved:\n",
            "   /content/drive/MyDrive/fair_pruning/results/base_models_results_20251206_175322.csv\n",
            "   /content/drive/MyDrive/fair_pruning/results/base_models_results_latest.csv\n",
            "   /content/drive/MyDrive/fair_pruning/results/base_models_results_20251206_175322.json\n",
            "\n",
            "âœ… All results exported successfully\n"
          ]
        }
      ],
      "source": [
        "if not df.empty:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save detailed results CSV\n",
        "    csv_path = f\"{RESULTS_DIR}/base_models_results_{timestamp}.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nðŸ’¾ Results saved:\")\n",
        "    print(f\"   {csv_path}\")\n",
        "\n",
        "    # Save latest version\n",
        "    latest_csv = f\"{RESULTS_DIR}/base_models_results_latest.csv\"\n",
        "    df.to_csv(latest_csv, index=False)\n",
        "    print(f\"   {latest_csv}\")\n",
        "\n",
        "    # Save JSON format\n",
        "    json_path = f\"{RESULTS_DIR}/base_models_results_{timestamp}.json\"\n",
        "    df.to_json(json_path, orient='records', indent=2)\n",
        "    print(f\"   {json_path}\")\n",
        "\n",
        "    print(f\"\\nâœ… All results exported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MptG-dIvXH"
      },
      "source": [
        "# 5. Summary Analysis\n",
        "\n",
        "Generate summary statistics comparing models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5RqknL6IvXH",
        "outputId": "0f1164d8-4d01-411f-d1a6-1603a82fec38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ðŸ“ˆ SUMMARY STATISTICS\n",
            "======================================================================\n",
            "\n",
            "                  model model_size  avg_accuracy  avg_perplexity  tasks_completed  tasks_with_accuracy  tasks_with_perplexity\n",
            "   BSC-LT/salamandra-2b         2b        0.3078          7.2700               14                   10                      1\n",
            "meta-llama/Llama-3.2-1B         1b        0.3281          5.4300               14                   10                      1\n",
            "meta-llama/Llama-3.2-3B         3b        0.4345          3.8800               14                   10                      1\n",
            "\n",
            "ðŸ’¾ Summary saved: /content/drive/MyDrive/fair_pruning/results/base_models_summary_20251206_175322.csv\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Celda 5: Summary Analysis Corregida ---\n",
        "if not df.empty:\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"ðŸ“ˆ SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    summary = []\n",
        "    # Agrupamos por modelo\n",
        "    for model_name, model_df in df.groupby('model'):\n",
        "\n",
        "        # --- CORRECCIÃ“N AQUÃ: Convertir a nÃºmeros explÃ­citamente ---\n",
        "        # Usamos errors='coerce' para que si hay texto no numÃ©rico se convierta en NaN\n",
        "        acc_series = pd.to_numeric(model_df['accuracy'], errors='coerce')\n",
        "        ppl_series = pd.to_numeric(model_df['perplexity'], errors='coerce')\n",
        "\n",
        "        # Ahora sÃ­ podemos hacer dropna() seguro\n",
        "        acc = acc_series.dropna()\n",
        "        ppl = ppl_series.dropna()\n",
        "        # -----------------------------------------------------------\n",
        "\n",
        "        # Obtener metadata (intentando ser robustos si falta 'model_size')\n",
        "        if 'model_size' in model_df.columns:\n",
        "            model_size = model_df['model_size'].iloc[0]\n",
        "        else:\n",
        "            # Fallback simple si no existe la columna\n",
        "            model_size = \"unknown\"\n",
        "            if \"1b\" in model_name.lower(): model_size = \"1b\"\n",
        "            elif \"2b\" in model_name.lower(): model_size = \"2b\"\n",
        "            elif \"3b\" in model_name.lower(): model_size = \"3b\"\n",
        "\n",
        "        summary.append({\n",
        "            \"model\": model_name,\n",
        "            \"model_size\": model_size,\n",
        "            \"avg_accuracy\": acc.mean() if len(acc) > 0 else None,\n",
        "            \"avg_perplexity\": ppl.mean() if len(ppl) > 0 else None,\n",
        "            \"tasks_completed\": len(model_df),\n",
        "            \"tasks_with_accuracy\": len(acc),\n",
        "            \"tasks_with_perplexity\": len(ppl)\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "\n",
        "    if not summary_df.empty:\n",
        "        summary_df = summary_df.sort_values(\"model\").reset_index(drop=True)\n",
        "        # Formato limpio para la tabla\n",
        "        print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "        # Guardar summary\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        summary_csv = f\"{RESULTS_DIR}/base_models_summary_{timestamp}.csv\"\n",
        "        summary_df.to_csv(summary_csv, index=False)\n",
        "        print(f\"\\nðŸ’¾ Summary saved: {summary_csv}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No se pudo generar el resumen (datos insuficientes).\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "else:\n",
        "    print(\"DataFrame vacÃ­o. No hay estadÃ­sticas que calcular.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBPO5o8oteVs",
        "outputId": "b390b8a6-821f-47b6-b7bd-30eb977ac374"
      },
      "source": [
        "# 6. Evaluation Complete\n",
        "\n",
        "## Summary\n",
        "\n",
        "Baseline performance metrics established for the Fairness Pruning project.\n",
        "\n",
        "**Generated Files:**\n",
        "- `base_models_results_latest.csv` - Full evaluation results\n",
        "- `base_models_results_YYYYMMDD_HHMMSS.json` - Structured export\n",
        "- `base_models_summary_YYYYMMDD_HHMMSS.csv` - Summary metrics\n",
        "- Individual checkpoint JSONs per model (in subdirectories by size)\n",
        "\n",
        "**Next Steps:**\n",
        "1. Use these baselines as reference for bias mitigation experiments\n",
        "2. Identify high-variance tasks that may be sensitive to interventions\n",
        "3. Proceed to bias detection and pruning notebooks\n",
        "\n",
        "---\n",
        "\n",
        "**Powered by OptiPFair** - Activation-Guided MLP Width Pruning for Bias Mitigation\n",
        "\n",
        "If this research helps your work:\n",
        "- â­ Star [the repo](https://github.com/peremartra/optipfair)\n",
        "- ðŸ“– Read the [documentation](https://peremartra.github.io/optipfair/)\n",
        "- ðŸ› Report issues or suggest features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENb-L1tWhrRg",
        "outputId": "f6018975-8a3f-4f16-e6dd-fba23563bfe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ðŸ“ GENERATED FILES\n",
            "======================================================================\n",
            "\n",
            "Results:\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/results/base_models_results_20251206_175322.csv\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/results/base_models_results_latest.csv\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/results/base_models_results_20251206_175322.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/results/base_models_summary_20251206_175322.csv\n",
            "\n",
            "Checkpoints:\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/1b/meta_llama_llama_3.2_1b.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/2b/bsc_lt_salamandra_2b.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/3b/meta_llama_llama_3.2_3b.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/results/lm_evals/bsc_lt_salamandra_2b_veritas_qa_ca.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/results/lm_evals/bsc_lt_salamandra_2b_veritas_qa_es.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/results/lm_evals/meta_llama_llama_3.2_1b_truthfulqa_mc2.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/results/lm_evals/meta_llama_llama_3.2_1b_veritas_qa_ca.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/results/lm_evals/meta_llama_llama_3.2_1b_veritas_qa_es.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/results/lm_evals/meta_llama_llama_3.2_3b_veritas_qa_ca.json\n",
            "  âœ… /content/drive/MyDrive/fair_pruning/checkpoints/results/lm_evals/meta_llama_llama_3.2_3b_veritas_qa_es.json\n",
            "\n",
            "======================================================================\n",
            "âœ… EVALUATION COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“ GENERATED FILES\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(\"Results:\")\n",
        "if 'csv_path' in locals() and os.path.exists(csv_path):\n",
        "    print(f\"  âœ… {csv_path}\")\n",
        "if 'latest_csv' in locals() and os.path.exists(latest_csv):\n",
        "    print(f\"  âœ… {latest_csv}\")\n",
        "if 'json_path' in locals() and os.path.exists(json_path):\n",
        "    print(f\"  âœ… {json_path}\")\n",
        "if 'summary_csv' in locals() and os.path.exists(summary_csv):\n",
        "    print(f\"  âœ… {summary_csv}\")\n",
        "\n",
        "print(\"\\nCheckpoints:\")\n",
        "if 'checkpoint_files' in locals():\n",
        "    for f in sorted(checkpoint_files)[:10]:  # Show first 10\n",
        "        print(f\"  âœ… {f}\")\n",
        "    if len(checkpoint_files) > 10:\n",
        "        print(f\"  ... and {len(checkpoint_files) - 10} more\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ… EVALUATION COMPLETE\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mc2FNF5NThaA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
