{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/fairness-pruning/blob/main/notebooks/02_Evaluate_Base_Capabilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# Fairness Pruning Research - Base Model Evaluation\n",
        "## 02 - Comprehensive Benchmark Suite for Unpruned Models\n",
        "\n",
        "### Establishing Performance Bias using BBVQ, EsBBQ & CatBBQ for Bias Mitigation Research\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/â­_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/fairness-pruning](https://github.com/peremartra/fairness-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 or A100\n",
        "\n",
        "**Models to Evaluate:**\n",
        "* Llama-3.2-1B (base)\n",
        "* Llama-3.2-3B (base)\n",
        "* Salamandra-2B (base)\n",
        "---\n",
        "\n",
        "**Note:** This notebook evaluates ONLY base models (no pruning applied). For bias mitigation experiments with pruned models, see subsequent notebooks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAo67s0lIvXF",
        "outputId": "c07e1c6e-63a5-46ad-d81f-9ecf0dd1cb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWIHQuIGIvXG",
        "outputId": "4499fb84-0504-49f3-be1d-b2945776c23b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2nO7YpIvXG",
        "outputId": "6f5291b0-a443-4264-edb8-c9b2ddcf9656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gQKRdEHcSFYK"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/veritas_qa_ca.yaml\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/veritas_qa_es.yaml\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/veritas_lm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOqtWXbhrRf"
      },
      "source": [
        "# 1. Helper Functions\n",
        "\n",
        "Utility functions for automatic checkpoint path generation and model size detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb0Sq8eqhrRg",
        "outputId": "d638708e-52d1-4953-e486-4be5648f13a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing helper functions with EXPERIMENT_CONFIG:\n",
            "----------------------------------------------------------------------\n",
            "BSC-LT/salamandra-2b                               â†’ 2b\n",
            "meta-llama/Llama-3.2-1B                            â†’ 1b\n",
            "meta-llama/Llama-3.2-3B                            â†’ 3b\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def get_model_size(model_name: str) -> str:\n",
        "    \"\"\"Extract model size identifier from HuggingFace model name.\n",
        "\n",
        "    Examples:\n",
        "        \"meta-llama/Llama-3.2-1B\" â†’ \"1b\"\n",
        "        \"meta-llama/Llama-3.2-3B-Instruct\" â†’ \"3b_instruct\"\n",
        "        \"BSC-LT/salamandra-2b\" â†’ \"2b\"\n",
        "    \"\"\"\n",
        "    match = re.search(r'(\\d+\\.?\\d*)[Bb]', model_name)\n",
        "    if not match:\n",
        "        return \"unknown\"\n",
        "\n",
        "    size = match.group(1).replace('.', '_') + \"b\"\n",
        "    if \"instruct\" in model_name.lower():\n",
        "        size += \"_instruct\"\n",
        "\n",
        "    return size.lower()\n",
        "\n",
        "def get_checkpoint_path(model_name: str, base_dir: str) -> str:\n",
        "    \"\"\"Generate checkpoint path with size-based subdirectory.\n",
        "\n",
        "    Args:\n",
        "        model_name: Full HuggingFace model identifier\n",
        "        base_dir: Base directory for checkpoints\n",
        "\n",
        "    Returns:\n",
        "        Full path to checkpoint file\n",
        "    \"\"\"\n",
        "    model_size = get_model_size(model_name)\n",
        "    safe_name = model_name.replace('/', '_').replace('-', '_').lower()\n",
        "    checkpoint_dir = os.path.join(base_dir, model_size)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    return os.path.join(checkpoint_dir, f\"{safe_name}.json\")\n",
        "\n",
        "# Test with EXPERIMENT_CONFIG\n",
        "print(\"Testing helper functions with EXPERIMENT_CONFIG:\")\n",
        "print(\"-\" * 70)\n",
        "for cfg in EXPERIMENT_CONFIG:\n",
        "    model_id = cfg['base_model']\n",
        "    size = get_model_size(model_id)\n",
        "    print(f\"{model_id:<50} â†’ {size}\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Configuration & Evaluation Plan (BBQ-only)\n",
        "\n",
        "Configure paths, select the BBQ task, and list the models we will evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import os\n",
        "from IPython.display import display\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG,\n",
        "    run_robust_evaluation,\n",
        "    load_or_create_model,\n",
        "    clear_gpu_cache,\n",
        "    format_results_table,\n",
        "    get_model_stats,\n",
        " )\n",
        "\n",
        "# Paths (Drive recommended in Colab)\n",
        "CHECKPOINT_BASE_DIR = \"/content/drive/MyDrive/fair_pruning/checkpoints_bbq\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/fair_pruning/results\"\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# BBQ task list (0-shot)\n",
        "BBQ_TASKS = [{\"name\": \"bbq\", \"num_fewshot\": 0}]\n",
        "\n",
        "# De-duplicate models from EXPERIMENT_CONFIG\n",
        "unique_models = list(dict.fromkeys([cfg[\"base_model\"] for cfg in EXPERIMENT_CONFIG]))\n",
        "\n",
        "logging.getLogger(\"lm_eval\").setLevel(logging.INFO)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“Š EVALUATION PLAN: BBQ-only\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Checkpoints: {CHECKPOINT_BASE_DIR}\")\n",
        "print(f\"Results: {RESULTS_DIR}\")\n",
        "print(f\"Tasks: {[task['name'] for task in BBQ_TASKS]}\")\n",
        "print(\"Models:\")\n",
        "for m in unique_models:\n",
        "    print(f\" - {m}\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Run BBQ Evaluation\n",
        "\n",
        "Evaluate each base model on the BBQ task with checkpoint/resume and raw result saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ðŸš€ STARTING BBQ EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "all_model_results = {}\n",
        "\n",
        "for idx, model_id in enumerate(unique_models, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ“Š MODEL {idx}/{len(unique_models)}: {model_id}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    try:\n",
        "        checkpoint_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n",
        "        raw_results_dir = os.path.join(os.path.dirname(checkpoint_path), \"lm_evals\")\n",
        "\n",
        "        # Load model/tokenizer (no pruning applied here)\n",
        "        model, tokenizer, stats = load_or_create_model({\"base_model\": model_id})\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(\"âœ… Model loaded\")\n",
        "\n",
        "        # Display model stats when available\n",
        "        if stats:\n",
        "            total_params = stats.get(\"total_parameters\")\n",
        "            size_gb = stats.get(\"size_gb\")\n",
        "            if total_params is not None:\n",
        "                print(f\"ðŸ“ˆ Params: {total_params:,}\")\n",
        "            if size_gb is not None:\n",
        "                print(f\"ðŸ“¦ Size: {size_gb:.2f} GB\")\n",
        "        print(f\"ðŸ“ Checkpoint: {checkpoint_path}\\n\")\n",
        "\n",
        "        # Run BBQ task with checkpoint/resume and raw saves\n",
        "        results = run_robust_evaluation(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            tasks=BBQ_TASKS,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            model_name=model_id,\n",
        "            save_raw_results=True,\n",
        "            raw_results_dir=raw_results_dir,\n",
        "        )\n",
        "\n",
        "        all_model_results[model_id] = results\n",
        "        print(f\"\\nâœ… Completed: {model_id}\")\n",
        "        print(\"Results Preview:\")\n",
        "        print(format_results_table(results))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR evaluating {model_id}: {e}\")\n",
        "        if 'model' in locals():\n",
        "            del model\n",
        "        if 'tokenizer' in locals():\n",
        "            del tokenizer\n",
        "        clear_gpu_cache()\n",
        "        continue\n",
        "\n",
        "    # Cleanup and move to next model\n",
        "    del model\n",
        "    del tokenizer\n",
        "    clear_gpu_cache()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"âœ… BBQ EVALUATION COMPLETE: {len(all_model_results)}/{len(unique_models)} models\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Consolidate BBQ Results\n",
        "\n",
        "Load checkpoint files, flatten metrics, and export combined BBQ results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def flatten_metrics(metrics, prefix=''):\n",
        "    flat = {}\n",
        "    for k, v in metrics.items():\n",
        "        if isinstance(v, dict):\n",
        "            flat.update(flatten_metrics(v, prefix=f\"{prefix}{k}_\"))\n",
        "        else:\n",
        "            flat[f\"{prefix}{k}\"] = v\n",
        "    return flat\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\" CONSOLIDATING BBQ RESULTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "checkpoint_files = glob.glob(f\"{CHECKPOINT_BASE_DIR}/**/*.json\", recursive=True)\n",
        "print(f\"Total checkpoint JSONs: {len(checkpoint_files)}\")\n",
        "\n",
        "consolidated_data = []\n",
        "\n",
        "for json_path in sorted(checkpoint_files):\n",
        "    if \"lm_evals\" in json_path:\n",
        "        continue  # skip raw per-task dumps\n",
        "    try:\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        metadata = data.get(\"metadata\", {})\n",
        "        model_name = metadata.get(\"model_name\", \"Unknown\")\n",
        "\n",
        "        if \"1b\" in model_name.lower(): model_size = \"1b\"\n",
        "        elif \"2b\" in model_name.lower(): model_size = \"2b\"\n",
        "        elif \"3b\" in model_name.lower(): model_size = \"3b\"\n",
        "        else: model_size = \"unknown\"\n",
        "\n",
        "        results = data.get(\"results\", {})\n",
        "        if not results:\n",
        "            continue\n",
        "\n",
        "        for task_name, metrics in results.items():\n",
        "            row = {\n",
        "                \"model\": model_name,\n",
        "                \"model_size\": model_size,\n",
        "                \"task\": task_name,\n",
        "            }\n",
        "            row.update(flatten_metrics(metrics))\n",
        "            consolidated_data.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   -> Error reading {json_path}: {e}\")\n",
        "\n",
        "if consolidated_data:\n",
        "    df = pd.DataFrame(consolidated_data)\n",
        "    df = df.sort_values(by=[\"model\", \"task\"]).reset_index(drop=True)\n",
        "    display(df.head())\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"{RESULTS_DIR}/base_models_bbq_results_{timestamp}.csv\"\n",
        "    latest_csv = f\"{RESULTS_DIR}/base_models_bbq_results_latest.csv\"\n",
        "    json_path = f\"{RESULTS_DIR}/base_models_bbq_results_{timestamp}.json\"\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_csv(latest_csv, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(\"\\nðŸ’¾ BBQ results saved:\")\n",
        "    print(f\"   {csv_path}\")\n",
        "    print(f\"   {latest_csv}\")\n",
        "    print(f\"   {json_path}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No BBQ results found to consolidate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Summary Analysis\n",
        "\n",
        "Quick per-model accuracy summary for BBQ and file inventory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary stats\n",
        "summary_df = None\n",
        "if 'df' in locals() and not df.empty:\n",
        "    summaries = []\n",
        "    for model_name, model_df in df.groupby('model'):\n",
        "        acc_series = pd.to_numeric(model_df.get('accuracy'), errors='coerce').dropna()\n",
        "        summaries.append({\n",
        "            \"model\": model_name,\n",
        "            \"model_size\": model_df.get('model_size', pd.Series(['unknown'])).iloc[0],\n",
        "            \"avg_accuracy\": acc_series.mean() if len(acc_series) else None,\n",
        "            \"tasks_completed\": len(model_df)\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summaries)\n",
        "    summary_df = summary_df.sort_values(\"model\").reset_index(drop=True)\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"ðŸ“ˆ BBQ SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    summary_csv = f\"{RESULTS_DIR}/base_models_bbq_summary_{timestamp}.csv\"\n",
        "    summary_df.to_csv(summary_csv, index=False)\n",
        "    print(f\"\\nðŸ’¾ Summary saved: {summary_csv}\\n\")\n",
        "else:\n",
        "    print(\"No consolidated data available for summary.\")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“ GENERATED FILES\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if 'csv_path' in locals() and os.path.exists(csv_path):\n",
        "    print(f\"  âœ… {csv_path}\")\n",
        "if 'latest_csv' in locals() and os.path.exists(latest_csv):\n",
        "    print(f\"  âœ… {latest_csv}\")\n",
        "if 'json_path' in locals() and os.path.exists(json_path):\n",
        "    print(f\"  âœ… {json_path}\")\n",
        "if 'summary_csv' in locals() and os.path.exists(summary_csv):\n",
        "    print(f\"  âœ… {summary_csv}\")\n",
        "\n",
        "print(\"\\nCheckpoints (first 10):\")\n",
        "if 'checkpoint_files' in locals():\n",
        "    for f in sorted(checkpoint_files)[:10]:\n",
        "        print(f\"  âœ… {f}\")\n",
        "    if len(checkpoint_files) > 10:\n",
        "        print(f\"  ... and {len(checkpoint_files) - 10} more\")\n",
        "\n",
        "print(f\"{'='*70}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
