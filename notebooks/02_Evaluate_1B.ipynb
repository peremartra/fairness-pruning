{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href=\"https://colab.research.google.com/github/peremartra/fairness-pruning/blob/main/notebooks/02_Evaluate_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzfjNg_SIvXD"
   },
   "source": "# Fairness Pruning Research - Base Model Evaluation\n## 02 - Comprehensive Benchmark Suite for Unpruned Models\n\n### Establishing Performance Baselines for Bias Mitigation Research\nby [Pere Martra](https://github.com/peremartra)\n\n[![GitHub](https://img.shields.io/badge/â­_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n\n**Repository:** [github.com/peremartra/fairness-pruning](https://github.com/peremartra/fairness-pruning)\n\n---\n\n**Colab Environment:** GPU L4 or A100\n\n**Models to Evaluate:**\n* Llama-3.2-1B (base)\n* Llama-3.2-3B (base)\n* Additional models defined in `EXPERIMENT_CONFIG`\n\n**Benchmarks (15 total):**\n* English: MMLU, HellaSwag, BoolQ, ARC-Challenge, WinoGrande, PIQA, TruthfulQA, GSM8K, IFEval, MUSR\n* Spanish: Belebele, XCOPA, MMLU-ES\n* Language Modeling: WikiText, Lambada-OpenAI\n\n**Estimated Runtime:** ~3-4 hours (varies by number of models)\n\n---\n\n## ðŸ“‹ Objective\n\nEstablish **performance baselines** for the Fairness Pruning project by evaluating unpruned base models.\n\n**Purpose:**\n1. Measure baseline performance before bias mitigation interventions\n2. Create reference metrics for future pruned model comparisons\n3. Validate benchmark configurations across different architectures\n4. Capture cross-lingual performance (English + Spanish)\n\n**Features:**\n- âœ… Checkpoint/Resume Support (survives Colab disconnections)\n- âœ… Multi-Model Support (generic, not 1B-specific)\n- âœ… Robust Error Handling (continues on task failures)\n- âœ… Automated Path Management (no manual configuration needed)\n\n**Note:** This notebook evaluates ONLY base models (no pruning applied). For bias mitigation experiments with pruned models, see subsequent notebooks.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkVFbeCMIvXF"
   },
   "source": [
    "# 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nAo67s0lIvXF",
    "outputId": "fcc0fec9-8e7a-4d45-d800-8b78d934d358",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q optipfair\n",
    "!pip install -q lm-eval\n",
    "!pip install -q langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWIHQuIGIvXG",
    "outputId": "60623ec2-4d82-4c25-c3ac-f9135787205f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive for checkpoint persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DG2nO7YpIvXG",
    "outputId": "2beedbd0-89ea-4141-dbb6-6a3cac0331d9"
   },
   "outputs": [],
   "source": "# Download utils.py from GitHub repository\n!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/utils.py\n\n# Verify download\nimport os\nif os.path.exists('utils.py'):\n    print(\"âœ… utils.py downloaded successfully\")\nelse:\n    print(\"âŒ Failed to download utils.py\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHjkx6_QIvXG",
    "outputId": "c200e528-3fd0-47dc-ec52-cda5f382b062"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… All imports successful\n",
      "ðŸ“± Device: GPU\n",
      "   GPU: NVIDIA L4\n",
      "   Memory: 23.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries and utilities\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our utility functions\n",
    "from utils import (\n",
    "    EXPERIMENT_CONFIG,\n",
    "    BENCHMARKS_BASE,\n",
    "    load_or_create_model,\n",
    "    run_robust_evaluation,\n",
    "    clear_gpu_cache,\n",
    "    get_model_stats,\n",
    "    format_results_table\n",
    ")\n",
    "\n",
    "print(\"âœ… All imports successful\")\n",
    "print(f\"ðŸ“± Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Helper Functions\n\nUtility functions for automatic checkpoint path generation and model size detection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\nimport os\n\ndef get_model_size(model_name: str) -> str:\n    \"\"\"Extract model size identifier from HuggingFace model name.\n    \n    Examples:\n        \"meta-llama/Llama-3.2-1B\" â†’ \"1b\"\n        \"meta-llama/Llama-3.2-3B-Instruct\" â†’ \"3b_instruct\"\n        \"BSC-LT/salamandra-2b\" â†’ \"2b\"\n    \"\"\"\n    match = re.search(r'(\\d+\\.?\\d*)[Bb]', model_name)\n    if not match:\n        return \"unknown\"\n    \n    size = match.group(1).replace('.', '_') + \"b\"\n    if \"instruct\" in model_name.lower():\n        size += \"_instruct\"\n    \n    return size.lower()\n\ndef get_checkpoint_path(model_name: str, base_dir: str) -> str:\n    \"\"\"Generate checkpoint path with size-based subdirectory.\n    \n    Args:\n        model_name: Full HuggingFace model identifier\n        base_dir: Base directory for checkpoints\n        \n    Returns:\n        Full path to checkpoint file\n    \"\"\"\n    model_size = get_model_size(model_name)\n    safe_name = model_name.replace('/', '_').replace('-', '_').lower()\n    checkpoint_dir = os.path.join(base_dir, model_size)\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    return os.path.join(checkpoint_dir, f\"{safe_name}.json\")\n\n# Test with EXPERIMENT_CONFIG\nprint(\"Testing helper functions with EXPERIMENT_CONFIG:\")\nprint(\"-\" * 70)\nfor cfg in EXPERIMENT_CONFIG:\n    model_id = cfg['base_model']\n    size = get_model_size(model_id)\n    print(f\"{model_id:<50} â†’ {size}\")\nprint(\"-\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Configuration & Evaluation Plan\n\nThis section prepares the evaluation for all models defined in `EXPERIMENT_CONFIG`.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LixoDuXJIvXG"
   },
   "source": "# Directory setup\nCHECKPOINT_BASE_DIR = \"/content/drive/MyDrive/glu_pruning/checkpoints\"\nRESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\nPath(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n\n# De-duplicate models from EXPERIMENT_CONFIG\nunique_models = list(dict.fromkeys([cfg[\"base_model\"] for cfg in EXPERIMENT_CONFIG]))\n\nprint(f\"{'='*70}\")\nprint(\"ðŸ“Š EVALUATION PLAN: Base Model Benchmarking\")\nprint(f\"{'='*70}\\n\")\nprint(f\"Models to evaluate: {len(unique_models)}\")\nprint(f\"Benchmarks per model: {len(BENCHMARKS_BASE)}\")\nprint(f\"Total evaluations: {len(unique_models) * len(BENCHMARKS_BASE)}\")\nprint(f\"Estimated time: ~{len(unique_models) * 1.5:.1f} hours\\n\")\n\n# Display models with checkpoint status\nprint(\"Models to evaluate:\")\nprint(\"-\" * 70)\nprint(f\"{'Model ID':<50} {'Size':<10} {'Status'}\")\nprint(\"-\" * 70)\nfor model_id in unique_models:\n    size = get_model_size(model_id)\n    cp_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n    exists = \"âœ… Exists\" if Path(cp_path).exists() else \"ðŸ†• New\"\n    print(f\"{model_id:<50} {size:<10} {exists}\")\nprint(\"-\" * 70)\n\n# Display benchmarks\nprint(\"\\nBenchmarks:\")\nprint(\"-\" * 70)\nfor i, task in enumerate(BENCHMARKS_BASE, 1):\n    fewshot_str = f\"{task['num_fewshot']}-shot\"\n    print(f\"{i:2d}. {task['name']:<30} {fewshot_str}\")\nprint(\"-\" * 70)\n\nprint(f\"\\nâš™ï¸  Configuration:\")\nprint(f\"   - Checkpointing: Enabled (per-task granularity)\")\nprint(f\"   - Auto-resume: Yes (survives disconnections)\")\nprint(f\"   - Error handling: Skip failed tasks, continue evaluation\")\nprint(f\"   - Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\\n\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7U0stRUIvXG",
    "outputId": "b731cf61-e051-448a-d2fd-e3dcb0e00b69"
   },
   "outputs": [],
   "source": "# 3. Base Model Evaluation\n\nEvaluates each base model across all benchmarks with checkpoint/resume support.\n\n**Process:**\n1. Load model directly from HuggingFace Hub (no pruning applied)\n2. Calculate model statistics (parameters, size)\n3. Run evaluation with checkpoint system (saves progress after each task)\n4. Clear GPU memory before next model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x5yTl5gTIvXH"
   },
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(f\"\\n{'='*70}\")\nprint(\"ðŸš€ STARTING EVALUATION\")\nprint(f\"{'='*70}\\n\")\n\nall_model_results = {}\n\nfor i, model_id in enumerate(unique_models, 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"ðŸ“Š MODEL {i}/{len(unique_models)}: {model_id}\")\n    print(f\"{'='*70}\\n\")\n    \n    try:\n        # 1. Load model from HuggingFace Hub (NO pruning)\n        print(f\"Loading from HuggingFace Hub...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id, \n            torch_dtype=torch.bfloat16,  # Use bfloat16 for A100, float16 for T4/L4\n            device_map=\"auto\"\n        )\n        \n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        \n        print(\"âœ… Model loaded successfully\\n\")\n        \n        # 2. Display model statistics\n        stats = get_model_stats(model)\n        print(f\"ðŸ“ˆ Model Statistics:\")\n        print(f\"   Parameters: {stats['total_parameters']:,}\")\n        print(f\"   Size: {stats['size_gb']:.2f} GB\\n\")\n        \n        # 3. Generate checkpoint path automatically\n        checkpoint_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n        print(f\"ðŸ“ Checkpoint: {checkpoint_path}\\n\")\n        \n        # 4. Run evaluation with checkpoint/resume support\n        results = run_robust_evaluation(\n            model=model,\n            tokenizer=tokenizer,\n            tasks=BENCHMARKS_BASE,\n            checkpoint_path=checkpoint_path,\n            model_name=model_id\n        )\n        \n        all_model_results[model_id] = results\n        \n        print(f\"\\nâœ… Completed: {model_id}\")\n        print(\"\\nResults Preview:\")\n        print(format_results_table(results))\n        \n        # 5. Cleanup memory before next model\n        del model, tokenizer\n        clear_gpu_cache()\n        \n    except Exception as e:\n        print(f\"\\nâŒ ERROR evaluating {model_id}: {str(e)}\")\n        \n        # Check for common issues\n        if \"401\" in str(e) or \"403\" in str(e):\n            print(\"   â†’ Authentication required. Run: huggingface-cli login\")\n        elif \"CUDA out of memory\" in str(e):\n            print(\"   â†’ GPU OOM. Try reducing batch size or using smaller model\")\n        \n        print(\"   â†’ Continuing with next model...\\n\")\n        \n        # Cleanup and continue\n        if 'model' in locals():\n            del model\n        if 'tokenizer' in locals():\n            del tokenizer\n        clear_gpu_cache()\n        continue\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ… EVALUATION COMPLETE: {len(all_model_results)}/{len(unique_models)} models\")\nprint(f\"{'='*70}\\n\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "PjcHGNrsIvXH"
   },
   "outputs": [],
   "source": "# 4. Results Consolidation\n\nLoad checkpoint files and consolidate into a single DataFrame for analysis."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HtbaAsxIIvXH"
   },
   "source": "import glob\n\nprint(f\"{'='*70}\")\nprint(\"ðŸ“Š CONSOLIDATING RESULTS\")\nprint(f\"{'='*70}\\n\")\n\n# Find all checkpoint files recursively\ncheckpoint_files = glob.glob(f\"{CHECKPOINT_BASE_DIR}/**/*.json\", recursive=True)\nprint(f\"Found {len(checkpoint_files)} checkpoint files\\n\")\n\nconsolidated_data = []\n\nfor json_path in sorted(checkpoint_files):\n    print(f\"  â†’ Processing: {os.path.basename(json_path)}\")\n    \n    try:\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        \n        # Extract metadata\n        metadata = data.get(\"metadata\", {})\n        model_name = metadata.get(\"model_name\", \"Unknown\")\n        model_size = get_model_size(model_name)\n        \n        # Extract results for each task\n        results = data.get(\"results\", {})\n        if not results:\n            print(f\"    âš ï¸ No results found, skipping\")\n            continue\n        \n        # Process each task\n        for task_name, metrics in results.items():\n            row = {\n                \"model\": model_name,\n                \"model_size\": model_size,\n                \"task\": task_name\n            }\n            \n            # Add all metrics\n            for metric_name, value in metrics.items():\n                try:\n                    row[metric_name] = float(value)\n                except (ValueError, TypeError):\n                    row[metric_name] = value\n            \n            consolidated_data.append(row)\n    \n    except Exception as e:\n        print(f\"    âš ï¸ Error processing file: {e}\")\n        continue\n\n# Create DataFrame\ndf = pd.DataFrame(consolidated_data)\n\nif not df.empty:\n    df = df.sort_values(by=[\"model\", \"task\"]).reset_index(drop=True)\n    print(f\"\\nâœ… Consolidated {len(df)} result rows\")\n    print(f\"   Models: {df['model'].nunique()}\")\n    print(f\"   Tasks: {df['task'].nunique()}\")\n    print(f\"   Metrics per task: {len(df.columns) - 3}\")  # Exclude model, model_size, task\n    \n    print(\"\\nDataFrame Preview:\")\n    print(df.head(15).to_string())\nelse:\n    print(\"\\nâš ï¸ No data consolidated\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPoZiAfjIvXH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "051729ec-a62d-4dd4-b8db-0359c37de5f6"
   },
   "outputs": [],
   "source": "if not df.empty:\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Save detailed results CSV\n    csv_path = f\"{RESULTS_DIR}/base_models_results_{timestamp}.csv\"\n    df.to_csv(csv_path, index=False)\n    print(f\"\\nðŸ’¾ Results saved:\")\n    print(f\"   {csv_path}\")\n    \n    # Save latest version\n    latest_csv = f\"{RESULTS_DIR}/base_models_results_latest.csv\"\n    df.to_csv(latest_csv, index=False)\n    print(f\"   {latest_csv}\")\n    \n    # Save JSON format\n    json_path = f\"{RESULTS_DIR}/base_models_results_{timestamp}.json\"\n    df.to_json(json_path, orient='records', indent=2)\n    print(f\"   {json_path}\")\n    \n    print(f\"\\nâœ… All results exported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1MptG-dIvXH"
   },
   "source": "# 5. Summary Analysis\n\nGenerate summary statistics comparing models."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5RqknL6IvXH",
    "outputId": "baf7de39-1caf-46de-cac6-c2e87135d21a"
   },
   "outputs": [],
   "source": "if not df.empty:\n    print(f\"{'='*70}\")\n    print(\"ðŸ“ˆ SUMMARY STATISTICS\")\n    print(f\"{'='*70}\\n\")\n    \n    summary = []\n    for model_name, model_df in df.groupby('model'):\n        # Calculate aggregated metrics\n        acc = model_df['accuracy'].dropna()\n        ppl = model_df['perplexity'].dropna()\n        \n        # Get model metadata (use first row since all rows have same metadata)\n        model_size = model_df['model_size'].iloc[0] if 'model_size' in model_df.columns else get_model_size(model_name)\n        \n        summary.append({\n            \"model\": model_name,\n            \"model_size\": model_size,\n            \"avg_accuracy\": acc.mean() if len(acc) > 0 else None,\n            \"avg_perplexity\": ppl.mean() if len(ppl) > 0 else None,\n            \"tasks_completed\": len(model_df),\n            \"tasks_with_accuracy\": len(acc),\n            \"tasks_with_perplexity\": len(ppl)\n        })\n    \n    summary_df = pd.DataFrame(summary)\n    summary_df = summary_df.sort_values(\"model\").reset_index(drop=True)\n    \n    print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n    \n    # Save summary\n    summary_csv = f\"{RESULTS_DIR}/base_models_summary_{timestamp}.csv\"\n    summary_df.to_csv(summary_csv, index=False)\n    \n    print(f\"\\nðŸ’¾ Summary saved: {summary_csv}\")\n    print(f\"\\n{'='*70}\")"
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Evaluation Complete\n\n## Summary\n\nBaseline performance metrics established for the Fairness Pruning project.\n\n**Generated Files:**\n- `base_models_results_latest.csv` - Full evaluation results\n- `base_models_results_YYYYMMDD_HHMMSS.json` - Structured export\n- `base_models_summary_YYYYMMDD_HHMMSS.csv` - Summary metrics\n- Individual checkpoint JSONs per model (in subdirectories by size)\n\n**Next Steps:**\n1. Use these baselines as reference for bias mitigation experiments\n2. Identify high-variance tasks that may be sensitive to interventions\n3. Proceed to bias detection and pruning notebooks\n\n---\n\n**Powered by OptiPFair** - Activation-Guided MLP Width Pruning for Bias Mitigation\n\nIf this research helps your work:\n- â­ Star [the repo](https://github.com/peremartra/optipfair)\n- ðŸ“– Read the [documentation](https://peremartra.github.io/optipfair/)\n- ðŸ› Report issues or suggest features\n\n---",
   "metadata": {
    "id": "TBPO5o8oteVs",
    "outputId": "b390b8a6-821f-47b6-b7bd-30eb977ac374",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(f\"{'='*70}\")\nprint(\"ðŸ“ GENERATED FILES\")\nprint(f\"{'='*70}\\n\")\n\nprint(\"Results:\")\nif 'csv_path' in locals() and os.path.exists(csv_path):\n    print(f\"  âœ… {csv_path}\")\nif 'latest_csv' in locals() and os.path.exists(latest_csv):\n    print(f\"  âœ… {latest_csv}\")\nif 'json_path' in locals() and os.path.exists(json_path):\n    print(f\"  âœ… {json_path}\")\nif 'summary_csv' in locals() and os.path.exists(summary_csv):\n    print(f\"  âœ… {summary_csv}\")\n\nprint(\"\\nCheckpoints:\")\nif 'checkpoint_files' in locals():\n    for f in sorted(checkpoint_files)[:10]:  # Show first 10\n        print(f\"  âœ… {f}\")\n    if len(checkpoint_files) > 10:\n        print(f\"  ... and {len(checkpoint_files) - 10} more\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"âœ… EVALUATION COMPLETE\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}