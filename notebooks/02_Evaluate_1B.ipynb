{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/fairness-pruning/blob/main/notebooks/02_Evaluate_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# Fairness Pruning Research - Base Model Evaluation\n",
        "## 02 - Comprehensive Benchmark Suite for Unpruned Models\n",
        "\n",
        "### Establishing Performance Baselines for Bias Mitigation Research\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/â­_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/fairness-pruning](https://github.com/peremartra/fairness-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 or A100\n",
        "\n",
        "**Models to Evaluate:**\n",
        "* Llama-3.2-1B (base)\n",
        "* Llama-3.2-3B (base)\n",
        "* Additional models defined in `EXPERIMENT_CONFIG`\n",
        "\n",
        "**Benchmarks (15 total):**\n",
        "* English: MMLU, HellaSwag, BoolQ, ARC-Challenge, WinoGrande, PIQA, TruthfulQA, GSM8K, IFEval, MUSR\n",
        "* Spanish: Belebele, XCOPA, MMLU-ES\n",
        "* Language Modeling: WikiText, Lambada-OpenAI\n",
        "\n",
        "**Estimated Runtime:** ~3-4 hours (varies by number of models)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Objective\n",
        "\n",
        "Establish **performance baselines** for the Fairness Pruning project by evaluating unpruned base models.\n",
        "\n",
        "**Purpose:**\n",
        "1. Measure baseline performance before bias mitigation interventions\n",
        "2. Create reference metrics for future pruned model comparisons\n",
        "3. Validate benchmark configurations across different architectures\n",
        "4. Capture cross-lingual performance (English + Spanish)\n",
        "\n",
        "**Features:**\n",
        "- âœ… Checkpoint/Resume Support (survives Colab disconnections)\n",
        "- âœ… Multi-Model Support (generic, not 1B-specific)\n",
        "- âœ… Robust Error Handling (continues on task failures)\n",
        "- âœ… Automated Path Management (no manual configuration needed)\n",
        "\n",
        "**Note:** This notebook evaluates ONLY base models (no pruning applied). For bias mitigation experiments with pruned models, see subsequent notebooks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAo67s0lIvXF",
        "outputId": "fcc0fec9-8e7a-4d45-d800-8b78d934d358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWIHQuIGIvXG",
        "outputId": "60623ec2-4d82-4c25-c3ac-f9135787205f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG2nO7YpIvXG"
      },
      "outputs": [],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/fairness-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHjkx6_QIvXG",
        "outputId": "c200e528-3fd0-47dc-ec52-cda5f382b062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All imports successful\n",
            "ðŸ“± Device: GPU\n",
            "   GPU: NVIDIA L4\n",
            "   Memory: 23.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Import core libraries and utilities\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our utility functions\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG,\n",
        "    BENCHMARKS_BASE,\n",
        "    load_or_create_model,\n",
        "    run_robust_evaluation,\n",
        "    clear_gpu_cache,\n",
        "    get_model_stats,\n",
        "    format_results_table\n",
        ")\n",
        "\n",
        "print(\"âœ… All imports successful\")\n",
        "print(f\"ðŸ“± Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Helper Functions\n",
        "\n",
        "Utility functions for automatic checkpoint path generation and model size detection."
      ],
      "metadata": {
        "id": "hwu_owMJhh2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def get_model_size(model_name: str) -> str:\n",
        "    \"\"\"Extract model size identifier from HuggingFace model name.\n",
        "\n",
        "    Examples:\n",
        "        \"meta-llama/Llama-3.2-1B\" â†’ \"1b\"\n",
        "        \"meta-llama/Llama-3.2-3B-Instruct\" â†’ \"3b_instruct\"\n",
        "        \"BSC-LT/salamandra-2b\" â†’ \"2b\"\n",
        "    \"\"\"\n",
        "    match = re.search(r'(\\d+\\.?\\d*)[Bb]', model_name)\n",
        "    if not match:\n",
        "        return \"unknown\"\n",
        "\n",
        "    size = match.group(1).replace('.', '_') + \"b\"\n",
        "    if \"instruct\" in model_name.lower():\n",
        "        size += \"_instruct\"\n",
        "\n",
        "    return size.lower()\n",
        "\n",
        "def get_checkpoint_path(model_name: str, base_dir: str) -> str:\n",
        "    \"\"\"Generate checkpoint path with size-based subdirectory.\n",
        "\n",
        "    Args:\n",
        "        model_name: Full HuggingFace model identifier\n",
        "        base_dir: Base directory for checkpoints\n",
        "\n",
        "    Returns:\n",
        "        Full path to checkpoint file\n",
        "    \"\"\"\n",
        "    model_size = get_model_size(model_name)\n",
        "    safe_name = model_name.replace('/', '_').replace('-', '_').lower()\n",
        "    checkpoint_dir = os.path.join(base_dir, model_size)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    return os.path.join(checkpoint_dir, f\"{safe_name}.json\")\n",
        "\n",
        "# Test with EXPERIMENT_CONFIG\n",
        "print(\"Testing helper functions with EXPERIMENT_CONFIG:\")\n",
        "print(\"-\" * 70)\n",
        "for cfg in EXPERIMENT_CONFIG:\n",
        "    model_id = cfg['base_model']\n",
        "    size = get_model_size(model_id)\n",
        "    print(f\"{model_id:<50} â†’ {size}\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "DBzmqSrXhh2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configuration & Evaluation Plan\n",
        "\n",
        "This section prepares the evaluation for all models defined in `EXPERIMENT_CONFIG`."
      ],
      "metadata": {
        "id": "kh9t7tJghh2i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LixoDuXJIvXG"
      },
      "source": [
        "# Directory setup\n",
        "CHECKPOINT_BASE_DIR = \"/content/drive/MyDrive/glu_pruning/checkpoints\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# De-duplicate models from EXPERIMENT_CONFIG\n",
        "unique_models = list(dict.fromkeys([cfg[\"base_model\"] for cfg in EXPERIMENT_CONFIG]))\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“Š EVALUATION PLAN: Base Model Benchmarking\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(f\"Models to evaluate: {len(unique_models)}\")\n",
        "print(f\"Benchmarks per model: {len(BENCHMARKS_BASE)}\")\n",
        "print(f\"Total evaluations: {len(unique_models) * len(BENCHMARKS_BASE)}\")\n",
        "print(f\"Estimated time: ~{len(unique_models) * 1.5:.1f} hours\\n\")\n",
        "\n",
        "# Display models with checkpoint status\n",
        "print(\"Models to evaluate:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model ID':<50} {'Size':<10} {'Status'}\")\n",
        "print(\"-\" * 70)\n",
        "for model_id in unique_models:\n",
        "    size = get_model_size(model_id)\n",
        "    cp_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n",
        "    exists = \"âœ… Exists\" if Path(cp_path).exists() else \"ðŸ†• New\"\n",
        "    print(f\"{model_id:<50} {size:<10} {exists}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Display benchmarks\n",
        "print(\"\\nBenchmarks:\")\n",
        "print(\"-\" * 70)\n",
        "for i, task in enumerate(BENCHMARKS_BASE, 1):\n",
        "    fewshot_str = f\"{task['num_fewshot']}-shot\"\n",
        "    print(f\"{i:2d}. {task['name']:<30} {fewshot_str}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(f\"\\nâš™ï¸  Configuration:\")\n",
        "print(f\"   - Checkpointing: Enabled (per-task granularity)\")\n",
        "print(f\"   - Auto-resume: Yes (survives disconnections)\")\n",
        "print(f\"   - Error handling: Skip failed tasks, continue evaluation\")\n",
        "print(f\"   - Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7U0stRUIvXG",
        "outputId": "b731cf61-e051-448a-d2fd-e3dcb0e00b69"
      },
      "source": [
        "# 3. Base Model Evaluation\n",
        "\n",
        "Evaluates each base model across all benchmarks with checkpoint/resume support.\n",
        "\n",
        "**Process:**\n",
        "1. Load model directly from HuggingFace Hub (no pruning applied)\n",
        "2. Calculate model statistics (parameters, size)\n",
        "3. Run evaluation with checkpoint system (saves progress after each task)\n",
        "4. Clear GPU memory before next model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5yTl5gTIvXH"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ðŸš€ STARTING EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "all_model_results = {}\n",
        "\n",
        "for i, model_id in enumerate(unique_models, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ“Š MODEL {i}/{len(unique_models)}: {model_id}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    try:\n",
        "        # 1. Load model from HuggingFace Hub (NO pruning)\n",
        "        print(f\"Loading from HuggingFace Hub...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,  # Use bfloat16 for A100, float16 for T4/L4\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        print(\"âœ… Model loaded successfully\\n\")\n",
        "\n",
        "        # 2. Display model statistics\n",
        "        stats = get_model_stats(model)\n",
        "        print(f\"ðŸ“ˆ Model Statistics:\")\n",
        "        print(f\"   Parameters: {stats['total_parameters']:,}\")\n",
        "        print(f\"   Size: {stats['size_gb']:.2f} GB\\n\")\n",
        "\n",
        "        # 3. Generate checkpoint path automatically\n",
        "        checkpoint_path = get_checkpoint_path(model_id, CHECKPOINT_BASE_DIR)\n",
        "        print(f\"ðŸ“ Checkpoint: {checkpoint_path}\\n\")\n",
        "\n",
        "        # 4. Run evaluation with checkpoint/resume support\n",
        "        results = run_robust_evaluation(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            tasks=BENCHMARKS_BASE,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            model_name=model_id\n",
        "        )\n",
        "\n",
        "        all_model_results[model_id] = results\n",
        "\n",
        "        print(f\"\\nâœ… Completed: {model_id}\")\n",
        "        print(\"\\nResults Preview:\")\n",
        "        print(format_results_table(results))\n",
        "\n",
        "        # 5. Cleanup memory before next model\n",
        "        del model, tokenizer\n",
        "        clear_gpu_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR evaluating {model_id}: {str(e)}\")\n",
        "\n",
        "        # Check for common issues\n",
        "        if \"401\" in str(e) or \"403\" in str(e):\n",
        "            print(\"   â†’ Authentication required. Run: huggingface-cli login\")\n",
        "        elif \"CUDA out of memory\" in str(e):\n",
        "            print(\"   â†’ GPU OOM. Try reducing batch size or using smaller model\")\n",
        "\n",
        "        print(\"   â†’ Continuing with next model...\\n\")\n",
        "\n",
        "        # Cleanup and continue\n",
        "        if 'model' in locals():\n",
        "            del model\n",
        "        if 'tokenizer' in locals():\n",
        "            del tokenizer\n",
        "        clear_gpu_cache()\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"âœ… EVALUATION COMPLETE: {len(all_model_results)}/{len(unique_models)} models\")\n",
        "print(f\"{'='*70}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjcHGNrsIvXH"
      },
      "source": [
        "# 4. Results Consolidation\n",
        "\n",
        "Load checkpoint files and consolidate into a single DataFrame for analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtbaAsxIIvXH"
      },
      "source": [
        "import glob\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“Š CONSOLIDATING RESULTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Find all checkpoint files recursively\n",
        "checkpoint_files = glob.glob(f\"{CHECKPOINT_BASE_DIR}/**/*.json\", recursive=True)\n",
        "print(f\"Found {len(checkpoint_files)} checkpoint files\\n\")\n",
        "\n",
        "consolidated_data = []\n",
        "\n",
        "for json_path in sorted(checkpoint_files):\n",
        "    print(f\"  â†’ Processing: {os.path.basename(json_path)}\")\n",
        "\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract metadata\n",
        "        metadata = data.get(\"metadata\", {})\n",
        "        model_name = metadata.get(\"model_name\", \"Unknown\")\n",
        "        model_size = get_model_size(model_name)\n",
        "\n",
        "        # Extract results for each task\n",
        "        results = data.get(\"results\", {})\n",
        "        if not results:\n",
        "            print(f\"    âš ï¸ No results found, skipping\")\n",
        "            continue\n",
        "\n",
        "        # Process each task\n",
        "        for task_name, metrics in results.items():\n",
        "            row = {\n",
        "                \"model\": model_name,\n",
        "                \"model_size\": model_size,\n",
        "                \"task\": task_name\n",
        "            }\n",
        "\n",
        "            # Add all metrics\n",
        "            for metric_name, value in metrics.items():\n",
        "                try:\n",
        "                    row[metric_name] = float(value)\n",
        "                except (ValueError, TypeError):\n",
        "                    row[metric_name] = value\n",
        "\n",
        "            consolidated_data.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Error processing file: {e}\")\n",
        "        continue\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "if not df.empty:\n",
        "    df = df.sort_values(by=[\"model\", \"task\"]).reset_index(drop=True)\n",
        "    print(f\"\\nâœ… Consolidated {len(df)} result rows\")\n",
        "    print(f\"   Models: {df['model'].nunique()}\")\n",
        "    print(f\"   Tasks: {df['task'].nunique()}\")\n",
        "    print(f\"   Metrics per task: {len(df.columns) - 3}\")  # Exclude model, model_size, task\n",
        "\n",
        "    print(\"\\nDataFrame Preview:\")\n",
        "    print(df.head(15).to_string())\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No data consolidated\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPoZiAfjIvXH"
      },
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save detailed results CSV\n",
        "    csv_path = f\"{RESULTS_DIR}/base_models_results_{timestamp}.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nðŸ’¾ Results saved:\")\n",
        "    print(f\"   {csv_path}\")\n",
        "\n",
        "    # Save latest version\n",
        "    latest_csv = f\"{RESULTS_DIR}/base_models_results_latest.csv\"\n",
        "    df.to_csv(latest_csv, index=False)\n",
        "    print(f\"   {latest_csv}\")\n",
        "\n",
        "    # Save JSON format\n",
        "    json_path = f\"{RESULTS_DIR}/base_models_results_{timestamp}.json\"\n",
        "    df.to_json(json_path, orient='records', indent=2)\n",
        "    print(f\"   {json_path}\")\n",
        "\n",
        "    print(f\"\\nâœ… All results exported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MptG-dIvXH"
      },
      "source": [
        "# 5. Summary Analysis\n",
        "\n",
        "Generate summary statistics comparing models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5RqknL6IvXH"
      },
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"ðŸ“ˆ SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    summary = []\n",
        "    for model_name, model_df in df.groupby('model'):\n",
        "        # Calculate aggregated metrics\n",
        "        acc = model_df['accuracy'].dropna()\n",
        "        ppl = model_df['perplexity'].dropna()\n",
        "\n",
        "        # Get model metadata (use first row since all rows have same metadata)\n",
        "        model_size = model_df['model_size'].iloc[0] if 'model_size' in model_df.columns else get_model_size(model_name)\n",
        "\n",
        "        summary.append({\n",
        "            \"model\": model_name,\n",
        "            \"model_size\": model_size,\n",
        "            \"avg_accuracy\": acc.mean() if len(acc) > 0 else None,\n",
        "            \"avg_perplexity\": ppl.mean() if len(ppl) > 0 else None,\n",
        "            \"tasks_completed\": len(model_df),\n",
        "            \"tasks_with_accuracy\": len(acc),\n",
        "            \"tasks_with_perplexity\": len(ppl)\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    summary_df = summary_df.sort_values(\"model\").reset_index(drop=True)\n",
        "\n",
        "    print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    # Save summary\n",
        "    summary_csv = f\"{RESULTS_DIR}/base_models_summary_{timestamp}.csv\"\n",
        "    summary_df.to_csv(summary_csv, index=False)\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Summary saved: {summary_csv}\")\n",
        "    print(f\"\\n{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluation Complete\n",
        "\n",
        "## Summary\n",
        "\n",
        "Baseline performance metrics established for the Fairness Pruning project.\n",
        "\n",
        "**Generated Files:**\n",
        "- `base_models_results_latest.csv` - Full evaluation results\n",
        "- `base_models_results_YYYYMMDD_HHMMSS.json` - Structured export\n",
        "- `base_models_summary_YYYYMMDD_HHMMSS.csv` - Summary metrics\n",
        "- Individual checkpoint JSONs per model (in subdirectories by size)\n",
        "\n",
        "**Next Steps:**\n",
        "1. Use these baselines as reference for bias mitigation experiments\n",
        "2. Identify high-variance tasks that may be sensitive to interventions\n",
        "3. Proceed to bias detection and pruning notebooks\n",
        "\n",
        "---\n",
        "\n",
        "**Powered by OptiPFair** - Activation-Guided MLP Width Pruning for Bias Mitigation\n",
        "\n",
        "If this research helps your work:\n",
        "- â­ Star [the repo](https://github.com/peremartra/optipfair)\n",
        "- ðŸ“– Read the [documentation](https://peremartra.github.io/optipfair/)\n",
        "- ðŸ› Report issues or suggest features\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TBPO5o8oteVs",
        "outputId": "b390b8a6-821f-47b6-b7bd-30eb977ac374"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{'='*70}\")\n",
        "print(\"ðŸ“ GENERATED FILES\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(\"Results:\")\n",
        "if 'csv_path' in locals() and os.path.exists(csv_path):\n",
        "    print(f\"  âœ… {csv_path}\")\n",
        "if 'latest_csv' in locals() and os.path.exists(latest_csv):\n",
        "    print(f\"  âœ… {latest_csv}\")\n",
        "if 'json_path' in locals() and os.path.exists(json_path):\n",
        "    print(f\"  âœ… {json_path}\")\n",
        "if 'summary_csv' in locals() and os.path.exists(summary_csv):\n",
        "    print(f\"  âœ… {summary_csv}\")\n",
        "\n",
        "print(\"\\nCheckpoints:\")\n",
        "if 'checkpoint_files' in locals():\n",
        "    for f in sorted(checkpoint_files)[:10]:  # Show first 10\n",
        "        print(f\"  âœ… {f}\")\n",
        "    if len(checkpoint_files) > 10:\n",
        "        print(f\"  ... and {len(checkpoint_files) - 10} more\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ… EVALUATION COMPLETE\")\n",
        "print(f\"{'='*70}\")"
      ],
      "metadata": {
        "id": "YBaxL_h0hh2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}