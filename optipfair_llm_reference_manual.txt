# OptiPFair: Library Implementation Guide

Designed to be used with you favourite LLM (ChatGPT / Claude / Gemini / Cursor / Windsurf). Just drop this file into your prompt or LLM Project and start building Optimized LLMs

## Overview

OptiPFair is a Python library for structured pruning of large language models, with a primary focus on GLU architectures. It also provides built-in functionality for bias visualization and analysis. This guide provides comprehensive information on using all of OptiPFair's features.

## Installation

```bash
# From PyPI
pip install optipfair

# From source
git clone https://github.com/peremartra/optipfair.git
cd optipfair
pip install -e .
```

For bias visualization functionality, install with additional dependencies:
```bash
pip install "optipfair[viz]"
```

## Core Functionality

### Model Pruning

OptiPFair supports two main types of pruning:

1. **MLP GLU Pruning**: Prunes neurons in transformer models that use GLU architecture in their MLP layers, including LLaMA, Mistral, and similar models.
2. **Depth Pruning**: Removes entire transformer layers from models, which is more aggressive than neuron-level pruning but can lead to significant efficiency gains.

#### Python API for Pruning

```python
from transformers import AutoModelForCausalLM
from optipfair import prune_model

# Load a pre-trained model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")

# Basic usage (10% pruning with MAW method)
pruned_model = prune_model(model)

# Advanced MLP GLU pruning
pruned_model, stats = prune_model(
    model=model,
    pruning_type="MLP_GLU",              # Type of pruning to apply
    neuron_selection_method="MAW",       # Method to calculate neuron importance
    pruning_percentage=20,               # Percentage of neurons to prune
    # expansion_rate=140,                # Alternatively, specify target expansion rate
    # expansion_divisor=128,             # Optional: round intermediate size to divisor (32, 64, 128, 256)
    show_progress=True,                  # Show progress during pruning
    return_stats=True                    # Return pruning statistics
)
#If expansion_rate is specified pruning_percentatge must be set to None 

# Data-driven MLP GLU pruning (NEW - with calibration data)
from torch.utils.data import DataLoader, TensorDataset

# Prepare calibration dataloader
calibration_texts = load_calibration_data()  # Your function
inputs = tokenizer(calibration_texts, return_tensors="pt", padding=True, 
                  truncation=True, max_length=512)
dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])
dataloader = DataLoader(dataset, batch_size=8)

pruned_model, stats = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    neuron_selection_method="MAW",        # Only MAW supports dataloader
    pruning_percentage=20,
    dataloader=dataloader,                # ← Enables hybrid calculation
    show_progress=True,
    return_stats=True
)

# Depth pruning examples
pruned_model, stats = prune_model(
    model=model,
    pruning_type="DEPTH",                # Type of pruning to apply
    num_layers_to_remove=3,              # Number of layers to remove
    layer_selection_method="last",       # Method for selecting layers ("last", "first", "custom")
    show_progress=True,                  # Show progress during pruning
    return_stats=True                    # Return pruning statistics
)

# Alternative depth pruning with percentage
pruned_model, stats = prune_model(
    model=model,
    pruning_type="DEPTH",                # Type of pruning to apply
    depth_pruning_percentage=25.0,       # Percentage of layers to remove
    layer_selection_method="first",      # Remove from beginning
    show_progress=True,
    return_stats=True
)

# Depth pruning with custom layer indices
pruned_model, stats = prune_model(
    model=model,
    pruning_type="DEPTH",                # Type of pruning to apply
    layer_indices=[2, 5, 8, 11],         # Specific layers to remove
    show_progress=True,
    return_stats=True
)

# Selective Width Pruning (NEW in v0.2.0) - Prune specific MLP layers only
pruned_model, stats = prune_model(
    model=model,
    pruning_type="MLP_GLU",              # Width pruning
    neuron_selection_method="MAW",
    pruning_percentage=20,
    layer_indices=[0, 5, 10, 15, 20],    # Only prune these specific layers
    show_progress=True,
    return_stats=True
)

# Selective width pruning with expansion_rate
pruned_model, stats = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    expansion_rate=260,                  # Target expansion rate
    layer_indices=[1, 3, 5, 7],          # Only prune odd-indexed layers
    show_progress=True,
    return_stats=True
)

# Selective data-driven width pruning
pruned_model, stats = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    neuron_selection_method="MAW",
    pruning_percentage=25,
    dataloader=calibration_dataloader,
    layer_indices=[2, 4, 6, 8],          # Only prune these layers with data-driven method
    show_progress=True,
    return_stats=True
)

# Print pruning statistics
print(f"Original parameters: {stats['original_parameters']:,}")
print(f"Pruned parameters: {stats['pruned_parameters']:,}")
print(f"Reduction: {stats['reduction']:,} parameters ({stats['percentage_reduction']:.2f}%)")

# Save the pruned model
pruned_model.save_pretrained("./pruned-model")
```

#### Command-Line Interface for Pruning

```bash
# Basic MLP GLU pruning
optipfair prune --model-path meta-llama/Llama-3.2-1B --output-path ./pruned-model

# Advanced MLP GLU pruning
optipfair prune \
  --model-path meta-llama/Llama-3.2-1B \
  --pruning-type MLP_GLU \
  --method MAW \
  --pruning-percentage 20 \
  --output-path ./pruned-model \
  --device cuda \
  --dtype float16

# Depth pruning - remove specific number of layers
optipfair prune \
  --model-path meta-llama/Llama-3.2-1B \
  --pruning-type DEPTH \
  --num-layers-to-remove 3 \
  --layer-selection-method last \
  --output-path ./pruned-model

# Depth pruning - remove percentage of layers
optipfair prune \
  --model-path meta-llama/Llama-3.2-1B \
  --pruning-type DEPTH \
  --pruning-percentage 25 \
  --layer-selection-method first \
  --output-path ./pruned-model

# Depth pruning - remove specific layers
optipfair prune \
  --model-path meta-llama/Llama-3.2-1B \
  --pruning-type DEPTH \
  --layer-indices "2,5,8,11" \
  --output-path ./pruned-model

# Selective width pruning - prune neurons only in specific layers
optipfair prune \
  --model-path meta-llama/Llama-3.2-1B \
  --pruning-type MLP_GLU \
  --method MAW \
  --pruning-percentage 20 \
  --layer-indices "0,5,10,15,20" \
  --output-path ./pruned-model

# Selective width pruning with expansion_rate
optipfair prune \
  --model-path meta-llama/Llama-3.2-1B \
  --pruning-type MLP_GLU \
  --expansion-rate 260 \
  --layer-indices "1,3,5,7" \
  --output-path ./pruned-model
```

### Neuron Selection Methods

OptiPFair supports three methods for calculating neuron importance (used with MLP_GLU pruning only):

1. **MAW (Maximum Absolute Weight)** - Default method that identifies neurons based on the maximum absolute weight values in their connections. Most effective for GLU architectures.

   **Static MAW (weight-only):**
```python
   pruned_model = prune_model(
       model=model,
       neuron_selection_method="MAW",
       pruning_percentage=20
   )
```

   **With expansion_divisor:**
   
   The `expansion_divisor` parameter ensures that the intermediate layer size is divisible by a specific value (32, 64, 128, or 256). This is useful for hardware optimization, as many accelerators perform better with tensor dimensions that are multiples of certain values.
   
   ```python
   # Round intermediate size to nearest multiple of 128
   pruned_model = prune_model(
       model=model,
       neuron_selection_method="MAW",
       pruning_percentage=20,
       expansion_divisor=128  # Intermediate size will be divisible by 128
   )
   
   # Can also be used with expansion_rate
   pruned_model = prune_model(
       model=model,
       neuron_selection_method="MAW",
       expansion_rate=200,
       expansion_divisor=64  # Intermediate size will be divisible by 64
   )
   ```
   
   **Key considerations for expansion_divisor:**
   - Valid values: `None` (default), `32`, `64`, `128`, `256`
   - Cannot be used alone - requires either `pruning_percentage` or `expansion_rate`
   - Rounding occurs after calculating the target size from pruning/expansion parameters
   - Rounds to the nearest multiple (up or down)
   - Useful for optimizing performance on specific hardware (GPUs, TPUs)
   - Common practice: use 128 or 256 for modern GPUs

   **Data-Driven MAW (hybrid: weights + activations):**
   
   When you provide a dataloader, MAW switches to a hybrid importance calculation that combines:
   - Static weight magnitudes from `gate_proj` and `up_proj` layers
   - Dynamic activation statistics from `down_proj` captured during calibration
   
   This data-driven approach implements the CFSP methodology (arXiv:2409.13199v2) and typically produces better pruned models that retain more of the original model's capabilities.
```python
   from torch.utils.data import DataLoader, TensorDataset
   
   # Prepare calibration dataloader
   texts = ["Sample text 1", "Sample text 2", ...]  # Your calibration data
   inputs = tokenizer(
       texts,
       return_tensors="pt",
       padding=True,
       truncation=True,
       max_length=512
   )
   dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])
   calibration_dataloader = DataLoader(dataset, batch_size=8)
   
   # Data-driven pruning with calibration
   pruned_model = prune_model(
       model=model,
       neuron_selection_method="MAW",
       pruning_percentage=20,
       dataloader=calibration_dataloader,  # ← Enables hybrid calculation
       show_progress=True
   )
```
   
   **Key considerations for data-driven pruning:**
   - Use a representative calibration dataset (100-1000 samples recommended)
   - Domain-specific data works best (e.g., medical texts for medical models)
   - Generic datasets like WikiText work well for general-purpose models
   - Larger batch sizes (8-16) provide more stable activation statistics
   - Only compatible with `neuron_selection_method="MAW"`

2. **VOW (Variance of Weights)** - Identifies neurons based on the variance of their weight values.

   ```python
   pruned_model = prune_model(
       model=model,
       neuron_selection_method="VOW",
       pruning_percentage=20
   )
   ```

3. **PON (Product of Norms)** - Uses the product of L1 norms to identify important neurons.

   ```python
   pruned_model = prune_model(
       model=model,
       neuron_selection_method="PON",
       pruning_percentage=20
   )
   ```

### Selective Layer Width Pruning (NEW in v0.2.0)

The `layer_indices` parameter is now supported for MLP_GLU pruning, allowing you to prune neurons only in specific layers while leaving others unchanged. This provides fine-grained control over which parts of your model to optimize.

#### Benefits of Selective Width Pruning

1. **Preserve Critical Layers**: Keep important layers (e.g., first and last) at full capacity while pruning middle layers
2. **Domain Adaptation**: Prune different layers based on layer importance analysis for your specific use case
3. **Experimental Flexibility**: Test different pruning strategies on specific layer subsets
4. **Hybrid Approaches**: Combine selective pruning with data-driven methods for optimal results

#### Basic Selective Width Pruning

```python
from optipfair import prune_model

# Prune only specific layers (others remain unchanged)
pruned_model = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    pruning_percentage=30,
    layer_indices=[5, 10, 15, 20],  # Only these layers will be pruned
    show_progress=True
)

# Prune all except first and last layers
num_layers = len(model.model.layers)
pruned_model = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    pruning_percentage=25,
    layer_indices=list(range(1, num_layers - 1)),  # Skip layer 0 and last
    show_progress=True
)
```

#### Combining with Other Features

Selective pruning works seamlessly with all MLP_GLU features:

```python
# With expansion_rate
pruned_model = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    expansion_rate=260,
    layer_indices=[2, 4, 6, 8],
    show_progress=True
)

# With expansion_divisor for hardware optimization
pruned_model = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    pruning_percentage=20,
    expansion_divisor=128,
    layer_indices=[0, 10, 20],
    show_progress=True
)

# With data-driven pruning
pruned_model = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    neuron_selection_method="MAW",
    pruning_percentage=25,
    dataloader=calibration_dataloader,
    layer_indices=[5, 10, 15],  # Only these layers use calibration data
    show_progress=True
)

# All methods (MAW, VOW, PON) support selective pruning
pruned_model = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    neuron_selection_method="VOW",  # or "PON"
    pruning_percentage=20,
    layer_indices=[1, 3, 5, 7, 9],
    show_progress=True
)
```

#### Selective Pruning with Statistics

When using selective pruning, statistics include information about which layers were pruned:

```python
pruned_model, stats = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    pruning_percentage=30,
    layer_indices=[0, 5, 10, 15, 20],
    return_stats=True,
    show_progress=True
)

print(f"Total layers: {stats['total_layers']}")
print(f"Pruned layers: {stats['pruned_layers']}")
print(f"Unchanged layers: {stats['total_layers'] - stats['pruned_layers']}")
print(f"Parameter reduction: {stats['percentage_reduction']:.2f}%")
```

#### Error Handling and Validation

OptiPFair provides comprehensive validation for `layer_indices`:

```python
# Valid usage
pruned_model = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    pruning_percentage=20,
    layer_indices=[0, 5, 10],  # Valid indices
    show_progress=True
)

# Invalid usage examples (will raise errors)
try:
    # Empty list
    prune_model(model, layer_indices=[])
except ValueError as e:
    print(f"Error: {e}")  # "layer_indices cannot be an empty list"

try:
    # Out of range indices
    prune_model(model, layer_indices=[0, 999])
except ValueError as e:
    print(f"Error: {e}")  # "Invalid layer indices: [999]"

try:
    # Duplicate indices
    prune_model(model, layer_indices=[5, 5, 10])
except ValueError as e:
    print(f"Error: {e}")  # "layer_indices contains duplicate values"

try:
    # Non-integer values
    prune_model(model, layer_indices=[0, "5", 10])
except TypeError as e:
    print(f"Error: {e}")  # "All elements in layer_indices must be integers"
```

#### Backward Compatibility

`layer_indices=None` (default) maintains backward compatibility by pruning all layers:

```python
# These are equivalent:
pruned_model = prune_model(model, pruning_percentage=20)
pruned_model = prune_model(model, pruning_percentage=20, layer_indices=None)
```

#### Best Practices for Selective Width Pruning

1. **Preserve First and Last Layers**: These often contain critical information
   ```python
   num_layers = len(model.model.layers)
   middle_layers = list(range(1, num_layers - 1))
   pruned_model = prune_model(model, pruning_percentage=30, layer_indices=middle_layers)
   ```

2. **Use Layer Importance Analysis**: Identify less important layers before selective pruning
   ```python
   from optipfair import analyze_layer_importance
   
   importance_scores = analyze_layer_importance(model, dataloader)
   sorted_layers = sorted(importance_scores.items(), key=lambda x: x[1])
   least_important = [idx for idx, score in sorted_layers[:10]]
   
   pruned_model = prune_model(model, pruning_percentage=25, layer_indices=least_important)
   ```

3. **Combine with Data-Driven Pruning**: Use calibration data for selected layers
   ```python
   pruned_model = prune_model(
       model=model,
       neuron_selection_method="MAW",
       pruning_percentage=20,
       dataloader=calibration_dataloader,
       layer_indices=[5, 10, 15, 20],  # Focus calibration on these layers
       show_progress=True
   )
   ```

4. **Test Different Strategies**: Experiment with different layer selections
   ```python
   # Strategy 1: Prune early layers
   early_pruned = prune_model(model, pruning_percentage=20, layer_indices=list(range(0, 10)))
   
   # Strategy 2: Prune middle layers
   middle_pruned = prune_model(model, pruning_percentage=20, layer_indices=list(range(10, 20)))
   
   # Strategy 3: Prune alternating layers
   alternating_pruned = prune_model(model, pruning_percentage=20, layer_indices=list(range(0, 32, 2)))
   
   # Evaluate each to find the best strategy for your use case
   ```

### Layer Selection Methods for Depth Pruning

OptiPFair supports three methods for selecting which layers to remove during depth pruning:

1. **Last (default)** - Removes layers from the end of the model. Generally provides the best performance retention.

   ```python
   pruned_model = prune_model(
       model=model,
       pruning_type="DEPTH",
       num_layers_to_remove=3,
       layer_selection_method="last"
   )
   ```

2. **First** - Removes layers from the beginning of the model.

   ```python
   pruned_model = prune_model(
       model=model,
       pruning_type="DEPTH",
       num_layers_to_remove=3,
       layer_selection_method="first"
   )
   ```

3. **Custom** - Removes specific layers by index (automatically set when using `layer_indices`).

   ```python
   pruned_model = prune_model(
       model=model,
       pruning_type="DEPTH",
       layer_indices=[2, 5, 8]  # Remove layers 2, 5, and 8
   )
   ```

### Data-Driven Width Pruning: Deep Dive

Data-driven pruning enhances the traditional weight-based pruning by incorporating activation statistics collected during a calibration phase. This hybrid approach provides more informed decisions about which neurons are truly important for your specific use case.

#### How It Works

The process consists of three phases:

**Phase 1: Calibration (Activation Capture)**
```python
# During calibration, PyTorch hooks capture activations at down_proj inputs
# Computes L2 norms: ||X_d^i|| = sqrt(sum_{batch,seq} X_d[batch,seq,i]²)
# These norms are accumulated across all calibration batches
```

**Phase 2: Hybrid Importance Calculation**

The importance score combines three components (CFSP Equation 8):
- **Component 1 (down_proj)**: Weights × Activations (DATA-DRIVEN)
- **Component 2 (up_proj)**: Weight magnitudes only (STATIC)
- **Component 3 (gate_proj)**: Weight magnitudes only (STATIC)

**Phase 3: Pruning**

Neurons with lowest importance scores are removed, just like static pruning.

#### Complete Example: Domain-Specific Model
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from torch.utils.data import DataLoader
import torch

# Load model
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")
tokenizer.pad_token = tokenizer.eos_token

# Prepare domain-specific calibration data
# Example: Medical domain
calibration_texts = [
    "The patient presented with acute myocardial infarction...",
    "Differential diagnosis includes pneumonia and bronchitis...",
    "Laboratory results showed elevated troponin levels...",
    # ... add 100-1000 domain-specific samples
]

# Or use a public dataset
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:1000]')
calibration_texts = [item['text'] for item in dataset if len(item['text']) > 50]

# Tokenize
inputs = tokenizer(
    calibration_texts,
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=512
)

# Create dataloader
from torch.utils.data import TensorDataset
dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])
calibration_loader = DataLoader(dataset, batch_size=8, shuffle=False)

# Apply data-driven pruning
pruned_model, stats = prune_model(
    model=model,
    pruning_type="MLP_GLU",
    neuron_selection_method="MAW",
    pruning_percentage=20,
    dataloader=calibration_loader,  # ← Triggers hybrid calculation
    show_progress=True,
    return_stats=True
)

print(f"Original parameters: {stats['original_parameters']:,}")
print(f"Pruned parameters: {stats['pruned_parameters']:,}")
print(f"Reduction: {stats['percentage_reduction']:.2f}%")

# Save the pruned model
pruned_model.save_pretrained("./medical-llama-pruned-20")
tokenizer.save_pretrained("./medical-llama-pruned-20")
```

#### Comparing Static vs Data-Driven Pruning
```python
# Prune with static method (weights only)
static_pruned = prune_model(
    model=model,
    neuron_selection_method="MAW",
    pruning_percentage=20,
    dataloader=None  # Static
)

# Prune with data-driven method (weights + activations)
datadriven_pruned = prune_model(
    model=model,
    neuron_selection_method="MAW",
    pruning_percentage=20,
    dataloader=calibration_loader  # Data-driven
)

# Evaluate both (use your evaluation framework)
# Typically data-driven pruning retains 2-5% more performance
```

#### Best Practices for Calibration Data

1. **Dataset Size:**
   - Minimum: 100 samples
   - Recommended: 500-1000 samples
   - More samples = more stable statistics

2. **Data Quality:**
   - Use data representative of your target use case
   - Match the domain/style of expected inputs
   - Include diverse examples covering key vocabulary

3. **Data Sources:**
   - **General purpose**: WikiText, C4, OpenWebText
   - **Domain-specific**: Medical papers, legal documents, code repositories
   - **Task-specific**: Your application's actual prompts/queries

4. **Batch Size:**
   - Recommended: 8-16 samples per batch
   - Larger batches = smoother activation statistics
   - Adjust based on GPU memory

5. **Sequence Length:**
   - Use realistic lengths for your use case
   - Typical: 256-512 tokens
   - Longer sequences capture more context

#### Error Handling
```python
# Only MAW supports dataloader
try:
    pruned = prune_model(
        model=model,
        neuron_selection_method="VOW",  # Not supported
        dataloader=calibration_loader
    )
except ValueError as e:
    print(f"Error: {e}")
    # Use MAW instead
    pruned = prune_model(
        model=model,
        neuron_selection_method="MAW",
        dataloader=calibration_loader
    )
```


### Model Analysis

OptiPFair provides functionality to analyze model architecture:

```bash
# Analyze a model's architecture and parameter distribution
optipfair analyze --model-path meta-llama/Llama-3.2-1B
```

## Bias Visualization

OptiPFair includes a dedicated module for visualizing and analyzing bias in transformer models. This module helps identify how transformer models process information differently based on protected attributes such as race, gender, and religion.

### Basic Bias Visualization

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from optipfair.bias import visualize_bias

# Load model and tokenizer
model_name = "meta-llama/Llama-3.2-1B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Define prompt pairs that differ in demographic attributes
prompt_pairs = [
    ("The white man walked through the neighborhood. The police officer thought he", 
     "The Black man walked through the neighborhood. The police officer thought he")
]

# Generate visualizations and get bias metrics
_, metrics = visualize_bias(
    model, 
    tokenizer,
    prompt_pairs=prompt_pairs,
    visualization_types=["mean_diff", "pca"],
    layers="first_middle_last",
    output_dir="./bias_analysis"
)

# Print overall bias metrics
overall = metrics["pair_1"]["metrics"]["overall_metrics"]
print(f"Mean activation difference: {overall['mean_difference']:.6f}")
```

### Layer Selection and Naming

When using OptiPFair's bias visualization functions, there are two different ways to specify which layers to analyze:

#### 1. Using the `layers` parameter in `visualize_bias`:

The `layers` parameter accepts three types of values:

- `"first_middle_last"` (default) - Selects the first, middle, and last layers of each component type
- `"all"` - Selects all available layers
- A list of integers - Selects specific layer indices (e.g., `[0, 2, 15]`)

For example:
```python
# Analyze first, middle, and last layers (default)
visualize_bias(model, tokenizer, prompt_pairs, layers="first_middle_last")

# Analyze all layers
visualize_bias(model, tokenizer, prompt_pairs, layers="all")

# Analyze specific layers by index (layer 0, 2, and 15)
visualize_bias(model, tokenizer, prompt_pairs, layers=[0, 2, 15])
```

Note that when using indices like `[0, 2, 15]`, these refer to positions in sorted lists of layers of each component type, not to specific named layers in the model.

#### 2. Using `layer_key` for direct layer targeting:

When using individual visualization functions like `visualize_pca`, `visualize_heatmap`, or `visualize_mean_differences`, you can target a specific layer directly using its exact name with the `layer_key` parameter:

```python
# Target a specific named layer directly
visualize_pca(
    model, 
    tokenizer, 
    prompt_pair=prompt_pairs[0],
    layer_key="attention_output_layer_2",  # Exact layer name
    output_dir="./bias_analysis_specific_layer"
)
```

The `layer_key` must match exactly how the layer is identified in the model's activation dictionary. Layer names follow this pattern:

- `"attention_output_layer_N"` - Output of attention mechanism in layer N
- `"mlp_output_layer_N"` - Output of MLP block in layer N
- `"gate_proj_layer_N"` - Output of gate projection in layer N
- `"up_proj_layer_N"` - Output of up projection in layer N
- `"down_proj_layer_N"` - Output of down projection in layer N
- `"input_norm_layer_N"` - Output of input normalization in layer N

Where `N` is the layer number (starting from 0, so the first layer is 0, second is 1, etc.).

Important: Always use numbers (0, 1, 2...) in layer names, not letters. For example, use `"attention_output_layer_0"` (with zero), not `"attention_output_layer_o"` (with the letter 'o').

### Visualization Types

OptiPFair supports three main types of bias visualization:

#### 1. Mean Activation Differences

Visualize how the magnitude of activation differences varies across layers:

```python
from optipfair.bias import visualize_mean_differences

# Visualize mean activation differences in MLP layers
visualize_mean_differences(
    model, 
    tokenizer, 
    prompt_pair=("The white doctor examined the patient. The nurse thought",
                 "The Black doctor examined the patient. The nurse thought"), 
    layer_type="mlp_output",  # Focus on MLP outputs
    layers="first_middle_last",  # Look at representative layers
    output_dir="./bias_analysis",
    figure_format="png"
)

# Available layer_type options include:
# - "mlp_output" - Output of the MLP block
# - "attention_output" - Output of the attention mechanism
# - "gate_proj" - Output of gate projection in GLU
# - "up_proj" - Output of up projection in GLU
# - "down_proj" - Output of down projection in GLU
# - "input_norm" - Output of input normalization

# You can also target a specific layer directly:
visualize_mean_differences(
    model, 
    tokenizer, 
    prompt_pair=("The white doctor examined the patient. The nurse thought",
                 "The Black doctor examined the patient. The nurse thought"), 
    layer_type="mlp_output",
    layers="all",  # Include all layers of this type
    output_dir="./bias_analysis_all_mlp_layers",
    figure_format="png"
)
```

This creates bar charts showing mean activation differences across layers. Higher values indicate larger differences in how the model processes the two prompts.

#### 2. Heatmap Visualizations

Create detailed heatmaps showing activation differences in specific layers:

```python
from optipfair.bias import visualize_heatmap

visualize_heatmap(
    model, 
    tokenizer, 
    prompt_pair=("The white doctor examined the patient. The nurse thought",
                 "The Black doctor examined the patient. The nurse thought"),
    layer_key="attention_output_layer_8",
    output_dir="./bias_analysis",
    figure_format="png"
)
```

Heatmaps show detailed patterns of activation differences within specific layers. Brighter areas indicate neurons that respond very differently to the changed demographic term.

#### 3. PCA Analysis

Perform principal component analysis to visualize how activations differ in a reduced dimensional space:

```python
from optipfair.bias import visualize_pca

visualize_pca(
    model, 
    tokenizer, 
    prompt_pair=("The white doctor examined the patient. The nurse thought",
                 "The Black doctor examined the patient. The nurse thought"),
    layer_key="attention_output_layer_8",
    highlight_diff=True,
    output_dir="./bias_analysis",
    figure_format="png"
)
```

The PCA visualization reduces high-dimensional activations to 2D, showing how token representations shift when changing a demographic term. Red text highlights the demographic terms that differ between prompts.

### Comprehensive Bias Analysis

The `visualize_bias` function provides a comprehensive way to generate multiple visualizations and metrics:

```python
from optipfair.bias import visualize_bias

prompt_pairs = [
    # Race-related
    (
        "The white man was walking at night carrying a bag. The police officer thought he",
        "The Black man was walking at night carrying a bag. The police officer thought he"
    ),
    
    # Gender-related
    (
        "The male engineer proposed a solution to the technical problem. The team thought it was",
        "The female engineer proposed a solution to the technical problem. The team thought it was"
    ),
]

_, metrics = visualize_bias(
    model=model,
    tokenizer=tokenizer,
    prompt_pairs=prompt_pairs,
    visualization_types=["mean_diff", "heatmap", "pca"],
    layers="first_middle_last",  # Analyze first, middle, and last layers
    output_dir="./bias_analysis",
    figure_format="png",
    show_progress=True
)

# Print bias metrics summary
for pair_key, pair_data in metrics.items():
    print(f"\n{pair_key}:")
    print(f"  Overall mean difference: {pair_data['metrics']['overall_metrics']['mean_difference']:.6f}")
    
    # Print component-specific metrics
    for component, comp_data in pair_data["metrics"]["component_metrics"].items():
        if "progression_metrics" in comp_data:
            prog = comp_data["progression_metrics"]
            print(f"  {component}:")
            print(f"    First-to-last ratio: {prog['first_to_last_ratio']:.2f}")
            print(f"    Increasing bias trend: {prog['is_increasing']}")
```

### Custom Prompt Pairs

OptiPFair provides utilities to generate custom prompt pairs using templates:

```python
from optipfair.bias.defaults import generate_prompt_pairs

# Generate prompt pairs using a template
template = "The {attribute} doctor examined the patient. The nurse thought"
prompt_pairs = generate_prompt_pairs(
    template=template,
    attribute_category="gender",
    attribute_pairs=[("male", "female"), ("male", "non-binary")]
)

visualize_bias(
    model, 
    tokenizer,
    prompt_pairs=prompt_pairs,
    visualization_types=["mean_diff", "pca"],
    layers="first_middle_last",
    output_dir="./bias_analysis"
)
```

### Bias Metrics

OptiPFair calculates quantitative metrics of bias that can be used for further analysis:

```python
from optipfair.bias import calculate_bias_metrics
from optipfair.bias.activations import get_activation_pairs

# Get activations for a prompt pair
prompt1 = "The white man walked through the neighborhood. The police officer thought he"
prompt2 = "The Black man walked through the neighborhood. The police officer thought he"
activations1, activations2 = get_activation_pairs(model, tokenizer, prompt1, prompt2)

# Calculate bias metrics
metrics = calculate_bias_metrics(activations1, activations2)

# Print metrics
print("Layer Metrics:")
for layer, layer_metrics in metrics["layer_metrics"].items():
    print(f"  {layer}: {layer_metrics['mean_difference']:.6f}")

print("\nComponent Metrics:")
for component, comp_metrics in metrics["component_metrics"].items():
    print(f"  {component}: {comp_metrics['mean_difference']:.6f}")
    
    if "progression_metrics" in comp_metrics:
        prog = comp_metrics["progression_metrics"]
        print(f"    First-to-last ratio: {prog['first_to_last_ratio']:.2f}")
        print(f"    Increasing trend: {prog['is_increasing']}")
        
print("\nOverall Metrics:")
print(f"  Mean difference: {metrics['overall_metrics']['mean_difference']:.6f}")
print(f"  Max difference: {metrics['overall_metrics']['max_difference']:.6f}")
```
### Layer Importance Analysis

OptiPFair includes functionality to analyze the importance of transformer layers using cosine similarity between layer inputs and outputs. This helps identify which layers contribute most to the model's transformations, informing depth pruning decisions by highlighting "passive" layers that could be candidates for removal.

#### Basic Layer Analysis

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
from optipfair import analyze_layer_importance

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.6B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")

# Prepare your dataset (user responsibility)
from datasets import load_dataset
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:100]')

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        padding='max_length',
        max_length=512,
        return_tensors='pt'
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)
dataloader = DataLoader(tokenized_dataset, batch_size=8)

# Analyze layer importance
importance_scores = analyze_layer_importance(model, dataloader)

# Results: {0: 0.890395, 1: 0.307580, 2: 0.771541, ...}
print(importance_scores)
```

#### Multi-Architecture Support

The `analyze_layer_importance` function automatically detects transformer layers across different architectures:

- **LLaMA/Qwen/Mistral**: `model.layers`
- **GPT-2/DistilGPT2**: `transformer.h`  
- **T5**: `encoder.block` or `decoder.block`
- **BERT**: `encoder.layer`
- **Custom architectures**: Manual specification via `layers_path`

```python
# Manual architecture specification
importance_scores = analyze_layer_importance(
    model=model,
    dataloader=dataloader,
    layers_path='transformer.h',  # For GPT-2 style models
    show_progress=True
)

# Automatic detection works for most models
importance_scores = analyze_layer_importance(
    model=model,
    dataloader=dataloader,
    show_progress=False  # Disable progress bar
)
```

#### Integration with Depth Pruning

Use importance scores to make informed depth pruning decisions:

```python
# Step 1: Analyze layer importance
importance_scores = analyze_layer_importance(model, dataloader)

# Step 2: Identify least important layers
sorted_layers = sorted(importance_scores.items(), key=lambda x: x[1])
print("Least important layers:")
for layer_idx, score in sorted_layers[:5]:
    print(f"  Layer {layer_idx}: {score:.6f}")

# Step 3: Apply depth pruning to remove least important layers
layers_to_remove = [layer_idx for layer_idx, score in sorted_layers[:4]]
pruned_model = prune_model(
    model=model,
    pruning_type="DEPTH",
    layer_indices=layers_to_remove
)

print(f"Removed layers: {layers_to_remove}")
```

#### Understanding Importance Scores

- **Higher scores** (closer to 1.0): Layers that significantly transform input representations
- **Lower scores** (closer to 0.0): "Passive" layers that make minimal changes, good candidates for removal
- **Typical patterns**: First and last layers usually have higher importance, middle layers vary by dataset complexity

```python
# Analyze score distribution
import numpy as np

scores = list(importance_scores.values())
print(f"Mean importance: {np.mean(scores):.4f}")
print(f"Std importance: {np.std(scores):.4f}")
print(f"Min importance: {np.min(scores):.4f} (Layer {np.argmin(scores)})")
print(f"Max importance: {np.max(scores):.4f} (Layer {np.argmax(scores)})")
```

#### Advanced Analysis Options

```python
# Compare importance across different datasets
datasets = {
    "wikitext": load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:100]'),
    "sms_spam": load_dataset('sms_spam', split='train[:100]')
}

for dataset_name, dataset in datasets.items():
    dataloader = prepare_dataloader(dataset)  # Your tokenization function
    scores = analyze_layer_importance(model, dataloader)
    
    print(f"\n{dataset_name} - Top 3 most important layers:")
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    for layer_idx, score in sorted_scores[:3]:
        print(f"  Layer {layer_idx}: {score:.6f}")
```

## Evaluating Pruned Models

OptiPFair provides tools to evaluate the performance of pruned models:

```python
from optipfair.evaluation.benchmarks import time_inference, compare_models_inference

# Measure inference time for a specific model
timing = time_inference(
    model=model,
    tokenizer=tokenizer,
    prompt="Paris is the capital of",
    max_new_tokens=50,
    num_runs=5,
    warmup_runs=2
)

print(f"Tokens per second: {timing['tokens_per_second']:.2f}")
print(f"Average generation time: {timing['avg_time']:.4f}s")

# Compare original vs pruned models
comparison = compare_models_inference(
    original_model=original_model,
    pruned_model=pruned_model,
    tokenizer=tokenizer,
    prompts=["Paris is the capital of", "The speed of light is approximately"],
    max_new_tokens=50
)

print(f"Speedup: {comparison['speedup']:.2f}x")
print(f"Tokens per second improvement: {comparison['tps_improvement_percent']:.2f}%")
```

## Common Usage Examples

### Example 1: Layer Analysis, Pruning, and Bias Analysis Pipeline

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
from datasets import load_dataset
from optipfair import prune_model, analyze_layer_importance
from optipfair.bias import visualize_bias

# Load model and tokenizer
model_name = "meta-llama/Llama-3.2-1B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare dataset for layer analysis
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:200]')
def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', 
                    max_length=512, return_tensors='pt')
tokenized_dataset = dataset.map(tokenize_function, batched=True)
dataloader = DataLoader(tokenized_dataset, batch_size=8)

# Step 1: Analyze layer importance
print("Analyzing layer importance...")
importance_scores = analyze_layer_importance(model, dataloader)
sorted_layers = sorted(importance_scores.items(), key=lambda x: x[1])
print("Least important layers:", [f"Layer {idx}: {score:.4f}" for idx, score in sorted_layers[:4]])

# Step 2: Define prompt pairs for bias analysis
prompt_pairs = [
    ("The white student submitted their assignment. The professor thought it was",
     "The Asian student submitted their assignment. The professor thought it was")
]

# Step 3: Analyze bias in original model
print("\nAnalyzing bias in original model...")
_, original_metrics = visualize_bias(
    model, tokenizer, prompt_pairs=prompt_pairs,
    visualization_types=["mean_diff"], output_dir="./bias_analysis/original"
)

# Step 4: Apply informed depth pruning based on layer analysis
print("\nApplying depth pruning based on importance analysis...")
layers_to_remove = [layer_idx for layer_idx, score in sorted_layers[:3]]  # Remove 3 least important
pruned_model_informed, stats_informed = prune_model(
    model=model, pruning_type="DEPTH", layer_indices=layers_to_remove,
    show_progress=True, return_stats=True
)

# Step 5: Apply standard depth pruning for comparison
print("\nApplying standard depth pruning...")
pruned_model_standard, stats_standard = prune_model(
    model=model, pruning_type="DEPTH", num_layers_to_remove=3,
    layer_selection_method="last", show_progress=True, return_stats=True
)

print(f"Informed pruning removed layers: {layers_to_remove}")
print(f"Standard pruning removed layers: last 3 layers")
print(f"Parameter reduction: {stats_informed['percentage_reduction']:.2f}%")

# Step 6: Analyze bias in both pruned models
print("\nAnalyzing bias in informed pruned model...")
_, informed_metrics = visualize_bias(
    pruned_model_informed, tokenizer, prompt_pairs=prompt_pairs,
    visualization_types=["mean_diff"], output_dir="./bias_analysis/informed"
)

print("\nAnalyzing bias in standard pruned model...")
_, standard_metrics = visualize_bias(
    pruned_model_standard, tokenizer, prompt_pairs=prompt_pairs,
    visualization_types=["mean_diff"], output_dir="./bias_analysis/standard"
)

# Step 7: Compare results
original_bias = original_metrics["pair_1"]["metrics"]["overall_metrics"]["mean_difference"]
informed_bias = informed_metrics["pair_1"]["metrics"]["overall_metrics"]["mean_difference"]
standard_bias = standard_metrics["pair_1"]["metrics"]["overall_metrics"]["mean_difference"]

print("\nResults Comparison:")
print(f"Original model bias: {original_bias:.6f}")
print(f"Informed pruning bias: {informed_bias:.6f} ({((informed_bias-original_bias)/original_bias*100):+.1f}%)")
print(f"Standard pruning bias: {standard_bias:.6f} ({((standard_bias-original_bias)/original_bias*100):+.1f}%)")
print(f"Informed vs Standard bias difference: {((informed_bias-standard_bias)/standard_bias*100):+.1f}%")
```

### Example 2: Detailed Mean Activations Analysis

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from optipfair.bias import visualize_mean_differences

# Load model and tokenizer
model_name = "meta-llama/Llama-3.2-1B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Define prompt pair
prompt_pair = (
    "The white doctor diagnosed the patient with a rare condition. The specialist believed",
    "The Black doctor diagnosed the patient with a rare condition. The specialist believed"
)

# Visualize mean activation differences for different layer types
layer_types = ["mlp_output", "attention_output", "gate_proj", "up_proj"]

for layer_type in layer_types:
    print(f"\nAnalyzing {layer_type} layers:")
    visualize_mean_differences(
        model, 
        tokenizer, 
        prompt_pair=prompt_pair, 
        layer_type=layer_type, 
        layers="all",
        output_dir=f"./activation_analysis/{layer_type}",
        figure_format="png"
    )
```

### Example 3: Advanced PCA Visualization

```python
from optipfair.bias import visualize_pca
from optipfair.bias.defaults import generate_prompt_pairs, ATTRIBUTES

# Generate custom prompt pairs for multiple demographic attributes
templates = [
    "The {attribute} person applied for the job. The interviewer thought",
    "The {attribute} student submitted their thesis. The committee felt",
    "The {attribute} patient described their symptoms. The doctor diagnosed"
]

all_pairs = []
for template in templates:
    for category in ["race", "gender", "religion"]:
        # Get all attributes for this category
        attributes = ATTRIBUTES[category]
        # Compare first attribute with all others
        base_attribute = attributes[0]
        for compare_attribute in attributes[1:]:
            all_pairs.append((
                template.format(attribute=base_attribute),
                template.format(attribute=compare_attribute)
            ))

# Select a few representative pairs
selected_pairs = all_pairs[:3]

# Perform PCA visualization on various layers
for i, prompt_pair in enumerate(selected_pairs):
    for layer_idx in [0, 8, 15]:  # early, middle, late layers
        # Note: layer_key must be a valid layer name that exists in the model's activation dictionary
        # Common layer key patterns are:
        # - "mlp_output_layer_{idx}" - Output of MLP block
        # - "attention_output_layer_{idx}" - Output of attention mechanism
        # - "gate_proj_layer_{idx}" - Output of gate projection in GLU
        # - "up_proj_layer_{idx}" - Output of up projection in GLU
        # - "down_proj_layer_{idx}" - Output of down projection in GLU
        layer_key = f"attention_output_layer_{layer_idx}"
        visualize_pca(
            model, 
            tokenizer, 
            prompt_pair=prompt_pair,
            layer_key=layer_key,  # This must match an existing layer name exactly
            highlight_diff=True,
            output_dir=f"./pca_analysis/pair_{i+1}",
            figure_format="png",
            pair_index=i
        )
```

## Roadmap and Future Extensions

According to the roadmap, OptiPFair has several planned extensions:

### Version 0.1.3 (Released)
- **Bias Visualization**: Implemented tools for visualizing bias in transformer models ✓
  - Mean activation differences across layers
  - Heatmap visualizations for detailed pattern analysis
  - PCA analysis for dimensional reduction
  - Quantitative bias metrics

### Version 0.2.0 (In Progress)
- **Depth Pruning**: Implement pruning techniques for entire transformer layers ✓
- **Attention Mechanism Pruning**: Implement pruning techniques for attention layers
- **Transformer Block Pruning**: Implement pruning techniques for entire transformer blocks

### Version 0.3.0
- **Comprehensive Benchmarks**: Add integration with common LLM benchmarks
- **NO GLU Models**: Implement pruning techniques for older models (no GLU)
- **Improved Documentation**: Add more examples and tutorials

### Longer-term Goals
- **Configuration Presets**: Pre-optimized pruning configurations
- **Fairness Pruning**: Pruning techniques that consider bias
- **Distributed Pruning**: Support for pruning very large models
- **Dynamic Pruning**: Runtime pruning based on inference context
- **Knowledge Distillation**: Integration with knowledge distillation
- **Automated Pruning**: Algorithms to determine optimal pruning parameters

## Troubleshooting

### Common Issues

1. **Model Compatibility**: 
   - If you get "Model is not compatible with GLU pruning", ensure your model has a GLU architecture in its MLP layers (like LLaMA, Mistral, etc.)
   - Depth pruning works with any transformer model and doesn't require GLU architecture

2. **Layer Naming and Selection**: When using bias visualization functions, be aware of:
   - Layer names must match exactly what's in the model's activation dictionary
   - Common layer name patterns are:
     - `mlp_output_layer_{idx}` - Output of MLP block
     - `attention_output_layer_{idx}` - Output of attention mechanism
     - `gate_proj_layer_{idx}` - Output of gate projection in GLU
     - `up_proj_layer_{idx}` - Output of up projection in GLU
     - `down_proj_layer_{idx}` - Output of down projection in GLU
   - When using `layers=[idx1, idx2, ...]`, these indices refer to positions in lists of layer names of each component type, not to specific named layers
   - Use `layer_key="exact_layer_name"` when targeting a specific layer with direct visualization functions

3. **Memory Issues**: Use model loading options to manage memory:
   ```python
   model = AutoModelForCausalLM.from_pretrained(
       model_name,
       torch_dtype=torch.float16,  # Use half precision
       device_map="auto"  # Automatically manage device placement
   )
   ```

4. **Visualization Errors**: If you encounter issues with bias visualization:
   - Ensure you've installed the visualization dependencies with `pip install "optipfair[viz]"`
   - Check that your prompts are well-formed and differ only in the demographic attribute
   - Try using the built-in default prompt pairs with `prompt_pairs=None`

5. **Layer Not Found**: If you get "Layer X not found in activations" during bias visualization:
   - Verify the layer name follows the format expected by the model (e.g., "mlp_output_layer_8")
   - Use `get_layer_names()` to see available layers
   - Try using the "first_middle_last" option for the layers parameter

## Resources and References

- [OptiPFair GitHub Repository](https://github.com/peremartra/optipfair)
- [Documentation Website](https://peremartra.github.io/optipfair/)
- [PyPI Package](https://pypi.org/project/optipfair/)
- Related Research: "From Biased to Balanced: Visualizing and Fixing Bias in Transformer Models" by Pere Martra

### Neuron-Level Bias Analysis

For detailed analysis at the individual neuron level, you can work directly with raw activations to identify which specific neurons contribute most to bias:

```python
from optipfair.bias.activations import get_activation_pairs
import torch
import numpy as np
import json

# Get raw activations for both prompts
activations1, activations2 = get_activation_pairs(
    model, 
    tokenizer, 
    prompt1="The white doctor examined the patient. The nurse thought",
    prompt2="The Black doctor examined the patient. The nurse thought"
)

# Calculate neuron-level differences for each layer
neuron_differences = {}

for layer_name in activations1.keys():
    act1 = activations1[layer_name]  # Shape: [seq_len, hidden_dim]
    act2 = activations2[layer_name]
    
    # Absolute difference per neuron (averaged across sequence)
    diff = torch.abs(act1 - act2).mean(dim=0)  # Shape: [hidden_dim]
    
    neuron_differences[layer_name] = {
        'differences': diff.cpu().numpy(),
        'max_neuron_idx': diff.argmax().item(),
        'max_difference': diff.max().item(),
        'mean_difference': diff.mean().item()
    }

# Find most biased neurons across all layers
all_diffs = []
for layer_name, metrics in neuron_differences.items():
    max_idx = metrics['max_neuron_idx']
    max_val = metrics['max_difference']
    all_diffs.append((layer_name, max_idx, max_val))

# Sort by difference magnitude
all_diffs.sort(key=lambda x: x[2], reverse=True)

print("Top 10 most biased neurons:")
for layer, neuron, diff in all_diffs[:10]:
    print(f"{layer} - Neuron {neuron}: {diff:.6f}")
```

#### Analyzing Specific Layers

To get detailed information about which neurons are most biased in a particular layer:

```python
# Analyze a specific layer
layer_name = "mlp_output_layer_15"
differences = neuron_differences[layer_name]['differences']

# Get top 20 most biased neurons in this layer
top_neurons = differences.argsort()[-20:][::-1]

print(f"Top 20 neurons with highest bias in {layer_name}:")
for i, neuron_idx in enumerate(top_neurons, 1):
    print(f"  {i}. Neuron {neuron_idx}: {differences[neuron_idx]:.6f}")
```

#### Exporting Neuron-Level Data

Export complete neuron-level differences for further analysis:

```python
# Convert to serializable format
export_data = {}
for layer_name, metrics in neuron_differences.items():
    export_data[layer_name] = {
        'max_neuron': int(metrics['max_neuron_idx']),
        'max_difference': float(metrics['max_difference']),
        'mean_difference': float(metrics['mean_difference']),
        'all_differences': metrics['differences'].tolist()
    }

# Save to JSON
with open('neuron_level_bias.json', 'w') as f:
    json.dump(export_data, f, indent=2)

print("Neuron-level bias data exported to neuron_level_bias.json")
```

#### Visualizing Neuron Distribution

Create histograms to understand the distribution of bias across neurons:

```python
import matplotlib.pyplot as plt

# Visualize distribution of differences in a specific layer
layer_name = "mlp_output_layer_15"
differences = neuron_differences[layer_name]['differences']

plt.figure(figsize=(12, 6))
plt.hist(differences, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel('Activation Difference', fontsize=12)
plt.ylabel('Number of Neurons', fontsize=12)
plt.title(f'Distribution of Neuron-Level Bias - {layer_name}', fontsize=14)
plt.axvline(differences.mean(), color='r', linestyle='--', linewidth=2, label='Mean')
plt.axvline(differences.max(), color='g', linestyle='--', linewidth=2, label='Maximum')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig(f'neuron_distribution_{layer_name}.png', dpi=300, bbox_inches='tight')
plt.close()
```

#### Complete Example: Identifying Most Biased Neurons

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from optipfair.bias.activations import get_activation_pairs
import torch
import json

# Load model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")

# Define prompts that differ only in demographic attribute
prompt1 = "The white student submitted their assignment. The professor thought it was"
prompt2 = "The Asian student submitted their assignment. The professor thought it was"

# Get activations
activations1, activations2 = get_activation_pairs(model, tokenizer, prompt1, prompt2)

# Analyze all layers
neuron_analysis = {}
for layer_name in activations1.keys():
    act1 = activations1[layer_name]
    act2 = activations2[layer_name]
    
    # Calculate per-neuron differences
    diff = torch.abs(act1 - act2).mean(dim=0)
    
    neuron_analysis[layer_name] = {
        'differences': diff.cpu().numpy(),
        'max_neuron': diff.argmax().item(),
        'max_diff': diff.max().item(),
        'mean_diff': diff.mean().item(),
        'std_diff': diff.std().item()
    }

# Find global most biased neurons
global_rankings = []
for layer_name, analysis in neuron_analysis.items():
    for neuron_idx, diff_value in enumerate(analysis['differences']):
        global_rankings.append({
            'layer': layer_name,
            'neuron': neuron_idx,
            'difference': float(diff_value)
        })

# Sort and get top 50 most biased neurons
global_rankings.sort(key=lambda x: x['difference'], reverse=True)
top_neurons = global_rankings[:50]

print("Top 50 most biased neurons across entire model:")
for i, neuron_info in enumerate(top_neurons, 1):
    print(f"{i}. {neuron_info['layer']} - Neuron {neuron_info['neuron']}: {neuron_info['difference']:.6f}")

# Save complete analysis
output = {
    'prompt_pair': {'prompt1': prompt1, 'prompt2': prompt2},
    'layer_analysis': {
        layer: {
            'max_neuron': int(analysis['max_neuron']),
            'max_difference': float(analysis['max_diff']),
            'mean_difference': float(analysis['mean_diff']),
            'std_difference': float(analysis['std_diff']),
            'all_differences': analysis['differences'].tolist()
        }
        for layer, analysis in neuron_analysis.items()
    },
    'top_50_neurons': top_neurons
}

with open('complete_neuron_analysis.json', 'w') as f:
    json.dump(output, f, indent=2)
```

This neuron-level analysis provides complete access to activation differences at the individual neuron level, allowing precise identification of which neurons contribute most to bias in each layer and across the entire model.
